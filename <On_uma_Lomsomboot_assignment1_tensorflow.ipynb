{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "<On-uma_Lomsomboot_assignment1_tensorflow.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPDq2iLsRFtN5haHAUeUa00",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmujoeAdiday/A-star-search-to-find-the-best-path/blob/main/%3COn_uma_Lomsomboot_assignment1_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InksnBF7GmX1"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Use tensorflow for automatic differentiation**"
      ],
      "metadata": {
        "id": "sjhgH30JGs0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "9AAHcR4zHZ4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_parameters(nx,nh,ny):\n",
        "    #set the random seed so the same random values are generated every time you run this function\n",
        "    tf.random.set_seed(1)\n",
        "\n",
        "    \n",
        "    #initialize weights to small random numbers and biases to zeros for each layer\n",
        "    W1=tf.Variable(tf.random.uniform(shape=(nh,nx), minval=-0.01, maxval=0.01),name = \"W1\")\n",
        "    b1=tf.Variable(tf.zeros(shape=(nh,1),name = \"b1\"))\n",
        "    W2=tf.Variable(tf.random.uniform(shape=(ny,nh), minval=-0.01, maxval=0.01),name = \"W2\")\n",
        "    b2=tf.Variable(tf.zeros(shape=(ny,1),name = \"b2\"))\n",
        "   \n",
        "    #create a dictionary of network parameters\n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "    \n",
        "    return parameters"
      ],
      "metadata": {
        "id": "2ki7xRklHZ7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_pass(parameters,X):\n",
        "    X =tf.cast(X, tf.float32)\n",
        "    Z1= tf.matmul(parameters[\"W1\"],X)+parameters[\"b1\"] # b1 is broadcasted n times before it is added to np.dpt(W1,X1)\n",
        "    \n",
        "    Yhat=tf.matmul(parameters[\"W2\"],Z1)+parameters[\"b2\"] #b2 is broadcasted n times before it is added to np.dpt(W2,A1)\n",
        "    \n",
        "     \n",
        "    return Yhat"
      ],
      "metadata": {
        "id": "DIn0DJ5cHZ-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "n is the number of examples, y is a vector of actual/observed outputs and yhat is a vector of predicted outputs\n",
        "\"\"\" \n",
        "def compute_loss(Y,Yhat):\n",
        "   \n",
        "    per_sample_loss = tf.pow(Y-Yhat,2) \n",
        "    loss = tf.reduce_mean(per_sample_loss)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "IwrzLHC_HaJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.ops.gradients_impl import gradients\n",
        "def backward_pass(parameters, loss, tape):\n",
        "  gradients = tape.gradient(loss, parameters)\n",
        "  return gradients"
      ],
      "metadata": {
        "id": "8_mpvHdOHaMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_parameters(parameters, gradients, learning_rate):\n",
        "    parameters[\"W1\"].assign_sub(learning_rate*gradients[\"W1\"])\n",
        "    parameters[\"W2\"].assign_sub(learning_rate*gradients[\"W2\"])\n",
        "    parameters[\"b1\"].assign_sub(learning_rate*gradients[\"b1\"])\n",
        "    parameters[\"b2\"].assign_sub(learning_rate*gradients[\"b2\"])\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "NwbU3RSdHaPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Arguments: train_X: is the training dataset (features)\n",
        "           train_Y: is the vector of labels for training_X\n",
        "           val_X: is the vector of validation dataset (features)\n",
        "           val_y: is the vector of labels for val_X\n",
        "           nh: is the number of neurons in the hidden layer\n",
        "           num_iterations: The number of iterations of gradient descent\n",
        "\"\"\"\n",
        "def create_nn_model(train_X,train_Y,nh, val_X, val_Y, num_iterations, learning_rate):\n",
        "    \"\"\"\n",
        "    Do some safety check on the data before proceeding. \n",
        "    train_X and val_X must have the same number of features (i.e., same number of rows)\n",
        "    train_X must have the same number of examples as train_Y (i.e., same number of columns )\n",
        "    val_X must have the same number of examples as Val_Y\n",
        "    \"\"\" \n",
        "    assert(train_X.shape[0]==val_X.shape[0]), \"train_X and val_X must have the same number of features\"\n",
        "    assert(train_X.shape[1]==train_Y.size), \"train_X and train_Y must have the same number of examples\"\n",
        "    assert(val_X.shape[1]==val_Y.size), \"val_X and val_Y must have the same number of examples\" \n",
        "    \n",
        "    \n",
        "    #getting the number of features\n",
        "    nx=train_X.shape[0]\n",
        "    \n",
        "    # We want to use this network for binary classification, so we have only one neuron in the output layer with a sigmoid activation\n",
        "    ny=1\n",
        "    \n",
        "    # initializing the parameteres\n",
        "    parameters=initialize_parameters(nx,nh,ny)\n",
        "    \n",
        "    \n",
        "    #initialize lists to store the training and valideation losses for each iteration. \n",
        "    val_losses=[]\n",
        "    train_losses=[]\n",
        "    \n",
        "    #run num_iterations of gradient descent\n",
        "    for i in range (0, num_iterations):\n",
        "      with tf.GradientTape() as tape:\n",
        "        train_Yhat = forward_pass(parameters,train_X)\n",
        "        train_loss = compute_loss(train_Y, train_Yhat)\n",
        "      \n",
        "        #compute validation loss\n",
        "      Yhat_val= forward_pass(parameters,val_X)\n",
        "      val_loss=compute_loss(val_Y,Yhat_val)\n",
        "      \n",
        "      #print the trianing loss and validation loss for each iteration.\n",
        "      print(\"iteration {} :train_loss:{} val_loss{}\".format(i,train_loss,val_loss))\n",
        "\n",
        "       # append the train and validation loss for the current iteration to the train_losses and val_losses \n",
        "      train_losses.append(train_loss)\n",
        "      val_losses.append(val_loss)\n",
        "     \n",
        "      \"\"\"\n",
        "      Compute the gradients and update the parameters\n",
        "      \"\"\"    \n",
        "      #compute the gradients on the training data\n",
        "      gradients=backward_pass(parameters,train_loss,tape)\n",
        "\n",
        "      # update the parameters\n",
        "      parameters=update_parameters(parameters, gradients, learning_rate)\n",
        "\n",
        "    \n",
        "    \n",
        "    #create a dictionary history and put train_loss and validaiton_loss in it\n",
        "    history={\"val_loss\": val_losses,\n",
        "             \"train_loss\": train_losses}\n",
        "        \n",
        "        \n",
        "    #return the parameters and the history\n",
        "    return parameters, history\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Dq0UNc4yHaYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(parameters,X, prob_threshold=0.5):\n",
        "    Yhat=forward_pass(parameters, X)\n",
        "    # predict class 1 if the output is greater than prob_threshold; otherwise, predict zero\n",
        "    predicted_label=np.where(Yhat>prob_threshold, 1, 0)  \n",
        "    return predicted_label"
      ],
      "metadata": {
        "id": "TCBHyjYUHacI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(observedY,predictedY):\n",
        "    #return the ratio of the examples for which predictedY=observedY over the total number of examples\n",
        "    return float(np.sum(predictedY==observedY))/observedY.size"
      ],
      "metadata": {
        "id": "D5IbqMTwdhZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scale_label_down(arr):\n",
        "  return np.divide(arr,100000)\n"
      ],
      "metadata": {
        "id": "iwqFIGXIdhcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_wise_normalization(feature_data):\n",
        "  mean = np.mean(feature_data, axis = 0)\n",
        "  std = np.std(feature_data, axis = 0)\n",
        "  print(mean,std)\n",
        "  new_data = np.subtract(feature_data,mean)\n",
        "  return np.divide(new_data,std)\n",
        "\n",
        "x = np.array([[1,0,3],[4,5,16]])\n",
        "print(feature_wise_normalization(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fu-G8R2ddhfm",
        "outputId": "a6220b62-83c1-4392-bd43-08396cd90e34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2.5 2.5 9.5] [1.5 2.5 6.5]\n",
            "[[-1. -1. -1.]\n",
            " [ 1.  1.  1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"sample_data/california_housing_train.csv\").to_numpy()\n",
        "\n",
        "\n",
        "np.random.shuffle(data)\n",
        "# original_features = data[:,:-1]\n",
        "# features = feature_wise_normalization(original_features)\n",
        "\n",
        "# original_label =  data[:,-1]\n",
        "\n",
        "# labels = scale_label_down(original_label)\n",
        "\n",
        "split_size = int(0.8*len(data))\n",
        "\n",
        "# train_X, val_X = features[: split_size], features[split_size:]\n",
        "# train_Y, val_Y = labels[: split_size], labels[split_size:]\n",
        "\n",
        "\n",
        "train = np.transpose(data[:split_size,:])\n",
        "val = np.transpose(data[split_size:, :])\n",
        "\n",
        "\n",
        "#features are in X\n",
        "#label is in Y\n",
        "train_X = feature_wise_normalization(train[:-1,])\n",
        "train_Y = scale_label_down(train[-1,])\n",
        "\n",
        "\n",
        "#reshape train_Y and val_Y\n",
        "train_Y = np.reshape(train_Y, (1,train_Y.size))\n",
        "\n",
        "val_X = feature_wise_normalization(val[:-1,])\n",
        "val_Y = scale_label_down(val[-1,])\n",
        "\n",
        "val_Y = np.reshape(val_Y, (1,val_Y.size))\n",
        "\n",
        "print(\"trainx\",train_X.shape)\n",
        "print(\"val x\",val_X.shape)\n",
        "print(\"trainy\",train_Y.shape)\n",
        "\n",
        "print(\"val y\",val_Y.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31tJ40mQdhiC",
        "outputId": "51803fe4-8d2e-467a-a938-5f56aa5c03c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[471.0963875 438.0891    317.1333875 ... 470.51265   568.875225\n",
            " 849.14055  ] [ 669.16087182  662.53354739  438.40666015 ...  656.37111534  827.96444666\n",
            " 1203.4774696 ]\n",
            "[574.9236125 404.96735   547.1849125 ... 416.0913375 329.21625\n",
            " 403.9564125] [751.00160878 614.43472881 779.35249169 ... 605.00482494 433.42731132\n",
            " 551.69610686]\n",
            "trainx (8, 13600)\n",
            "val x (8, 3400)\n",
            "trainy (1, 13600)\n",
            "val y (1, 3400)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iterations=2000\n",
        "parameters, history=create_nn_model(train_X,train_Y,50, val_X, val_Y, iterations, 1e-2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ib9P0_cJdhkl",
        "outputId": "e12b440c-64d1-4611-e4b5-231e1a965903"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 0 :train_loss:5.6630377769470215 val_loss5.55766487121582\n",
            "iteration 1 :train_loss:5.487386226654053 val_loss5.3834547996521\n",
            "iteration 2 :train_loss:5.318718910217285 val_loss5.216200828552246\n",
            "iteration 3 :train_loss:5.156501770019531 val_loss5.055371284484863\n",
            "iteration 4 :train_loss:5.000255584716797 val_loss4.90048885345459\n",
            "iteration 5 :train_loss:4.849534511566162 val_loss4.751113414764404\n",
            "iteration 6 :train_loss:4.703921794891357 val_loss4.6068267822265625\n",
            "iteration 7 :train_loss:4.563014507293701 val_loss4.467231273651123\n",
            "iteration 8 :train_loss:4.4264235496521 val_loss4.331939220428467\n",
            "iteration 9 :train_loss:4.293766498565674 val_loss4.2005720138549805\n",
            "iteration 10 :train_loss:4.164668083190918 val_loss4.072756767272949\n",
            "iteration 11 :train_loss:4.038759231567383 val_loss3.9481279850006104\n",
            "iteration 12 :train_loss:3.915675640106201 val_loss3.8263251781463623\n",
            "iteration 13 :train_loss:3.7950644493103027 val_loss3.7069990634918213\n",
            "iteration 14 :train_loss:3.6765849590301514 val_loss3.589812755584717\n",
            "iteration 15 :train_loss:3.5599160194396973 val_loss3.4744479656219482\n",
            "iteration 16 :train_loss:3.444761276245117 val_loss3.3606152534484863\n",
            "iteration 17 :train_loss:3.330859422683716 val_loss3.2480556964874268\n",
            "iteration 18 :train_loss:3.2179946899414062 val_loss3.1365575790405273\n",
            "iteration 19 :train_loss:3.1060070991516113 val_loss3.025965929031372\n",
            "iteration 20 :train_loss:2.994807481765747 val_loss2.916194200515747\n",
            "iteration 21 :train_loss:2.8843870162963867 val_loss2.807236909866333\n",
            "iteration 22 :train_loss:2.774832248687744 val_loss2.6991822719573975\n",
            "iteration 23 :train_loss:2.6663315296173096 val_loss2.5922210216522217\n",
            "iteration 24 :train_loss:2.559183120727539 val_loss2.4866509437561035\n",
            "iteration 25 :train_loss:2.45379376411438 val_loss2.3828768730163574\n",
            "iteration 26 :train_loss:2.350672960281372 val_loss2.2814059257507324\n",
            "iteration 27 :train_loss:2.2504165172576904 val_loss2.182827949523926\n",
            "iteration 28 :train_loss:2.1536834239959717 val_loss2.0877957344055176\n",
            "iteration 29 :train_loss:2.0611627101898193 val_loss1.996990442276001\n",
            "iteration 30 :train_loss:1.9735361337661743 val_loss1.9110827445983887\n",
            "iteration 31 :train_loss:1.8914344310760498 val_loss1.830692172050476\n",
            "iteration 32 :train_loss:1.8153969049453735 val_loss1.7563459873199463\n",
            "iteration 33 :train_loss:1.745835304260254 val_loss1.688442349433899\n",
            "iteration 34 :train_loss:1.6830048561096191 val_loss1.6272251605987549\n",
            "iteration 35 :train_loss:1.6269912719726562 val_loss1.5727671384811401\n",
            "iteration 36 :train_loss:1.5777084827423096 val_loss1.5249710083007812\n",
            "iteration 37 :train_loss:1.5349106788635254 val_loss1.4835830926895142\n",
            "iteration 38 :train_loss:1.4982174634933472 val_loss1.4482146501541138\n",
            "iteration 39 :train_loss:1.4671435356140137 val_loss1.4183745384216309\n",
            "iteration 40 :train_loss:1.4411325454711914 val_loss1.3935054540634155\n",
            "iteration 41 :train_loss:1.4195955991744995 val_loss1.3730155229568481\n",
            "iteration 42 :train_loss:1.4019384384155273 val_loss1.35631263256073\n",
            "iteration 43 :train_loss:1.3875890970230103 val_loss1.3428258895874023\n",
            "iteration 44 :train_loss:1.376015067100525 val_loss1.3320271968841553\n",
            "iteration 45 :train_loss:1.3667364120483398 val_loss1.3234416246414185\n",
            "iteration 46 :train_loss:1.359331488609314 val_loss1.3166524171829224\n",
            "iteration 47 :train_loss:1.3534375429153442 val_loss1.3113031387329102\n",
            "iteration 48 :train_loss:1.3487498760223389 val_loss1.307094693183899\n",
            "iteration 49 :train_loss:1.3450161218643188 val_loss1.303780436515808\n",
            "iteration 50 :train_loss:1.3420295715332031 val_loss1.3011603355407715\n",
            "iteration 51 :train_loss:1.3396246433258057 val_loss1.2990742921829224\n",
            "iteration 52 :train_loss:1.3376694917678833 val_loss1.29739511013031\n",
            "iteration 53 :train_loss:1.3360600471496582 val_loss1.2960237264633179\n",
            "iteration 54 :train_loss:1.3347148895263672 val_loss1.294883370399475\n",
            "iteration 55 :train_loss:1.3335710763931274 val_loss1.2939153909683228\n",
            "iteration 56 :train_loss:1.3325804471969604 val_loss1.2930744886398315\n",
            "iteration 57 :train_loss:1.331705093383789 val_loss1.2923266887664795\n",
            "iteration 58 :train_loss:1.3309175968170166 val_loss1.2916470766067505\n",
            "iteration 59 :train_loss:1.3301957845687866 val_loss1.2910162210464478\n",
            "iteration 60 :train_loss:1.3295234441757202 val_loss1.2904201745986938\n",
            "iteration 61 :train_loss:1.3288887739181519 val_loss1.2898491621017456\n",
            "iteration 62 :train_loss:1.328282356262207 val_loss1.289294958114624\n",
            "iteration 63 :train_loss:1.3276971578598022 val_loss1.2887526750564575\n",
            "iteration 64 :train_loss:1.3271280527114868 val_loss1.288218379020691\n",
            "iteration 65 :train_loss:1.3265711069107056 val_loss1.287688970565796\n",
            "iteration 66 :train_loss:1.3260236978530884 val_loss1.2871628999710083\n",
            "iteration 67 :train_loss:1.3254834413528442 val_loss1.286638855934143\n",
            "iteration 68 :train_loss:1.324948787689209 val_loss1.2861155271530151\n",
            "iteration 69 :train_loss:1.3244186639785767 val_loss1.2855931520462036\n",
            "iteration 70 :train_loss:1.3238918781280518 val_loss1.2850706577301025\n",
            "iteration 71 :train_loss:1.3233681917190552 val_loss1.284548282623291\n",
            "iteration 72 :train_loss:1.3228466510772705 val_loss1.28402578830719\n",
            "iteration 73 :train_loss:1.3223271369934082 val_loss1.2835031747817993\n",
            "iteration 74 :train_loss:1.3218095302581787 val_loss1.2829805612564087\n",
            "iteration 75 :train_loss:1.3212932348251343 val_loss1.2824574708938599\n",
            "iteration 76 :train_loss:1.320778250694275 val_loss1.2819347381591797\n",
            "iteration 77 :train_loss:1.3202648162841797 val_loss1.2814120054244995\n",
            "iteration 78 :train_loss:1.3197520971298218 val_loss1.2808892726898193\n",
            "iteration 79 :train_loss:1.3192408084869385 val_loss1.2803670167922974\n",
            "iteration 80 :train_loss:1.3187304735183716 val_loss1.279844880104065\n",
            "iteration 81 :train_loss:1.3182209730148315 val_loss1.2793229818344116\n",
            "iteration 82 :train_loss:1.3177125453948975 val_loss1.278801679611206\n",
            "iteration 83 :train_loss:1.3172047138214111 val_loss1.2782808542251587\n",
            "iteration 84 :train_loss:1.3166981935501099 val_loss1.27776038646698\n",
            "iteration 85 :train_loss:1.3161925077438354 val_loss1.2772403955459595\n",
            "iteration 86 :train_loss:1.3156874179840088 val_loss1.2767210006713867\n",
            "iteration 87 :train_loss:1.315183401107788 val_loss1.2762020826339722\n",
            "iteration 88 :train_loss:1.3146799802780151 val_loss1.275684118270874\n",
            "iteration 89 :train_loss:1.3141775131225586 val_loss1.275166630744934\n",
            "iteration 90 :train_loss:1.313675880432129 val_loss1.2746496200561523\n",
            "iteration 91 :train_loss:1.3131749629974365 val_loss1.274133324623108\n",
            "iteration 92 :train_loss:1.3126747608184814 val_loss1.2736176252365112\n",
            "iteration 93 :train_loss:1.3121753931045532 val_loss1.2731026411056519\n",
            "iteration 94 :train_loss:1.3116768598556519 val_loss1.2725883722305298\n",
            "iteration 95 :train_loss:1.3111788034439087 val_loss1.272074580192566\n",
            "iteration 96 :train_loss:1.310681700706482 val_loss1.271561622619629\n",
            "iteration 97 :train_loss:1.3101853132247925 val_loss1.2710493803024292\n",
            "iteration 98 :train_loss:1.3096897602081299 val_loss1.2705377340316772\n",
            "iteration 99 :train_loss:1.309194803237915 val_loss1.270026683807373\n",
            "iteration 100 :train_loss:1.308700442314148 val_loss1.2695164680480957\n",
            "iteration 101 :train_loss:1.3082070350646973 val_loss1.2690067291259766\n",
            "iteration 102 :train_loss:1.3077139854431152 val_loss1.2684978246688843\n",
            "iteration 103 :train_loss:1.3072220087051392 val_loss1.2679893970489502\n",
            "iteration 104 :train_loss:1.3067303895950317 val_loss1.2674815654754639\n",
            "iteration 105 :train_loss:1.306239366531372 val_loss1.266974687576294\n",
            "iteration 106 :train_loss:1.3057494163513184 val_loss1.2664682865142822\n",
            "iteration 107 :train_loss:1.3052597045898438 val_loss1.2659624814987183\n",
            "iteration 108 :train_loss:1.3047709465026855 val_loss1.265457272529602\n",
            "iteration 109 :train_loss:1.304282546043396 val_loss1.2649527788162231\n",
            "iteration 110 :train_loss:1.3037948608398438 val_loss1.2644487619400024\n",
            "iteration 111 :train_loss:1.303308129310608 val_loss1.2639455795288086\n",
            "iteration 112 :train_loss:1.302821397781372 val_loss1.2634426355361938\n",
            "iteration 113 :train_loss:1.3023358583450317 val_loss1.2629406452178955\n",
            "iteration 114 :train_loss:1.3018505573272705 val_loss1.2624391317367554\n",
            "iteration 115 :train_loss:1.3013660907745361 val_loss1.2619383335113525\n",
            "iteration 116 :train_loss:1.300882339477539 val_loss1.2614378929138184\n",
            "iteration 117 :train_loss:1.3003989458084106 val_loss1.2609381675720215\n",
            "iteration 118 :train_loss:1.29991614818573 val_loss1.2604389190673828\n",
            "iteration 119 :train_loss:1.2994341850280762 val_loss1.259940505027771\n",
            "iteration 120 :train_loss:1.298952341079712 val_loss1.2594424486160278\n",
            "iteration 121 :train_loss:1.298471450805664 val_loss1.2589449882507324\n",
            "iteration 122 :train_loss:1.2979909181594849 val_loss1.2584480047225952\n",
            "iteration 123 :train_loss:1.2975112199783325 val_loss1.2579518556594849\n",
            "iteration 124 :train_loss:1.2970318794250488 val_loss1.257455825805664\n",
            "iteration 125 :train_loss:1.2965528964996338 val_loss1.2569606304168701\n",
            "iteration 126 :train_loss:1.2960747480392456 val_loss1.2564657926559448\n",
            "iteration 127 :train_loss:1.2955971956253052 val_loss1.2559716701507568\n",
            "iteration 128 :train_loss:1.2951200008392334 val_loss1.2554781436920166\n",
            "iteration 129 :train_loss:1.294643521308899 val_loss1.2549850940704346\n",
            "iteration 130 :train_loss:1.2941675186157227 val_loss1.254492163658142\n",
            "iteration 131 :train_loss:1.2936919927597046 val_loss1.2540003061294556\n",
            "iteration 132 :train_loss:1.2932170629501343 val_loss1.2535086870193481\n",
            "iteration 133 :train_loss:1.2927427291870117 val_loss1.2530179023742676\n",
            "iteration 134 :train_loss:1.2922686338424683 val_loss1.2525274753570557\n",
            "iteration 135 :train_loss:1.2917954921722412 val_loss1.2520372867584229\n",
            "iteration 136 :train_loss:1.2913228273391724 val_loss1.2515480518341064\n",
            "iteration 137 :train_loss:1.2908505201339722 val_loss1.2510590553283691\n",
            "iteration 138 :train_loss:1.2903786897659302 val_loss1.2505707740783691\n",
            "iteration 139 :train_loss:1.289907455444336 val_loss1.2500827312469482\n",
            "iteration 140 :train_loss:1.2894366979599 val_loss1.2495954036712646\n",
            "iteration 141 :train_loss:1.2889665365219116 val_loss1.2491084337234497\n",
            "iteration 142 :train_loss:1.2884966135025024 val_loss1.2486220598220825\n",
            "iteration 143 :train_loss:1.2880276441574097 val_loss1.2481361627578735\n",
            "iteration 144 :train_loss:1.287559151649475 val_loss1.2476507425308228\n",
            "iteration 145 :train_loss:1.2870908975601196 val_loss1.2471659183502197\n",
            "iteration 146 :train_loss:1.286623239517212 val_loss1.2466816902160645\n",
            "iteration 147 :train_loss:1.2861560583114624 val_loss1.2461977005004883\n",
            "iteration 148 :train_loss:1.2856894731521606 val_loss1.2457143068313599\n",
            "iteration 149 :train_loss:1.2852234840393066 val_loss1.2452313899993896\n",
            "iteration 150 :train_loss:1.2847577333450317 val_loss1.2447490692138672\n",
            "iteration 151 :train_loss:1.2842926979064941 val_loss1.2442671060562134\n",
            "iteration 152 :train_loss:1.2838281393051147 val_loss1.2437858581542969\n",
            "iteration 153 :train_loss:1.283363938331604 val_loss1.243304967880249\n",
            "iteration 154 :train_loss:1.2829006910324097 val_loss1.2428245544433594\n",
            "iteration 155 :train_loss:1.2824374437332153 val_loss1.242344617843628\n",
            "iteration 156 :train_loss:1.2819749116897583 val_loss1.2418652772903442\n",
            "iteration 157 :train_loss:1.2815128564834595 val_loss1.2413864135742188\n",
            "iteration 158 :train_loss:1.2810513973236084 val_loss1.240907907485962\n",
            "iteration 159 :train_loss:1.280590534210205 val_loss1.2404299974441528\n",
            "iteration 160 :train_loss:1.2801297903060913 val_loss1.239952564239502\n",
            "iteration 161 :train_loss:1.2796698808670044 val_loss1.2394758462905884\n",
            "iteration 162 :train_loss:1.2792103290557861 val_loss1.238999366760254\n",
            "iteration 163 :train_loss:1.2787514925003052 val_loss1.2385234832763672\n",
            "iteration 164 :train_loss:1.2782930135726929 val_loss1.2380481958389282\n",
            "iteration 165 :train_loss:1.2778348922729492 val_loss1.237573266029358\n",
            "iteration 166 :train_loss:1.2773776054382324 val_loss1.2370986938476562\n",
            "iteration 167 :train_loss:1.2769206762313843 val_loss1.236625075340271\n",
            "iteration 168 :train_loss:1.2764642238616943 val_loss1.2361515760421753\n",
            "iteration 169 :train_loss:1.2760084867477417 val_loss1.2356786727905273\n",
            "iteration 170 :train_loss:1.2755532264709473 val_loss1.2352064847946167\n",
            "iteration 171 :train_loss:1.275098204612732 val_loss1.2347347736358643\n",
            "iteration 172 :train_loss:1.274644136428833 val_loss1.23426353931427\n",
            "iteration 173 :train_loss:1.2741903066635132 val_loss1.233792781829834\n",
            "iteration 174 :train_loss:1.2737370729446411 val_loss1.2333226203918457\n",
            "iteration 175 :train_loss:1.2732844352722168 val_loss1.2328526973724365\n",
            "iteration 176 :train_loss:1.2728322744369507 val_loss1.2323837280273438\n",
            "iteration 177 :train_loss:1.2723808288574219 val_loss1.2319151163101196\n",
            "iteration 178 :train_loss:1.2719297409057617 val_loss1.2314468622207642\n",
            "iteration 179 :train_loss:1.2714792490005493 val_loss1.2309794425964355\n",
            "iteration 180 :train_loss:1.2710293531417847 val_loss1.2305123805999756\n",
            "iteration 181 :train_loss:1.2705800533294678 val_loss1.2300461530685425\n",
            "iteration 182 :train_loss:1.270131230354309 val_loss1.229580044746399\n",
            "iteration 183 :train_loss:1.2696828842163086 val_loss1.2291147708892822\n",
            "iteration 184 :train_loss:1.2692351341247559 val_loss1.2286498546600342\n",
            "iteration 185 :train_loss:1.2687878608703613 val_loss1.228185772895813\n",
            "iteration 186 :train_loss:1.2683414220809937 val_loss1.2277220487594604\n",
            "iteration 187 :train_loss:1.2678954601287842 val_loss1.2272590398788452\n",
            "iteration 188 :train_loss:1.2674503326416016 val_loss1.2267966270446777\n",
            "iteration 189 :train_loss:1.267005443572998 val_loss1.2263346910476685\n",
            "iteration 190 :train_loss:1.2665610313415527 val_loss1.2258734703063965\n",
            "iteration 191 :train_loss:1.2661173343658447 val_loss1.2254124879837036\n",
            "iteration 192 :train_loss:1.265674352645874 val_loss1.2249523401260376\n",
            "iteration 193 :train_loss:1.2652320861816406 val_loss1.2244927883148193\n",
            "iteration 194 :train_loss:1.2647901773452759 val_loss1.2240338325500488\n",
            "iteration 195 :train_loss:1.2643488645553589 val_loss1.2235753536224365\n",
            "iteration 196 :train_loss:1.2639082670211792 val_loss1.2231178283691406\n",
            "iteration 197 :train_loss:1.2634681463241577 val_loss1.2226605415344238\n",
            "iteration 198 :train_loss:1.2630289793014526 val_loss1.2222042083740234\n",
            "iteration 199 :train_loss:1.2625902891159058 val_loss1.2217479944229126\n",
            "iteration 200 :train_loss:1.2621521949768066 val_loss1.2212929725646973\n",
            "iteration 201 :train_loss:1.2617145776748657 val_loss1.2208384275436401\n",
            "iteration 202 :train_loss:1.261277675628662 val_loss1.2203843593597412\n",
            "iteration 203 :train_loss:1.2608416080474854 val_loss1.2199311256408691\n",
            "iteration 204 :train_loss:1.2604061365127563 val_loss1.2194783687591553\n",
            "iteration 205 :train_loss:1.259971261024475 val_loss1.2190264463424683\n",
            "iteration 206 :train_loss:1.2595369815826416 val_loss1.218575119972229\n",
            "iteration 207 :train_loss:1.259103536605835 val_loss1.2181241512298584\n",
            "iteration 208 :train_loss:1.258670687675476 val_loss1.2176743745803833\n",
            "iteration 209 :train_loss:1.258238434791565 val_loss1.2172249555587769\n",
            "iteration 210 :train_loss:1.2578070163726807 val_loss1.2167764902114868\n",
            "iteration 211 :train_loss:1.2573760747909546 val_loss1.2163283824920654\n",
            "iteration 212 :train_loss:1.256946086883545 val_loss1.2158812284469604\n",
            "iteration 213 :train_loss:1.2565168142318726 val_loss1.2154345512390137\n",
            "iteration 214 :train_loss:1.2560880184173584 val_loss1.2149888277053833\n",
            "iteration 215 :train_loss:1.255660057067871 val_loss1.2145439386367798\n",
            "iteration 216 :train_loss:1.255232810974121 val_loss1.2140992879867554\n",
            "iteration 217 :train_loss:1.2548062801361084 val_loss1.2136558294296265\n",
            "iteration 218 :train_loss:1.254380464553833 val_loss1.2132128477096558\n",
            "iteration 219 :train_loss:1.253955364227295 val_loss1.212770700454712\n",
            "iteration 220 :train_loss:1.2535310983657837 val_loss1.2123292684555054\n",
            "iteration 221 :train_loss:1.2531074285507202 val_loss1.2118885517120361\n",
            "iteration 222 :train_loss:1.2526847124099731 val_loss1.2114487886428833\n",
            "iteration 223 :train_loss:1.252263069152832 val_loss1.2110096216201782\n",
            "iteration 224 :train_loss:1.2518415451049805 val_loss1.2105712890625\n",
            "iteration 225 :train_loss:1.2514210939407349 val_loss1.210133671760559\n",
            "iteration 226 :train_loss:1.2510015964508057 val_loss1.2096971273422241\n",
            "iteration 227 :train_loss:1.2505825757980347 val_loss1.2092612981796265\n",
            "iteration 228 :train_loss:1.250164270401001 val_loss1.208825945854187\n",
            "iteration 229 :train_loss:1.2497472763061523 val_loss1.2083916664123535\n",
            "iteration 230 :train_loss:1.249330759048462 val_loss1.2079582214355469\n",
            "iteration 231 :train_loss:1.248915195465088 val_loss1.207525610923767\n",
            "iteration 232 :train_loss:1.2485003471374512 val_loss1.2070938348770142\n",
            "iteration 233 :train_loss:1.2480862140655518 val_loss1.2066627740859985\n",
            "iteration 234 :train_loss:1.2476731538772583 val_loss1.2062326669692993\n",
            "iteration 235 :train_loss:1.2472609281539917 val_loss1.205803394317627\n",
            "iteration 236 :train_loss:1.246849536895752 val_loss1.2053749561309814\n",
            "iteration 237 :train_loss:1.2464388608932495 val_loss1.2049474716186523\n",
            "iteration 238 :train_loss:1.246029257774353 val_loss1.2045209407806396\n",
            "iteration 239 :train_loss:1.2456203699111938 val_loss1.2040950059890747\n",
            "iteration 240 :train_loss:1.2452125549316406 val_loss1.2036700248718262\n",
            "iteration 241 :train_loss:1.2448055744171143 val_loss1.2032462358474731\n",
            "iteration 242 :train_loss:1.2443994283676147 val_loss1.2028230428695679\n",
            "iteration 243 :train_loss:1.243994116783142 val_loss1.202401041984558\n",
            "iteration 244 :train_loss:1.2435898780822754 val_loss1.2019797563552856\n",
            "iteration 245 :train_loss:1.2431864738464355 val_loss1.2015595436096191\n",
            "iteration 246 :train_loss:1.242783784866333 val_loss1.201140284538269\n",
            "iteration 247 :train_loss:1.242382526397705 val_loss1.2007219791412354\n",
            "iteration 248 :train_loss:1.241981863975525 val_loss1.2003045082092285\n",
            "iteration 249 :train_loss:1.2415822744369507 val_loss1.199887990951538\n",
            "iteration 250 :train_loss:1.2411835193634033 val_loss1.1994726657867432\n",
            "iteration 251 :train_loss:1.2407855987548828 val_loss1.1990580558776855\n",
            "iteration 252 :train_loss:1.2403889894485474 val_loss1.1986446380615234\n",
            "iteration 253 :train_loss:1.2399934530258179 val_loss1.1982320547103882\n",
            "iteration 254 :train_loss:1.2395986318588257 val_loss1.1978206634521484\n",
            "iteration 255 :train_loss:1.2392046451568604 val_loss1.197410225868225\n",
            "iteration 256 :train_loss:1.2388120889663696 val_loss1.1970007419586182\n",
            "iteration 257 :train_loss:1.2384202480316162 val_loss1.1965923309326172\n",
            "iteration 258 :train_loss:1.2380295991897583 val_loss1.1961849927902222\n",
            "iteration 259 :train_loss:1.2376397848129272 val_loss1.1957786083221436\n",
            "iteration 260 :train_loss:1.2372512817382812 val_loss1.1953734159469604\n",
            "iteration 261 :train_loss:1.2368634939193726 val_loss1.1949691772460938\n",
            "iteration 262 :train_loss:1.236477017402649 val_loss1.1945661306381226\n",
            "iteration 263 :train_loss:1.2360914945602417 val_loss1.1941639184951782\n",
            "iteration 264 :train_loss:1.23570716381073 val_loss1.1937628984451294\n",
            "iteration 265 :train_loss:1.235323429107666 val_loss1.1933633089065552\n",
            "iteration 266 :train_loss:1.2349413633346558 val_loss1.1929643154144287\n",
            "iteration 267 :train_loss:1.2345600128173828 val_loss1.1925666332244873\n",
            "iteration 268 :train_loss:1.234179973602295 val_loss1.1921700239181519\n",
            "iteration 269 :train_loss:1.2338008880615234 val_loss1.191774606704712\n",
            "iteration 270 :train_loss:1.233422875404358 val_loss1.191380262374878\n",
            "iteration 271 :train_loss:1.233046054840088 val_loss1.1909871101379395\n",
            "iteration 272 :train_loss:1.2326704263687134 val_loss1.1905951499938965\n",
            "iteration 273 :train_loss:1.2322957515716553 val_loss1.1902042627334595\n",
            "iteration 274 :train_loss:1.2319223880767822 val_loss1.189814567565918\n",
            "iteration 275 :train_loss:1.2315500974655151 val_loss1.1894261837005615\n",
            "iteration 276 :train_loss:1.2311787605285645 val_loss1.1890387535095215\n",
            "iteration 277 :train_loss:1.2308090925216675 val_loss1.188652515411377\n",
            "iteration 278 :train_loss:1.2304402589797974 val_loss1.188267707824707\n",
            "iteration 279 :train_loss:1.2300724983215332 val_loss1.187883973121643\n",
            "iteration 280 :train_loss:1.2297061681747437 val_loss1.187501311302185\n",
            "iteration 281 :train_loss:1.2293407917022705 val_loss1.1871198415756226\n",
            "iteration 282 :train_loss:1.2289767265319824 val_loss1.1867398023605347\n",
            "iteration 283 :train_loss:1.2286138534545898 val_loss1.1863608360290527\n",
            "iteration 284 :train_loss:1.2282522916793823 val_loss1.1859831809997559\n",
            "iteration 285 :train_loss:1.2278918027877808 val_loss1.1856069564819336\n",
            "iteration 286 :train_loss:1.2275325059890747 val_loss1.1852316856384277\n",
            "iteration 287 :train_loss:1.2271745204925537 val_loss1.1848578453063965\n",
            "iteration 288 :train_loss:1.2268177270889282 val_loss1.1844851970672607\n",
            "iteration 289 :train_loss:1.2264620065689087 val_loss1.1841137409210205\n",
            "iteration 290 :train_loss:1.2261079549789429 val_loss1.1837437152862549\n",
            "iteration 291 :train_loss:1.225754976272583 val_loss1.1833750009536743\n",
            "iteration 292 :train_loss:1.2254033088684082 val_loss1.1830073595046997\n",
            "iteration 293 :train_loss:1.2250525951385498 val_loss1.1826411485671997\n",
            "iteration 294 :train_loss:1.2247034311294556 val_loss1.1822763681411743\n",
            "iteration 295 :train_loss:1.2243554592132568 val_loss1.1819127798080444\n",
            "iteration 296 :train_loss:1.2240087985992432 val_loss1.18155038356781\n",
            "iteration 297 :train_loss:1.2236632108688354 val_loss1.1811896562576294\n",
            "iteration 298 :train_loss:1.223319172859192 val_loss1.1808301210403442\n",
            "iteration 299 :train_loss:1.2229764461517334 val_loss1.1804717779159546\n",
            "iteration 300 :train_loss:1.22263503074646 val_loss1.1801148653030396\n",
            "iteration 301 :train_loss:1.2222949266433716 val_loss1.1797592639923096\n",
            "iteration 302 :train_loss:1.2219561338424683 val_loss1.179404854774475\n",
            "iteration 303 :train_loss:1.2216185331344604 val_loss1.1790521144866943\n",
            "iteration 304 :train_loss:1.2212824821472168 val_loss1.1787006855010986\n",
            "iteration 305 :train_loss:1.220947504043579 val_loss1.1783509254455566\n",
            "iteration 306 :train_loss:1.2206140756607056 val_loss1.1780019998550415\n",
            "iteration 307 :train_loss:1.2202820777893066 val_loss1.1776546239852905\n",
            "iteration 308 :train_loss:1.2199511528015137 val_loss1.1773086786270142\n",
            "iteration 309 :train_loss:1.2196214199066162 val_loss1.176964282989502\n",
            "iteration 310 :train_loss:1.2192937135696411 val_loss1.1766210794448853\n",
            "iteration 311 :train_loss:1.2189671993255615 val_loss1.1762794256210327\n",
            "iteration 312 :train_loss:1.218641757965088 val_loss1.1759392023086548\n",
            "iteration 313 :train_loss:1.2183177471160889 val_loss1.175600290298462\n",
            "iteration 314 :train_loss:1.2179951667785645 val_loss1.1752628087997437\n",
            "iteration 315 :train_loss:1.2176740169525146 val_loss1.1749268770217896\n",
            "iteration 316 :train_loss:1.21735417842865 val_loss1.1745922565460205\n",
            "iteration 317 :train_loss:1.2170358896255493 val_loss1.1742589473724365\n",
            "iteration 318 :train_loss:1.2167189121246338 val_loss1.1739273071289062\n",
            "iteration 319 :train_loss:1.2164032459259033 val_loss1.1735972166061401\n",
            "iteration 320 :train_loss:1.216089129447937 val_loss1.1732683181762695\n",
            "iteration 321 :train_loss:1.2157765626907349 val_loss1.172940969467163\n",
            "iteration 322 :train_loss:1.2154650688171387 val_loss1.1726150512695312\n",
            "iteration 323 :train_loss:1.2151552438735962 val_loss1.1722908020019531\n",
            "iteration 324 :train_loss:1.2148467302322388 val_loss1.17196786403656\n",
            "iteration 325 :train_loss:1.2145395278930664 val_loss1.1716463565826416\n",
            "iteration 326 :train_loss:1.2142341136932373 val_loss1.1713262796401978\n",
            "iteration 327 :train_loss:1.2139301300048828 val_loss1.171007752418518\n",
            "iteration 328 :train_loss:1.2136270999908447 val_loss1.1706907749176025\n",
            "iteration 329 :train_loss:1.21332585811615 val_loss1.1703753471374512\n",
            "iteration 330 :train_loss:1.2130260467529297 val_loss1.1700613498687744\n",
            "iteration 331 :train_loss:1.212727665901184 val_loss1.1697487831115723\n",
            "iteration 332 :train_loss:1.2124308347702026 val_loss1.1694377660751343\n",
            "iteration 333 :train_loss:1.2121353149414062 val_loss1.1691282987594604\n",
            "iteration 334 :train_loss:1.2118412256240845 val_loss1.1688202619552612\n",
            "iteration 335 :train_loss:1.2115488052368164 val_loss1.1685136556625366\n",
            "iteration 336 :train_loss:1.2112575769424438 val_loss1.1682088375091553\n",
            "iteration 337 :train_loss:1.210968255996704 val_loss1.167905330657959\n",
            "iteration 338 :train_loss:1.2106801271438599 val_loss1.1676033735275269\n",
            "iteration 339 :train_loss:1.2103931903839111 val_loss1.1673029661178589\n",
            "iteration 340 :train_loss:1.2101081609725952 val_loss1.167004108428955\n",
            "iteration 341 :train_loss:1.209824562072754 val_loss1.1667065620422363\n",
            "iteration 342 :train_loss:1.209542155265808 val_loss1.1664108037948608\n",
            "iteration 343 :train_loss:1.209261417388916 val_loss1.16611647605896\n",
            "iteration 344 :train_loss:1.2089821100234985 val_loss1.1658238172531128\n",
            "iteration 345 :train_loss:1.2087044715881348 val_loss1.1655324697494507\n",
            "iteration 346 :train_loss:1.208428144454956 val_loss1.1652427911758423\n",
            "iteration 347 :train_loss:1.208153486251831 val_loss1.164954662322998\n",
            "iteration 348 :train_loss:1.2078800201416016 val_loss1.164668083190918\n",
            "iteration 349 :train_loss:1.2076083421707153 val_loss1.1643829345703125\n",
            "iteration 350 :train_loss:1.207337737083435 val_loss1.1640994548797607\n",
            "iteration 351 :train_loss:1.2070690393447876 val_loss1.1638174057006836\n",
            "iteration 352 :train_loss:1.2068017721176147 val_loss1.1635370254516602\n",
            "iteration 353 :train_loss:1.2065359354019165 val_loss1.1632579565048218\n",
            "iteration 354 :train_loss:1.2062715291976929 val_loss1.1629806756973267\n",
            "iteration 355 :train_loss:1.2060086727142334 val_loss1.1627049446105957\n",
            "iteration 356 :train_loss:1.205747365951538 val_loss1.1624306440353394\n",
            "iteration 357 :train_loss:1.205487608909607 val_loss1.1621577739715576\n",
            "iteration 358 :train_loss:1.2052291631698608 val_loss1.1618868112564087\n",
            "iteration 359 :train_loss:1.204972267150879 val_loss1.1616171598434448\n",
            "iteration 360 :train_loss:1.2047169208526611 val_loss1.1613489389419556\n",
            "iteration 361 :train_loss:1.204463005065918 val_loss1.1610825061798096\n",
            "iteration 362 :train_loss:1.204210638999939 val_loss1.1608175039291382\n",
            "iteration 363 :train_loss:1.2039598226547241 val_loss1.1605541706085205\n",
            "iteration 364 :train_loss:1.2037105560302734 val_loss1.160292387008667\n",
            "iteration 365 :train_loss:1.2034626007080078 val_loss1.160032033920288\n",
            "iteration 366 :train_loss:1.2032161951065063 val_loss1.1597731113433838\n",
            "iteration 367 :train_loss:1.202971339225769 val_loss1.1595159769058228\n",
            "iteration 368 :train_loss:1.202728033065796 val_loss1.1592603921890259\n",
            "iteration 369 :train_loss:1.2024861574172974 val_loss1.1590062379837036\n",
            "iteration 370 :train_loss:1.2022457122802734 val_loss1.1587536334991455\n",
            "iteration 371 :train_loss:1.2020068168640137 val_loss1.1585025787353516\n",
            "iteration 372 :train_loss:1.2017693519592285 val_loss1.1582531929016113\n",
            "iteration 373 :train_loss:1.201533317565918 val_loss1.1580053567886353\n",
            "iteration 374 :train_loss:1.2012989521026611 val_loss1.1577588319778442\n",
            "iteration 375 :train_loss:1.2010658979415894 val_loss1.157513976097107\n",
            "iteration 376 :train_loss:1.2008345127105713 val_loss1.1572706699371338\n",
            "iteration 377 :train_loss:1.2006045579910278 val_loss1.1570287942886353\n",
            "iteration 378 :train_loss:1.2003761529922485 val_loss1.1567885875701904\n",
            "iteration 379 :train_loss:1.2001490592956543 val_loss1.1565498113632202\n",
            "iteration 380 :train_loss:1.1999236345291138 val_loss1.1563125848770142\n",
            "iteration 381 :train_loss:1.1996994018554688 val_loss1.1560770273208618\n",
            "iteration 382 :train_loss:1.199476957321167 val_loss1.155842900276184\n",
            "iteration 383 :train_loss:1.1992557048797607 val_loss1.155610203742981\n",
            "iteration 384 :train_loss:1.1990361213684082 val_loss1.1553791761398315\n",
            "iteration 385 :train_loss:1.1988179683685303 val_loss1.1551496982574463\n",
            "iteration 386 :train_loss:1.198601245880127 val_loss1.1549214124679565\n",
            "iteration 387 :train_loss:1.1983859539031982 val_loss1.1546951532363892\n",
            "iteration 388 :train_loss:1.1981720924377441 val_loss1.1544698476791382\n",
            "iteration 389 :train_loss:1.1979596614837646 val_loss1.1542465686798096\n",
            "iteration 390 :train_loss:1.1977487802505493 val_loss1.154024600982666\n",
            "iteration 391 :train_loss:1.1975393295288086 val_loss1.1538039445877075\n",
            "iteration 392 :train_loss:1.1973313093185425 val_loss1.1535848379135132\n",
            "iteration 393 :train_loss:1.1971246004104614 val_loss1.1533674001693726\n",
            "iteration 394 :train_loss:1.196919560432434 val_loss1.1531513929367065\n",
            "iteration 395 :train_loss:1.1967158317565918 val_loss1.1529370546340942\n",
            "iteration 396 :train_loss:1.1965135335922241 val_loss1.152723789215088\n",
            "iteration 397 :train_loss:1.1963127851486206 val_loss1.1525123119354248\n",
            "iteration 398 :train_loss:1.1961132287979126 val_loss1.1523021459579468\n",
            "iteration 399 :train_loss:1.1959153413772583 val_loss1.1520934104919434\n",
            "iteration 400 :train_loss:1.195718765258789 val_loss1.1518864631652832\n",
            "iteration 401 :train_loss:1.1955235004425049 val_loss1.1516807079315186\n",
            "iteration 402 :train_loss:1.1953297853469849 val_loss1.151476502418518\n",
            "iteration 403 :train_loss:1.19513738155365 val_loss1.1512737274169922\n",
            "iteration 404 :train_loss:1.194946527481079 val_loss1.151072382926941\n",
            "iteration 405 :train_loss:1.1947567462921143 val_loss1.1508725881576538\n",
            "iteration 406 :train_loss:1.1945687532424927 val_loss1.1506739854812622\n",
            "iteration 407 :train_loss:1.1943819522857666 val_loss1.1504771709442139\n",
            "iteration 408 :train_loss:1.1941964626312256 val_loss1.150281548500061\n",
            "iteration 409 :train_loss:1.1940125226974487 val_loss1.1500874757766724\n",
            "iteration 410 :train_loss:1.193829894065857 val_loss1.1498948335647583\n",
            "iteration 411 :train_loss:1.1936485767364502 val_loss1.1497033834457397\n",
            "iteration 412 :train_loss:1.193468689918518 val_loss1.1495137214660645\n",
            "iteration 413 :train_loss:1.1932899951934814 val_loss1.1493251323699951\n",
            "iteration 414 :train_loss:1.193112850189209 val_loss1.149138331413269\n",
            "iteration 415 :train_loss:1.1929370164871216 val_loss1.148952603340149\n",
            "iteration 416 :train_loss:1.1927624940872192 val_loss1.148768424987793\n",
            "iteration 417 :train_loss:1.192589282989502 val_loss1.148585557937622\n",
            "iteration 418 :train_loss:1.1924173831939697 val_loss1.1484042406082153\n",
            "iteration 419 :train_loss:1.192246913909912 val_loss1.1482239961624146\n",
            "iteration 420 :train_loss:1.1920775175094604 val_loss1.148045301437378\n",
            "iteration 421 :train_loss:1.1919097900390625 val_loss1.1478679180145264\n",
            "iteration 422 :train_loss:1.1917431354522705 val_loss1.1476919651031494\n",
            "iteration 423 :train_loss:1.1915779113769531 val_loss1.1475173234939575\n",
            "iteration 424 :train_loss:1.1914138793945312 val_loss1.1473439931869507\n",
            "iteration 425 :train_loss:1.1912513971328735 val_loss1.147172212600708\n",
            "iteration 426 :train_loss:1.1910897493362427 val_loss1.1470016241073608\n",
            "iteration 427 :train_loss:1.1909297704696655 val_loss1.1468323469161987\n",
            "iteration 428 :train_loss:1.1907708644866943 val_loss1.1466643810272217\n",
            "iteration 429 :train_loss:1.1906133890151978 val_loss1.1464978456497192\n",
            "iteration 430 :train_loss:1.1904571056365967 val_loss1.1463326215744019\n",
            "iteration 431 :train_loss:1.1903021335601807 val_loss1.14616858959198\n",
            "iteration 432 :train_loss:1.1901483535766602 val_loss1.1460059881210327\n",
            "iteration 433 :train_loss:1.1899956464767456 val_loss1.145844578742981\n",
            "iteration 434 :train_loss:1.1898444890975952 val_loss1.1456844806671143\n",
            "iteration 435 :train_loss:1.1896944046020508 val_loss1.1455258131027222\n",
            "iteration 436 :train_loss:1.1895455121994019 val_loss1.145368218421936\n",
            "iteration 437 :train_loss:1.1893980503082275 val_loss1.145211935043335\n",
            "iteration 438 :train_loss:1.1892515420913696 val_loss1.145056962966919\n",
            "iteration 439 :train_loss:1.1891064643859863 val_loss1.144903302192688\n",
            "iteration 440 :train_loss:1.188962459564209 val_loss1.144750952720642\n",
            "iteration 441 :train_loss:1.1888196468353271 val_loss1.1445995569229126\n",
            "iteration 442 :train_loss:1.18867826461792 val_loss1.1444498300552368\n",
            "iteration 443 :train_loss:1.188537836074829 val_loss1.1443010568618774\n",
            "iteration 444 :train_loss:1.1883984804153442 val_loss1.1441535949707031\n",
            "iteration 445 :train_loss:1.1882603168487549 val_loss1.1440073251724243\n",
            "iteration 446 :train_loss:1.1881235837936401 val_loss1.143862247467041\n",
            "iteration 447 :train_loss:1.1879878044128418 val_loss1.1437183618545532\n",
            "iteration 448 :train_loss:1.1878533363342285 val_loss1.1435757875442505\n",
            "iteration 449 :train_loss:1.1877198219299316 val_loss1.1434341669082642\n",
            "iteration 450 :train_loss:1.1875876188278198 val_loss1.143293857574463\n",
            "iteration 451 :train_loss:1.1874563694000244 val_loss1.1431547403335571\n",
            "iteration 452 :train_loss:1.187326431274414 val_loss1.1430168151855469\n",
            "iteration 453 :train_loss:1.1871975660324097 val_loss1.1428800821304321\n",
            "iteration 454 :train_loss:1.1870697736740112 val_loss1.142744541168213\n",
            "iteration 455 :train_loss:1.1869430541992188 val_loss1.1426101922988892\n",
            "iteration 456 :train_loss:1.1868174076080322 val_loss1.1424769163131714\n",
            "iteration 457 :train_loss:1.1866929531097412 val_loss1.14234459400177\n",
            "iteration 458 :train_loss:1.1865695714950562 val_loss1.1422134637832642\n",
            "iteration 459 :train_loss:1.1864471435546875 val_loss1.1420836448669434\n",
            "iteration 460 :train_loss:1.186326026916504 val_loss1.141955018043518\n",
            "iteration 461 :train_loss:1.1862058639526367 val_loss1.1418272256851196\n",
            "iteration 462 :train_loss:1.1860865354537964 val_loss1.1417007446289062\n",
            "iteration 463 :train_loss:1.1859685182571411 val_loss1.1415753364562988\n",
            "iteration 464 :train_loss:1.1858515739440918 val_loss1.1414508819580078\n",
            "iteration 465 :train_loss:1.1857355833053589 val_loss1.1413273811340332\n",
            "iteration 466 :train_loss:1.1856204271316528 val_loss1.1412053108215332\n",
            "iteration 467 :train_loss:1.1855064630508423 val_loss1.14108407497406\n",
            "iteration 468 :train_loss:1.1853934526443481 val_loss1.1409640312194824\n",
            "iteration 469 :train_loss:1.18528151512146 val_loss1.1408448219299316\n",
            "iteration 470 :train_loss:1.1851706504821777 val_loss1.140726923942566\n",
            "iteration 471 :train_loss:1.185060739517212 val_loss1.1406099796295166\n",
            "iteration 472 :train_loss:1.184951901435852 val_loss1.1404939889907837\n",
            "iteration 473 :train_loss:1.184843897819519 val_loss1.1403790712356567\n",
            "iteration 474 :train_loss:1.1847368478775024 val_loss1.1402652263641357\n",
            "iteration 475 :train_loss:1.1846308708190918 val_loss1.1401523351669312\n",
            "iteration 476 :train_loss:1.184525728225708 val_loss1.140040397644043\n",
            "iteration 477 :train_loss:1.1844216585159302 val_loss1.1399296522140503\n",
            "iteration 478 :train_loss:1.1843184232711792 val_loss1.1398197412490845\n",
            "iteration 479 :train_loss:1.1842162609100342 val_loss1.1397109031677246\n",
            "iteration 480 :train_loss:1.1841150522232056 val_loss1.1396030187606812\n",
            "iteration 481 :train_loss:1.1840146780014038 val_loss1.1394959688186646\n",
            "iteration 482 :train_loss:1.183915138244629 val_loss1.1393898725509644\n",
            "iteration 483 :train_loss:1.18381667137146 val_loss1.1392848491668701\n",
            "iteration 484 :train_loss:1.1837188005447388 val_loss1.1391806602478027\n",
            "iteration 485 :train_loss:1.1836223602294922 val_loss1.1390775442123413\n",
            "iteration 486 :train_loss:1.1835265159606934 val_loss1.1389752626419067\n",
            "iteration 487 :train_loss:1.1834315061569214 val_loss1.1388740539550781\n",
            "iteration 488 :train_loss:1.1833375692367554 val_loss1.1387736797332764\n",
            "iteration 489 :train_loss:1.183244228363037 val_loss1.1386741399765015\n",
            "iteration 490 :train_loss:1.1831519603729248 val_loss1.138575553894043\n",
            "iteration 491 :train_loss:1.1830604076385498 val_loss1.1384780406951904\n",
            "iteration 492 :train_loss:1.1829699277877808 val_loss1.1383812427520752\n",
            "iteration 493 :train_loss:1.1828800439834595 val_loss1.1382852792739868\n",
            "iteration 494 :train_loss:1.1827911138534546 val_loss1.1381901502609253\n",
            "iteration 495 :train_loss:1.182702898979187 val_loss1.1380959749221802\n",
            "iteration 496 :train_loss:1.1826156377792358 val_loss1.1380027532577515\n",
            "iteration 497 :train_loss:1.1825292110443115 val_loss1.1379104852676392\n",
            "iteration 498 :train_loss:1.182443618774414 val_loss1.137818694114685\n",
            "iteration 499 :train_loss:1.182358741760254 val_loss1.1377280950546265\n",
            "iteration 500 :train_loss:1.182274580001831 val_loss1.1376380920410156\n",
            "iteration 501 :train_loss:1.1821913719177246 val_loss1.1375489234924316\n",
            "iteration 502 :train_loss:1.182108759880066 val_loss1.1374608278274536\n",
            "iteration 503 :train_loss:1.1820271015167236 val_loss1.1373733282089233\n",
            "iteration 504 :train_loss:1.1819462776184082 val_loss1.1372867822647095\n",
            "iteration 505 :train_loss:1.1818660497665405 val_loss1.1372005939483643\n",
            "iteration 506 :train_loss:1.1817866563796997 val_loss1.1371158361434937\n",
            "iteration 507 :train_loss:1.1817080974578857 val_loss1.1370316743850708\n",
            "iteration 508 :train_loss:1.18163001537323 val_loss1.1369479894638062\n",
            "iteration 509 :train_loss:1.181552767753601 val_loss1.136865258216858\n",
            "iteration 510 :train_loss:1.181476354598999 val_loss1.1367833614349365\n",
            "iteration 511 :train_loss:1.1814007759094238 val_loss1.1367021799087524\n",
            "iteration 512 :train_loss:1.1813256740570068 val_loss1.1366218328475952\n",
            "iteration 513 :train_loss:1.1812512874603271 val_loss1.1365420818328857\n",
            "iteration 514 :train_loss:1.1811777353286743 val_loss1.1364630460739136\n",
            "iteration 515 :train_loss:1.1811048984527588 val_loss1.1363848447799683\n",
            "iteration 516 :train_loss:1.1810325384140015 val_loss1.1363074779510498\n",
            "iteration 517 :train_loss:1.18096125125885 val_loss1.136230707168579\n",
            "iteration 518 :train_loss:1.180890440940857 val_loss1.1361546516418457\n",
            "iteration 519 :train_loss:1.180820345878601 val_loss1.1360793113708496\n",
            "iteration 520 :train_loss:1.180750846862793 val_loss1.1360045671463013\n",
            "iteration 521 :train_loss:1.180681824684143 val_loss1.1359307765960693\n",
            "iteration 522 :train_loss:1.1806137561798096 val_loss1.1358574628829956\n",
            "iteration 523 :train_loss:1.1805462837219238 val_loss1.1357851028442383\n",
            "iteration 524 :train_loss:1.1804794073104858 val_loss1.13571298122406\n",
            "iteration 525 :train_loss:1.1804132461547852 val_loss1.1356418132781982\n",
            "iteration 526 :train_loss:1.1803475618362427 val_loss1.1355712413787842\n",
            "iteration 527 :train_loss:1.1802825927734375 val_loss1.1355013847351074\n",
            "iteration 528 :train_loss:1.1802183389663696 val_loss1.1354320049285889\n",
            "iteration 529 :train_loss:1.1801543235778809 val_loss1.1353635787963867\n",
            "iteration 530 :train_loss:1.180091381072998 val_loss1.1352956295013428\n",
            "iteration 531 :train_loss:1.1800289154052734 val_loss1.1352282762527466\n",
            "iteration 532 :train_loss:1.1799668073654175 val_loss1.1351615190505981\n",
            "iteration 533 :train_loss:1.179905652999878 val_loss1.1350955963134766\n",
            "iteration 534 :train_loss:1.179844856262207 val_loss1.1350302696228027\n",
            "iteration 535 :train_loss:1.1797845363616943 val_loss1.134965419769287\n",
            "iteration 536 :train_loss:1.179724931716919 val_loss1.1349010467529297\n",
            "iteration 537 :train_loss:1.1796659231185913 val_loss1.1348373889923096\n",
            "iteration 538 :train_loss:1.1796075105667114 val_loss1.1347744464874268\n",
            "iteration 539 :train_loss:1.1795496940612793 val_loss1.1347118616104126\n",
            "iteration 540 :train_loss:1.1794923543930054 val_loss1.1346499919891357\n",
            "iteration 541 :train_loss:1.1794354915618896 val_loss1.1345888376235962\n",
            "iteration 542 :train_loss:1.1793793439865112 val_loss1.1345279216766357\n",
            "iteration 543 :train_loss:1.1793235540390015 val_loss1.1344677209854126\n",
            "iteration 544 :train_loss:1.179268479347229 val_loss1.1344083547592163\n",
            "iteration 545 :train_loss:1.1792137622833252 val_loss1.1343493461608887\n",
            "iteration 546 :train_loss:1.1791596412658691 val_loss1.1342906951904297\n",
            "iteration 547 :train_loss:1.1791058778762817 val_loss1.1342326402664185\n",
            "iteration 548 :train_loss:1.1790528297424316 val_loss1.134175419807434\n",
            "iteration 549 :train_loss:1.1790001392364502 val_loss1.1341183185577393\n",
            "iteration 550 :train_loss:1.178947925567627 val_loss1.1340621709823608\n",
            "iteration 551 :train_loss:1.178896427154541 val_loss1.1340062618255615\n",
            "iteration 552 :train_loss:1.1788451671600342 val_loss1.13395094871521\n",
            "iteration 553 :train_loss:1.1787946224212646 val_loss1.1338961124420166\n",
            "iteration 554 :train_loss:1.1787444353103638 val_loss1.1338417530059814\n",
            "iteration 555 :train_loss:1.178694725036621 val_loss1.1337881088256836\n",
            "iteration 556 :train_loss:1.1786454916000366 val_loss1.1337345838546753\n",
            "iteration 557 :train_loss:1.1785967350006104 val_loss1.1336816549301147\n",
            "iteration 558 :train_loss:1.1785483360290527 val_loss1.133629560470581\n",
            "iteration 559 :train_loss:1.1785005331039429 val_loss1.133577585220337\n",
            "iteration 560 :train_loss:1.1784532070159912 val_loss1.133526086807251\n",
            "iteration 561 :train_loss:1.1784062385559082 val_loss1.1334753036499023\n",
            "iteration 562 :train_loss:1.1783596277236938 val_loss1.1334248781204224\n",
            "iteration 563 :train_loss:1.1783136129379272 val_loss1.133374810218811\n",
            "iteration 564 :train_loss:1.1782678365707397 val_loss1.133325219154358\n",
            "iteration 565 :train_loss:1.17822265625 val_loss1.1332762241363525\n",
            "iteration 566 :train_loss:1.178177833557129 val_loss1.1332275867462158\n",
            "iteration 567 :train_loss:1.1781336069107056 val_loss1.1331793069839478\n",
            "iteration 568 :train_loss:1.1780894994735718 val_loss1.133131504058838\n",
            "iteration 569 :train_loss:1.1780459880828857 val_loss1.1330841779708862\n",
            "iteration 570 :train_loss:1.1780028343200684 val_loss1.1330372095108032\n",
            "iteration 571 :train_loss:1.1779600381851196 val_loss1.1329909563064575\n",
            "iteration 572 :train_loss:1.1779175996780396 val_loss1.132944941520691\n",
            "iteration 573 :train_loss:1.1778757572174072 val_loss1.1328990459442139\n",
            "iteration 574 :train_loss:1.1778340339660645 val_loss1.1328538656234741\n",
            "iteration 575 :train_loss:1.1777927875518799 val_loss1.132809042930603\n",
            "iteration 576 :train_loss:1.177752137184143 val_loss1.1327646970748901\n",
            "iteration 577 :train_loss:1.1777114868164062 val_loss1.1327204704284668\n",
            "iteration 578 :train_loss:1.1776714324951172 val_loss1.1326768398284912\n",
            "iteration 579 :train_loss:1.1776317358016968 val_loss1.1326336860656738\n",
            "iteration 580 :train_loss:1.177592396736145 val_loss1.132590889930725\n",
            "iteration 581 :train_loss:1.1775532960891724 val_loss1.132548213005066\n",
            "iteration 582 :train_loss:1.177514672279358 val_loss1.1325061321258545\n",
            "iteration 583 :train_loss:1.177476406097412 val_loss1.1324644088745117\n",
            "iteration 584 :train_loss:1.177438497543335 val_loss1.1324230432510376\n",
            "iteration 585 :train_loss:1.1774009466171265 val_loss1.1323819160461426\n",
            "iteration 586 :train_loss:1.1773635149002075 val_loss1.1323412656784058\n",
            "iteration 587 :train_loss:1.1773265600204468 val_loss1.1323009729385376\n",
            "iteration 588 :train_loss:1.1772898435592651 val_loss1.132261037826538\n",
            "iteration 589 :train_loss:1.1772537231445312 val_loss1.1322214603424072\n",
            "iteration 590 :train_loss:1.177217721939087 val_loss1.132182240486145\n",
            "iteration 591 :train_loss:1.1771820783615112 val_loss1.132143259048462\n",
            "iteration 592 :train_loss:1.1771469116210938 val_loss1.132104516029358\n",
            "iteration 593 :train_loss:1.1771118640899658 val_loss1.1320664882659912\n",
            "iteration 594 :train_loss:1.177077054977417 val_loss1.132028341293335\n",
            "iteration 595 :train_loss:1.1770427227020264 val_loss1.1319907903671265\n",
            "iteration 596 :train_loss:1.1770085096359253 val_loss1.1319535970687866\n",
            "iteration 597 :train_loss:1.1769746541976929 val_loss1.1319165229797363\n",
            "iteration 598 :train_loss:1.176941156387329 val_loss1.1318799257278442\n",
            "iteration 599 :train_loss:1.176908016204834 val_loss1.1318435668945312\n",
            "iteration 600 :train_loss:1.176875114440918 val_loss1.1318074464797974\n",
            "iteration 601 :train_loss:1.176842451095581 val_loss1.1317715644836426\n",
            "iteration 602 :train_loss:1.1768101453781128 val_loss1.1317362785339355\n",
            "iteration 603 :train_loss:1.176777958869934 val_loss1.1317009925842285\n",
            "iteration 604 :train_loss:1.176746129989624 val_loss1.1316663026809692\n",
            "iteration 605 :train_loss:1.1767144203186035 val_loss1.1316317319869995\n",
            "iteration 606 :train_loss:1.1766831874847412 val_loss1.1315975189208984\n",
            "iteration 607 :train_loss:1.1766523122787476 val_loss1.1315633058547974\n",
            "iteration 608 :train_loss:1.1766215562820435 val_loss1.1315295696258545\n",
            "iteration 609 :train_loss:1.1765910387039185 val_loss1.1314963102340698\n",
            "iteration 610 :train_loss:1.1765607595443726 val_loss1.1314631700515747\n",
            "iteration 611 :train_loss:1.1765308380126953 val_loss1.1314302682876587\n",
            "iteration 612 :train_loss:1.176500916481018 val_loss1.1313976049423218\n",
            "iteration 613 :train_loss:1.176471471786499 val_loss1.1313650608062744\n",
            "iteration 614 :train_loss:1.176442265510559 val_loss1.1313331127166748\n",
            "iteration 615 :train_loss:1.1764132976531982 val_loss1.1313011646270752\n",
            "iteration 616 :train_loss:1.1763845682144165 val_loss1.1312695741653442\n",
            "iteration 617 :train_loss:1.1763560771942139 val_loss1.1312382221221924\n",
            "iteration 618 :train_loss:1.1763278245925903 val_loss1.1312071084976196\n",
            "iteration 619 :train_loss:1.1762994527816772 val_loss1.131176233291626\n",
            "iteration 620 :train_loss:1.176271915435791 val_loss1.1311455965042114\n",
            "iteration 621 :train_loss:1.1762441396713257 val_loss1.1311153173446655\n",
            "iteration 622 :train_loss:1.176216721534729 val_loss1.1310851573944092\n",
            "iteration 623 :train_loss:1.176189661026001 val_loss1.1310551166534424\n",
            "iteration 624 :train_loss:1.1761624813079834 val_loss1.1310254335403442\n",
            "iteration 625 :train_loss:1.176135778427124 val_loss1.1309959888458252\n",
            "iteration 626 :train_loss:1.1761090755462646 val_loss1.1309667825698853\n",
            "iteration 627 :train_loss:1.176082968711853 val_loss1.1309378147125244\n",
            "iteration 628 :train_loss:1.1760568618774414 val_loss1.1309090852737427\n",
            "iteration 629 :train_loss:1.1760308742523193 val_loss1.130880355834961\n",
            "iteration 630 :train_loss:1.1760050058364868 val_loss1.130852222442627\n",
            "iteration 631 :train_loss:1.1759796142578125 val_loss1.1308239698410034\n",
            "iteration 632 :train_loss:1.1759543418884277 val_loss1.1307960748672485\n",
            "iteration 633 :train_loss:1.1759291887283325 val_loss1.1307682991027832\n",
            "iteration 634 :train_loss:1.1759041547775269 val_loss1.1307408809661865\n",
            "iteration 635 :train_loss:1.1758792400360107 val_loss1.1307133436203003\n",
            "iteration 636 :train_loss:1.1758548021316528 val_loss1.1306862831115723\n",
            "iteration 637 :train_loss:1.175830364227295 val_loss1.1306594610214233\n",
            "iteration 638 :train_loss:1.1758061647415161 val_loss1.1306326389312744\n",
            "iteration 639 :train_loss:1.1757820844650269 val_loss1.1306061744689941\n",
            "iteration 640 :train_loss:1.1757582426071167 val_loss1.1305797100067139\n",
            "iteration 641 :train_loss:1.175734519958496 val_loss1.1305537223815918\n",
            "iteration 642 :train_loss:1.1757110357284546 val_loss1.1305277347564697\n",
            "iteration 643 :train_loss:1.175687551498413 val_loss1.1305019855499268\n",
            "iteration 644 :train_loss:1.1756645441055298 val_loss1.130476474761963\n",
            "iteration 645 :train_loss:1.1756415367126465 val_loss1.1304508447647095\n",
            "iteration 646 :train_loss:1.1756185293197632 val_loss1.1304256916046143\n",
            "iteration 647 :train_loss:1.175595998764038 val_loss1.1304006576538086\n",
            "iteration 648 :train_loss:1.175573468208313 val_loss1.130375623703003\n",
            "iteration 649 :train_loss:1.1755510568618774 val_loss1.130350947380066\n",
            "iteration 650 :train_loss:1.175528883934021 val_loss1.1303263902664185\n",
            "iteration 651 :train_loss:1.1755067110061646 val_loss1.130301833152771\n",
            "iteration 652 :train_loss:1.1754847764968872 val_loss1.1302778720855713\n",
            "iteration 653 :train_loss:1.1754629611968994 val_loss1.1302536725997925\n",
            "iteration 654 :train_loss:1.1754415035247803 val_loss1.1302298307418823\n",
            "iteration 655 :train_loss:1.1754200458526611 val_loss1.1302061080932617\n",
            "iteration 656 :train_loss:1.175398826599121 val_loss1.1301823854446411\n",
            "iteration 657 :train_loss:1.175377368927002 val_loss1.1301590204238892\n",
            "iteration 658 :train_loss:1.175356388092041 val_loss1.1301356554031372\n",
            "iteration 659 :train_loss:1.1753356456756592 val_loss1.130112648010254\n",
            "iteration 660 :train_loss:1.1753147840499878 val_loss1.1300896406173706\n",
            "iteration 661 :train_loss:1.1752941608428955 val_loss1.1300667524337769\n",
            "iteration 662 :train_loss:1.1752736568450928 val_loss1.1300441026687622\n",
            "iteration 663 :train_loss:1.17525315284729 val_loss1.130021572113037\n",
            "iteration 664 :train_loss:1.1752331256866455 val_loss1.129999041557312\n",
            "iteration 665 :train_loss:1.1752129793167114 val_loss1.129976749420166\n",
            "iteration 666 :train_loss:1.175192952156067 val_loss1.1299546957015991\n",
            "iteration 667 :train_loss:1.1751731634140015 val_loss1.1299327611923218\n",
            "iteration 668 :train_loss:1.175153374671936 val_loss1.1299108266830444\n",
            "iteration 669 :train_loss:1.1751338243484497 val_loss1.1298891305923462\n",
            "iteration 670 :train_loss:1.1751142740249634 val_loss1.129867434501648\n",
            "iteration 671 :train_loss:1.1750949621200562 val_loss1.1298460960388184\n",
            "iteration 672 :train_loss:1.175075650215149 val_loss1.1298246383666992\n",
            "iteration 673 :train_loss:1.1750564575195312 val_loss1.1298034191131592\n",
            "iteration 674 :train_loss:1.1750375032424927 val_loss1.1297823190689087\n",
            "iteration 675 :train_loss:1.1750186681747437 val_loss1.1297613382339478\n",
            "iteration 676 :train_loss:1.1749998331069946 val_loss1.1297404766082764\n",
            "iteration 677 :train_loss:1.1749811172485352 val_loss1.129719853401184\n",
            "iteration 678 :train_loss:1.1749624013900757 val_loss1.1296991109848022\n",
            "iteration 679 :train_loss:1.1749441623687744 val_loss1.129678726196289\n",
            "iteration 680 :train_loss:1.1749255657196045 val_loss1.1296583414077759\n",
            "iteration 681 :train_loss:1.1749073266983032 val_loss1.1296379566192627\n",
            "iteration 682 :train_loss:1.1748894453048706 val_loss1.1296179294586182\n",
            "iteration 683 :train_loss:1.1748712062835693 val_loss1.1295979022979736\n",
            "iteration 684 :train_loss:1.1748532056808472 val_loss1.129577875137329\n",
            "iteration 685 :train_loss:1.174835443496704 val_loss1.1295580863952637\n",
            "iteration 686 :train_loss:1.174817681312561 val_loss1.1295384168624878\n",
            "iteration 687 :train_loss:1.174799919128418 val_loss1.1295186281204224\n",
            "iteration 688 :train_loss:1.174782395362854 val_loss1.1294993162155151\n",
            "iteration 689 :train_loss:1.17476487159729 val_loss1.1294798851013184\n",
            "iteration 690 :train_loss:1.1747475862503052 val_loss1.1294605731964111\n",
            "iteration 691 :train_loss:1.1747303009033203 val_loss1.1294413805007935\n",
            "iteration 692 :train_loss:1.1747130155563354 val_loss1.1294223070144653\n",
            "iteration 693 :train_loss:1.1746958494186401 val_loss1.1294032335281372\n",
            "iteration 694 :train_loss:1.174678921699524 val_loss1.1293842792510986\n",
            "iteration 695 :train_loss:1.1746619939804077 val_loss1.1293655633926392\n",
            "iteration 696 :train_loss:1.174645185470581 val_loss1.1293467283248901\n",
            "iteration 697 :train_loss:1.1746283769607544 val_loss1.1293281316757202\n",
            "iteration 698 :train_loss:1.1746116876602173 val_loss1.1293095350265503\n",
            "iteration 699 :train_loss:1.1745949983596802 val_loss1.1292911767959595\n",
            "iteration 700 :train_loss:1.1745785474777222 val_loss1.1292729377746582\n",
            "iteration 701 :train_loss:1.1745622158050537 val_loss1.1292544603347778\n",
            "iteration 702 :train_loss:1.1745456457138062 val_loss1.1292364597320557\n",
            "iteration 703 :train_loss:1.1745295524597168 val_loss1.1292182207107544\n",
            "iteration 704 :train_loss:1.1745132207870483 val_loss1.1292002201080322\n",
            "iteration 705 :train_loss:1.1744970083236694 val_loss1.1291823387145996\n",
            "iteration 706 :train_loss:1.1744810342788696 val_loss1.1291645765304565\n",
            "iteration 707 :train_loss:1.1744650602340698 val_loss1.1291468143463135\n",
            "iteration 708 :train_loss:1.1744492053985596 val_loss1.1291290521621704\n",
            "iteration 709 :train_loss:1.1744333505630493 val_loss1.1291115283966064\n",
            "iteration 710 :train_loss:1.1744176149368286 val_loss1.1290940046310425\n",
            "iteration 711 :train_loss:1.174401879310608 val_loss1.1290764808654785\n",
            "iteration 712 :train_loss:1.1743860244750977 val_loss1.1290591955184937\n",
            "iteration 713 :train_loss:1.1743706464767456 val_loss1.1290417909622192\n",
            "iteration 714 :train_loss:1.174355149269104 val_loss1.129024624824524\n",
            "iteration 715 :train_loss:1.1743396520614624 val_loss1.1290074586868286\n",
            "iteration 716 :train_loss:1.1743242740631104 val_loss1.1289905309677124\n",
            "iteration 717 :train_loss:1.1743091344833374 val_loss1.1289734840393066\n",
            "iteration 718 :train_loss:1.174293875694275 val_loss1.1289565563201904\n",
            "iteration 719 :train_loss:1.1742786169052124 val_loss1.1289397478103638\n",
            "iteration 720 :train_loss:1.17426335811615 val_loss1.1289228200912476\n",
            "iteration 721 :train_loss:1.1742483377456665 val_loss1.1289061307907104\n",
            "iteration 722 :train_loss:1.1742335557937622 val_loss1.1288894414901733\n",
            "iteration 723 :train_loss:1.1742184162139893 val_loss1.1288729906082153\n",
            "iteration 724 :train_loss:1.174203634262085 val_loss1.1288565397262573\n",
            "iteration 725 :train_loss:1.1741888523101807 val_loss1.1288398504257202\n",
            "iteration 726 :train_loss:1.174174189567566 val_loss1.1288233995437622\n",
            "iteration 727 :train_loss:1.1741594076156616 val_loss1.1288071870803833\n",
            "iteration 728 :train_loss:1.1741447448730469 val_loss1.1287909746170044\n",
            "iteration 729 :train_loss:1.1741300821304321 val_loss1.128774642944336\n",
            "iteration 730 :train_loss:1.1741156578063965 val_loss1.1287585496902466\n",
            "iteration 731 :train_loss:1.1741011142730713 val_loss1.1287425756454468\n",
            "iteration 732 :train_loss:1.1740868091583252 val_loss1.1287263631820679\n",
            "iteration 733 :train_loss:1.1740723848342896 val_loss1.1287105083465576\n",
            "iteration 734 :train_loss:1.1740580797195435 val_loss1.1286946535110474\n",
            "iteration 735 :train_loss:1.1740437746047974 val_loss1.1286786794662476\n",
            "iteration 736 :train_loss:1.1740294694900513 val_loss1.1286630630493164\n",
            "iteration 737 :train_loss:1.1740152835845947 val_loss1.1286470890045166\n",
            "iteration 738 :train_loss:1.1740013360977173 val_loss1.1286317110061646\n",
            "iteration 739 :train_loss:1.1739871501922607 val_loss1.1286157369613647\n",
            "iteration 740 :train_loss:1.1739732027053833 val_loss1.1286002397537231\n",
            "iteration 741 :train_loss:1.1739591360092163 val_loss1.128584861755371\n",
            "iteration 742 :train_loss:1.1739451885223389 val_loss1.128569483757019\n",
            "iteration 743 :train_loss:1.1739312410354614 val_loss1.1285539865493774\n",
            "iteration 744 :train_loss:1.173917293548584 val_loss1.1285384893417358\n",
            "iteration 745 :train_loss:1.1739035844802856 val_loss1.1285232305526733\n",
            "iteration 746 :train_loss:1.1738898754119873 val_loss1.1285079717636108\n",
            "iteration 747 :train_loss:1.1738762855529785 val_loss1.1284925937652588\n",
            "iteration 748 :train_loss:1.1738625764846802 val_loss1.1284775733947754\n",
            "iteration 749 :train_loss:1.1738487482070923 val_loss1.1284624338150024\n",
            "iteration 750 :train_loss:1.1738351583480835 val_loss1.1284472942352295\n",
            "iteration 751 :train_loss:1.1738216876983643 val_loss1.1284323930740356\n",
            "iteration 752 :train_loss:1.1738083362579346 val_loss1.1284173727035522\n",
            "iteration 753 :train_loss:1.1737946271896362 val_loss1.1284024715423584\n",
            "iteration 754 :train_loss:1.1737812757492065 val_loss1.1283875703811646\n",
            "iteration 755 :train_loss:1.1737679243087769 val_loss1.1283726692199707\n",
            "iteration 756 :train_loss:1.1737544536590576 val_loss1.1283577680587769\n",
            "iteration 757 :train_loss:1.1737412214279175 val_loss1.1283429861068726\n",
            "iteration 758 :train_loss:1.1737278699874878 val_loss1.1283283233642578\n",
            "iteration 759 :train_loss:1.1737147569656372 val_loss1.128313660621643\n",
            "iteration 760 :train_loss:1.1737016439437866 val_loss1.1282991170883179\n",
            "iteration 761 :train_loss:1.173688292503357 val_loss1.1282844543457031\n",
            "iteration 762 :train_loss:1.1736751794815063 val_loss1.128269910812378\n",
            "iteration 763 :train_loss:1.1736620664596558 val_loss1.1282554864883423\n",
            "iteration 764 :train_loss:1.1736490726470947 val_loss1.128240942955017\n",
            "iteration 765 :train_loss:1.1736359596252441 val_loss1.1282265186309814\n",
            "iteration 766 :train_loss:1.1736228466033936 val_loss1.1282119750976562\n",
            "iteration 767 :train_loss:1.1736098527908325 val_loss1.1281976699829102\n",
            "iteration 768 :train_loss:1.1735972166061401 val_loss1.128183364868164\n",
            "iteration 769 :train_loss:1.1735841035842896 val_loss1.1281689405441284\n",
            "iteration 770 :train_loss:1.1735713481903076 val_loss1.1281547546386719\n",
            "iteration 771 :train_loss:1.1735584735870361 val_loss1.1281405687332153\n",
            "iteration 772 :train_loss:1.1735455989837646 val_loss1.1281262636184692\n",
            "iteration 773 :train_loss:1.1735328435897827 val_loss1.1281123161315918\n",
            "iteration 774 :train_loss:1.1735202074050903 val_loss1.1280981302261353\n",
            "iteration 775 :train_loss:1.1735073328018188 val_loss1.1280841827392578\n",
            "iteration 776 :train_loss:1.1734946966171265 val_loss1.1280698776245117\n",
            "iteration 777 :train_loss:1.173482060432434 val_loss1.1280560493469238\n",
            "iteration 778 :train_loss:1.1734694242477417 val_loss1.1280421018600464\n",
            "iteration 779 :train_loss:1.1734567880630493 val_loss1.128028154373169\n",
            "iteration 780 :train_loss:1.173444151878357 val_loss1.1280142068862915\n",
            "iteration 781 :train_loss:1.173431634902954 val_loss1.1280003786087036\n",
            "iteration 782 :train_loss:1.1734192371368408 val_loss1.1279864311218262\n",
            "iteration 783 :train_loss:1.1734066009521484 val_loss1.1279724836349487\n",
            "iteration 784 :train_loss:1.1733942031860352 val_loss1.12795889377594\n",
            "iteration 785 :train_loss:1.1733816862106323 val_loss1.127945065498352\n",
            "iteration 786 :train_loss:1.173369288444519 val_loss1.1279313564300537\n",
            "iteration 787 :train_loss:1.1733568906784058 val_loss1.1279175281524658\n",
            "iteration 788 :train_loss:1.1733444929122925 val_loss1.127903938293457\n",
            "iteration 789 :train_loss:1.1733322143554688 val_loss1.1278902292251587\n",
            "iteration 790 :train_loss:1.173319935798645 val_loss1.12787663936615\n",
            "iteration 791 :train_loss:1.1733076572418213 val_loss1.1278630495071411\n",
            "iteration 792 :train_loss:1.1732953786849976 val_loss1.1278494596481323\n",
            "iteration 793 :train_loss:1.1732829809188843 val_loss1.127835988998413\n",
            "iteration 794 :train_loss:1.1732709407806396 val_loss1.1278225183486938\n",
            "iteration 795 :train_loss:1.1732585430145264 val_loss1.127808928489685\n",
            "iteration 796 :train_loss:1.1732465028762817 val_loss1.1277955770492554\n",
            "iteration 797 :train_loss:1.1732343435287476 val_loss1.1277822256088257\n",
            "iteration 798 :train_loss:1.1732221841812134 val_loss1.127768635749817\n",
            "iteration 799 :train_loss:1.1732100248336792 val_loss1.1277551651000977\n",
            "iteration 800 :train_loss:1.1731979846954346 val_loss1.127741813659668\n",
            "iteration 801 :train_loss:1.17318594455719 val_loss1.1277285814285278\n",
            "iteration 802 :train_loss:1.1731737852096558 val_loss1.1277152299880981\n",
            "iteration 803 :train_loss:1.1731617450714111 val_loss1.1277018785476685\n",
            "iteration 804 :train_loss:1.173149585723877 val_loss1.1276887655258179\n",
            "iteration 805 :train_loss:1.1731376647949219 val_loss1.1276756525039673\n",
            "iteration 806 :train_loss:1.1731257438659668 val_loss1.1276623010635376\n",
            "iteration 807 :train_loss:1.1731138229370117 val_loss1.127649188041687\n",
            "iteration 808 :train_loss:1.1731019020080566 val_loss1.1276359558105469\n",
            "iteration 809 :train_loss:1.173089861869812 val_loss1.1276229619979858\n",
            "iteration 810 :train_loss:1.173077940940857 val_loss1.1276097297668457\n",
            "iteration 811 :train_loss:1.1730661392211914 val_loss1.1275966167449951\n",
            "iteration 812 :train_loss:1.1730542182922363 val_loss1.127583384513855\n",
            "iteration 813 :train_loss:1.1730424165725708 val_loss1.1275705099105835\n",
            "iteration 814 :train_loss:1.1730304956436157 val_loss1.127557396888733\n",
            "iteration 815 :train_loss:1.1730186939239502 val_loss1.1275445222854614\n",
            "iteration 816 :train_loss:1.1730068922042847 val_loss1.1275314092636108\n",
            "iteration 817 :train_loss:1.1729952096939087 val_loss1.1275184154510498\n",
            "iteration 818 :train_loss:1.1729835271835327 val_loss1.1275054216384888\n",
            "iteration 819 :train_loss:1.1729717254638672 val_loss1.1274925470352173\n",
            "iteration 820 :train_loss:1.1729601621627808 val_loss1.1274797916412354\n",
            "iteration 821 :train_loss:1.1729482412338257 val_loss1.1274667978286743\n",
            "iteration 822 :train_loss:1.1729365587234497 val_loss1.1274539232254028\n",
            "iteration 823 :train_loss:1.1729249954223633 val_loss1.1274410486221313\n",
            "iteration 824 :train_loss:1.1729131937026978 val_loss1.1274281740188599\n",
            "iteration 825 :train_loss:1.1729015111923218 val_loss1.127415418624878\n",
            "iteration 826 :train_loss:1.1728898286819458 val_loss1.1274027824401855\n",
            "iteration 827 :train_loss:1.172878384590149 val_loss1.127389907836914\n",
            "iteration 828 :train_loss:1.172866702079773 val_loss1.1273770332336426\n",
            "iteration 829 :train_loss:1.172855019569397 val_loss1.1273643970489502\n",
            "iteration 830 :train_loss:1.1728435754776 val_loss1.1273517608642578\n",
            "iteration 831 :train_loss:1.1728318929672241 val_loss1.1273386478424072\n",
            "iteration 832 :train_loss:1.1728204488754272 val_loss1.1273261308670044\n",
            "iteration 833 :train_loss:1.1728088855743408 val_loss1.127313494682312\n",
            "iteration 834 :train_loss:1.1727973222732544 val_loss1.1273008584976196\n",
            "iteration 835 :train_loss:1.1727858781814575 val_loss1.1272883415222168\n",
            "iteration 836 :train_loss:1.172774314880371 val_loss1.1272755861282349\n",
            "iteration 837 :train_loss:1.1727628707885742 val_loss1.1272629499435425\n",
            "iteration 838 :train_loss:1.1727514266967773 val_loss1.1272504329681396\n",
            "iteration 839 :train_loss:1.172739863395691 val_loss1.1272377967834473\n",
            "iteration 840 :train_loss:1.1727285385131836 val_loss1.1272251605987549\n",
            "iteration 841 :train_loss:1.1727169752120972 val_loss1.1272127628326416\n",
            "iteration 842 :train_loss:1.1727056503295898 val_loss1.1272000074386597\n",
            "iteration 843 :train_loss:1.172694206237793 val_loss1.1271874904632568\n",
            "iteration 844 :train_loss:1.172682762145996 val_loss1.127174973487854\n",
            "iteration 845 :train_loss:1.1726713180541992 val_loss1.1271624565124512\n",
            "iteration 846 :train_loss:1.1726601123809814 val_loss1.127150058746338\n",
            "iteration 847 :train_loss:1.172648549079895 val_loss1.1271376609802246\n",
            "iteration 848 :train_loss:1.1726373434066772 val_loss1.1271252632141113\n",
            "iteration 849 :train_loss:1.1726258993148804 val_loss1.127112865447998\n",
            "iteration 850 :train_loss:1.1726146936416626 val_loss1.1271001100540161\n",
            "iteration 851 :train_loss:1.1726033687591553 val_loss1.1270878314971924\n",
            "iteration 852 :train_loss:1.172592043876648 val_loss1.1270756721496582\n",
            "iteration 853 :train_loss:1.1725808382034302 val_loss1.1270630359649658\n",
            "iteration 854 :train_loss:1.1725695133209229 val_loss1.127050757408142\n",
            "iteration 855 :train_loss:1.172558307647705 val_loss1.1270384788513184\n",
            "iteration 856 :train_loss:1.1725469827651978 val_loss1.1270262002944946\n",
            "iteration 857 :train_loss:1.17253577709198 val_loss1.1270136833190918\n",
            "iteration 858 :train_loss:1.1725244522094727 val_loss1.1270012855529785\n",
            "iteration 859 :train_loss:1.1725132465362549 val_loss1.1269890069961548\n",
            "iteration 860 :train_loss:1.1725019216537476 val_loss1.1269768476486206\n",
            "iteration 861 :train_loss:1.1724908351898193 val_loss1.1269644498825073\n",
            "iteration 862 :train_loss:1.172479510307312 val_loss1.1269521713256836\n",
            "iteration 863 :train_loss:1.1724684238433838 val_loss1.1269398927688599\n",
            "iteration 864 :train_loss:1.1724570989608765 val_loss1.1269276142120361\n",
            "iteration 865 :train_loss:1.1724460124969482 val_loss1.126915454864502\n",
            "iteration 866 :train_loss:1.17243492603302 val_loss1.1269031763076782\n",
            "iteration 867 :train_loss:1.1724237203598022 val_loss1.1268908977508545\n",
            "iteration 868 :train_loss:1.1724125146865845 val_loss1.1268788576126099\n",
            "iteration 869 :train_loss:1.1724013090133667 val_loss1.1268666982650757\n",
            "iteration 870 :train_loss:1.1723902225494385 val_loss1.126854419708252\n",
            "iteration 871 :train_loss:1.1723791360855103 val_loss1.1268421411514282\n",
            "iteration 872 :train_loss:1.172368049621582 val_loss1.1268301010131836\n",
            "iteration 873 :train_loss:1.1723568439483643 val_loss1.1268178224563599\n",
            "iteration 874 :train_loss:1.1723458766937256 val_loss1.1268057823181152\n",
            "iteration 875 :train_loss:1.1723346710205078 val_loss1.126793622970581\n",
            "iteration 876 :train_loss:1.1723237037658691 val_loss1.1267814636230469\n",
            "iteration 877 :train_loss:1.172312617301941 val_loss1.1267694234848022\n",
            "iteration 878 :train_loss:1.1723014116287231 val_loss1.1267573833465576\n",
            "iteration 879 :train_loss:1.1722904443740845 val_loss1.1267452239990234\n",
            "iteration 880 :train_loss:1.1722793579101562 val_loss1.1267330646514893\n",
            "iteration 881 :train_loss:1.1722681522369385 val_loss1.1267210245132446\n",
            "iteration 882 :train_loss:1.172257423400879 val_loss1.126708984375\n",
            "iteration 883 :train_loss:1.1722462177276611 val_loss1.126697063446045\n",
            "iteration 884 :train_loss:1.172235369682312 val_loss1.1266849040985107\n",
            "iteration 885 :train_loss:1.1722241640090942 val_loss1.1266729831695557\n",
            "iteration 886 :train_loss:1.1722133159637451 val_loss1.1266610622406006\n",
            "iteration 887 :train_loss:1.1722023487091064 val_loss1.1266487836837769\n",
            "iteration 888 :train_loss:1.1721912622451782 val_loss1.1266368627548218\n",
            "iteration 889 :train_loss:1.1721802949905396 val_loss1.1266248226165771\n",
            "iteration 890 :train_loss:1.1721692085266113 val_loss1.126612901687622\n",
            "iteration 891 :train_loss:1.1721582412719727 val_loss1.1266008615493774\n",
            "iteration 892 :train_loss:1.172147274017334 val_loss1.1265889406204224\n",
            "iteration 893 :train_loss:1.1721364259719849 val_loss1.1265769004821777\n",
            "iteration 894 :train_loss:1.1721255779266357 val_loss1.1265650987625122\n",
            "iteration 895 :train_loss:1.1721144914627075 val_loss1.1265531778335571\n",
            "iteration 896 :train_loss:1.1721035242080688 val_loss1.1265411376953125\n",
            "iteration 897 :train_loss:1.1720926761627197 val_loss1.1265292167663574\n",
            "iteration 898 :train_loss:1.172081708908081 val_loss1.1265172958374023\n",
            "iteration 899 :train_loss:1.172070860862732 val_loss1.1265053749084473\n",
            "iteration 900 :train_loss:1.1720597743988037 val_loss1.1264935731887817\n",
            "iteration 901 :train_loss:1.1720489263534546 val_loss1.1264816522598267\n",
            "iteration 902 :train_loss:1.172038197517395 val_loss1.126469612121582\n",
            "iteration 903 :train_loss:1.1720272302627563 val_loss1.1264578104019165\n",
            "iteration 904 :train_loss:1.1720162630081177 val_loss1.1264458894729614\n",
            "iteration 905 :train_loss:1.1720054149627686 val_loss1.126434087753296\n",
            "iteration 906 :train_loss:1.1719945669174194 val_loss1.1264222860336304\n",
            "iteration 907 :train_loss:1.1719835996627808 val_loss1.1264103651046753\n",
            "iteration 908 :train_loss:1.1719728708267212 val_loss1.1263984441757202\n",
            "iteration 909 :train_loss:1.171962022781372 val_loss1.1263867616653442\n",
            "iteration 910 :train_loss:1.1719510555267334 val_loss1.1263749599456787\n",
            "iteration 911 :train_loss:1.1719402074813843 val_loss1.1263630390167236\n",
            "iteration 912 :train_loss:1.1719292402267456 val_loss1.1263511180877686\n",
            "iteration 913 :train_loss:1.1719186305999756 val_loss1.1263394355773926\n",
            "iteration 914 :train_loss:1.1719077825546265 val_loss1.1263277530670166\n",
            "iteration 915 :train_loss:1.1718969345092773 val_loss1.1263158321380615\n",
            "iteration 916 :train_loss:1.1718860864639282 val_loss1.126304030418396\n",
            "iteration 917 :train_loss:1.171875238418579 val_loss1.1262922286987305\n",
            "iteration 918 :train_loss:1.17186439037323 val_loss1.126280426979065\n",
            "iteration 919 :train_loss:1.1718535423278809 val_loss1.1262688636779785\n",
            "iteration 920 :train_loss:1.1718428134918213 val_loss1.126257061958313\n",
            "iteration 921 :train_loss:1.1718319654464722 val_loss1.1262452602386475\n",
            "iteration 922 :train_loss:1.1718212366104126 val_loss1.126233458518982\n",
            "iteration 923 :train_loss:1.1718103885650635 val_loss1.1262216567993164\n",
            "iteration 924 :train_loss:1.171799659729004 val_loss1.12621009349823\n",
            "iteration 925 :train_loss:1.1717891693115234 val_loss1.1261982917785645\n",
            "iteration 926 :train_loss:1.1717780828475952 val_loss1.1261866092681885\n",
            "iteration 927 :train_loss:1.1717674732208252 val_loss1.1261746883392334\n",
            "iteration 928 :train_loss:1.171756625175476 val_loss1.1261630058288574\n",
            "iteration 929 :train_loss:1.171745777130127 val_loss1.126151442527771\n",
            "iteration 930 :train_loss:1.171735167503357 val_loss1.126139760017395\n",
            "iteration 931 :train_loss:1.1717243194580078 val_loss1.126128077507019\n",
            "iteration 932 :train_loss:1.1717135906219482 val_loss1.126116394996643\n",
            "iteration 933 :train_loss:1.1717028617858887 val_loss1.126104712486267\n",
            "iteration 934 :train_loss:1.171692132949829 val_loss1.1260930299758911\n",
            "iteration 935 :train_loss:1.1716814041137695 val_loss1.1260812282562256\n",
            "iteration 936 :train_loss:1.1716707944869995 val_loss1.1260695457458496\n",
            "iteration 937 :train_loss:1.17166006565094 val_loss1.1260579824447632\n",
            "iteration 938 :train_loss:1.1716493368148804 val_loss1.1260462999343872\n",
            "iteration 939 :train_loss:1.1716384887695312 val_loss1.1260346174240112\n",
            "iteration 940 :train_loss:1.1716278791427612 val_loss1.1260230541229248\n",
            "iteration 941 :train_loss:1.1716169118881226 val_loss1.1260113716125488\n",
            "iteration 942 :train_loss:1.1716063022613525 val_loss1.1259998083114624\n",
            "iteration 943 :train_loss:1.171595573425293 val_loss1.125988245010376\n",
            "iteration 944 :train_loss:1.171584963798523 val_loss1.1259765625\n",
            "iteration 945 :train_loss:1.1715742349624634 val_loss1.125964879989624\n",
            "iteration 946 :train_loss:1.1715635061264038 val_loss1.1259534358978271\n",
            "iteration 947 :train_loss:1.1715528964996338 val_loss1.1259417533874512\n",
            "iteration 948 :train_loss:1.1715421676635742 val_loss1.1259300708770752\n",
            "iteration 949 :train_loss:1.1715314388275146 val_loss1.1259185075759888\n",
            "iteration 950 :train_loss:1.1715208292007446 val_loss1.1259069442749023\n",
            "iteration 951 :train_loss:1.1715102195739746 val_loss1.125895380973816\n",
            "iteration 952 :train_loss:1.171499490737915 val_loss1.12588369846344\n",
            "iteration 953 :train_loss:1.171488881111145 val_loss1.1258721351623535\n",
            "iteration 954 :train_loss:1.1714781522750854 val_loss1.1258606910705566\n",
            "iteration 955 :train_loss:1.1714675426483154 val_loss1.1258491277694702\n",
            "iteration 956 :train_loss:1.1714568138122559 val_loss1.1258375644683838\n",
            "iteration 957 :train_loss:1.1714462041854858 val_loss1.1258258819580078\n",
            "iteration 958 :train_loss:1.1714357137680054 val_loss1.125814437866211\n",
            "iteration 959 :train_loss:1.1714248657226562 val_loss1.125802993774414\n",
            "iteration 960 :train_loss:1.1714143753051758 val_loss1.125791311264038\n",
            "iteration 961 :train_loss:1.1714036464691162 val_loss1.1257798671722412\n",
            "iteration 962 :train_loss:1.1713930368423462 val_loss1.1257681846618652\n",
            "iteration 963 :train_loss:1.1713824272155762 val_loss1.1257567405700684\n",
            "iteration 964 :train_loss:1.1713718175888062 val_loss1.1257452964782715\n",
            "iteration 965 :train_loss:1.1713613271713257 val_loss1.1257336139678955\n",
            "iteration 966 :train_loss:1.1713507175445557 val_loss1.1257221698760986\n",
            "iteration 967 :train_loss:1.1713398694992065 val_loss1.1257106065750122\n",
            "iteration 968 :train_loss:1.1713292598724365 val_loss1.1256991624832153\n",
            "iteration 969 :train_loss:1.171318769454956 val_loss1.1256877183914185\n",
            "iteration 970 :train_loss:1.171308159828186 val_loss1.125676155090332\n",
            "iteration 971 :train_loss:1.171297550201416 val_loss1.1256645917892456\n",
            "iteration 972 :train_loss:1.171286940574646 val_loss1.1256531476974487\n",
            "iteration 973 :train_loss:1.171276330947876 val_loss1.1256417036056519\n",
            "iteration 974 :train_loss:1.1712658405303955 val_loss1.1256301403045654\n",
            "iteration 975 :train_loss:1.171255111694336 val_loss1.1256186962127686\n",
            "iteration 976 :train_loss:1.1712446212768555 val_loss1.1256071329116821\n",
            "iteration 977 :train_loss:1.1712340116500854 val_loss1.1255958080291748\n",
            "iteration 978 :train_loss:1.1712234020233154 val_loss1.1255842447280884\n",
            "iteration 979 :train_loss:1.1712127923965454 val_loss1.125572919845581\n",
            "iteration 980 :train_loss:1.171202301979065 val_loss1.125561237335205\n",
            "iteration 981 :train_loss:1.1711915731430054 val_loss1.1255499124526978\n",
            "iteration 982 :train_loss:1.171181082725525 val_loss1.1255383491516113\n",
            "iteration 983 :train_loss:1.1711705923080444 val_loss1.125527024269104\n",
            "iteration 984 :train_loss:1.1711599826812744 val_loss1.1255154609680176\n",
            "iteration 985 :train_loss:1.171149492263794 val_loss1.1255042552947998\n",
            "iteration 986 :train_loss:1.171138882637024 val_loss1.1254926919937134\n",
            "iteration 987 :train_loss:1.1711283922195435 val_loss1.1254812479019165\n",
            "iteration 988 :train_loss:1.171117901802063 val_loss1.1254699230194092\n",
            "iteration 989 :train_loss:1.171107292175293 val_loss1.1254583597183228\n",
            "iteration 990 :train_loss:1.1710968017578125 val_loss1.1254470348358154\n",
            "iteration 991 :train_loss:1.1710859537124634 val_loss1.1254355907440186\n",
            "iteration 992 :train_loss:1.1710755825042725 val_loss1.1254242658615112\n",
            "iteration 993 :train_loss:1.171065092086792 val_loss1.1254127025604248\n",
            "iteration 994 :train_loss:1.1710546016693115 val_loss1.1254013776779175\n",
            "iteration 995 :train_loss:1.1710439920425415 val_loss1.1253899335861206\n",
            "iteration 996 :train_loss:1.1710333824157715 val_loss1.1253786087036133\n",
            "iteration 997 :train_loss:1.171022891998291 val_loss1.1253670454025269\n",
            "iteration 998 :train_loss:1.1710124015808105 val_loss1.125355839729309\n",
            "iteration 999 :train_loss:1.1710017919540405 val_loss1.1253443956375122\n",
            "iteration 1000 :train_loss:1.17099130153656 val_loss1.1253330707550049\n",
            "iteration 1001 :train_loss:1.1709809303283691 val_loss1.1253215074539185\n",
            "iteration 1002 :train_loss:1.1709704399108887 val_loss1.1253101825714111\n",
            "iteration 1003 :train_loss:1.170959711074829 val_loss1.1252988576889038\n",
            "iteration 1004 :train_loss:1.1709492206573486 val_loss1.1252875328063965\n",
            "iteration 1005 :train_loss:1.1709387302398682 val_loss1.1252760887145996\n",
            "iteration 1006 :train_loss:1.1709283590316772 val_loss1.1252647638320923\n",
            "iteration 1007 :train_loss:1.1709178686141968 val_loss1.1252533197402954\n",
            "iteration 1008 :train_loss:1.1709072589874268 val_loss1.125241994857788\n",
            "iteration 1009 :train_loss:1.1708967685699463 val_loss1.1252306699752808\n",
            "iteration 1010 :train_loss:1.1708862781524658 val_loss1.1252192258834839\n",
            "iteration 1011 :train_loss:1.170875906944275 val_loss1.1252079010009766\n",
            "iteration 1012 :train_loss:1.1708652973175049 val_loss1.1251965761184692\n",
            "iteration 1013 :train_loss:1.1708548069000244 val_loss1.125185251235962\n",
            "iteration 1014 :train_loss:1.170844316482544 val_loss1.1251739263534546\n",
            "iteration 1015 :train_loss:1.1708338260650635 val_loss1.1251626014709473\n",
            "iteration 1016 :train_loss:1.1708234548568726 val_loss1.1251511573791504\n",
            "iteration 1017 :train_loss:1.170812964439392 val_loss1.125139832496643\n",
            "iteration 1018 :train_loss:1.170802354812622 val_loss1.1251285076141357\n",
            "iteration 1019 :train_loss:1.1707918643951416 val_loss1.125117301940918\n",
            "iteration 1020 :train_loss:1.1707814931869507 val_loss1.125105857849121\n",
            "iteration 1021 :train_loss:1.1707708835601807 val_loss1.1250946521759033\n",
            "iteration 1022 :train_loss:1.1707605123519897 val_loss1.125083327293396\n",
            "iteration 1023 :train_loss:1.1707501411437988 val_loss1.1250720024108887\n",
            "iteration 1024 :train_loss:1.1707396507263184 val_loss1.1250605583190918\n",
            "iteration 1025 :train_loss:1.170729160308838 val_loss1.125049352645874\n",
            "iteration 1026 :train_loss:1.1707186698913574 val_loss1.1250380277633667\n",
            "iteration 1027 :train_loss:1.170708179473877 val_loss1.125026822090149\n",
            "iteration 1028 :train_loss:1.1706976890563965 val_loss1.125015377998352\n",
            "iteration 1029 :train_loss:1.170687198638916 val_loss1.1250041723251343\n",
            "iteration 1030 :train_loss:1.170676827430725 val_loss1.124992847442627\n",
            "iteration 1031 :train_loss:1.1706663370132446 val_loss1.1249815225601196\n",
            "iteration 1032 :train_loss:1.1706559658050537 val_loss1.1249703168869019\n",
            "iteration 1033 :train_loss:1.1706455945968628 val_loss1.124958872795105\n",
            "iteration 1034 :train_loss:1.1706351041793823 val_loss1.1249476671218872\n",
            "iteration 1035 :train_loss:1.1706246137619019 val_loss1.1249364614486694\n",
            "iteration 1036 :train_loss:1.170614242553711 val_loss1.124925136566162\n",
            "iteration 1037 :train_loss:1.1706037521362305 val_loss1.1249138116836548\n",
            "iteration 1038 :train_loss:1.1705933809280396 val_loss1.124902606010437\n",
            "iteration 1039 :train_loss:1.170582890510559 val_loss1.1248914003372192\n",
            "iteration 1040 :train_loss:1.1705724000930786 val_loss1.124880075454712\n",
            "iteration 1041 :train_loss:1.1705620288848877 val_loss1.1248688697814941\n",
            "iteration 1042 :train_loss:1.1705516576766968 val_loss1.1248575448989868\n",
            "iteration 1043 :train_loss:1.1705411672592163 val_loss1.124846339225769\n",
            "iteration 1044 :train_loss:1.1705307960510254 val_loss1.1248351335525513\n",
            "iteration 1045 :train_loss:1.170520305633545 val_loss1.1248236894607544\n",
            "iteration 1046 :train_loss:1.1705100536346436 val_loss1.1248124837875366\n",
            "iteration 1047 :train_loss:1.1704994440078735 val_loss1.1248013973236084\n",
            "iteration 1048 :train_loss:1.1704891920089722 val_loss1.124790072441101\n",
            "iteration 1049 :train_loss:1.1704787015914917 val_loss1.1247789859771729\n",
            "iteration 1050 :train_loss:1.1704683303833008 val_loss1.1247676610946655\n",
            "iteration 1051 :train_loss:1.1704578399658203 val_loss1.1247563362121582\n",
            "iteration 1052 :train_loss:1.1704473495483398 val_loss1.1247450113296509\n",
            "iteration 1053 :train_loss:1.1704370975494385 val_loss1.124733805656433\n",
            "iteration 1054 :train_loss:1.1704267263412476 val_loss1.1247227191925049\n",
            "iteration 1055 :train_loss:1.1704163551330566 val_loss1.1247113943099976\n",
            "iteration 1056 :train_loss:1.1704058647155762 val_loss1.1247001886367798\n",
            "iteration 1057 :train_loss:1.1703954935073853 val_loss1.124688982963562\n",
            "iteration 1058 :train_loss:1.1703850030899048 val_loss1.1246776580810547\n",
            "iteration 1059 :train_loss:1.1703746318817139 val_loss1.124666452407837\n",
            "iteration 1060 :train_loss:1.170364260673523 val_loss1.1246552467346191\n",
            "iteration 1061 :train_loss:1.170353889465332 val_loss1.1246440410614014\n",
            "iteration 1062 :train_loss:1.1703435182571411 val_loss1.1246328353881836\n",
            "iteration 1063 :train_loss:1.1703330278396606 val_loss1.1246216297149658\n",
            "iteration 1064 :train_loss:1.1703225374221802 val_loss1.1246105432510376\n",
            "iteration 1065 :train_loss:1.1703124046325684 val_loss1.1245992183685303\n",
            "iteration 1066 :train_loss:1.170301914215088 val_loss1.1245880126953125\n",
            "iteration 1067 :train_loss:1.170291543006897 val_loss1.1245768070220947\n",
            "iteration 1068 :train_loss:1.170281171798706 val_loss1.124565601348877\n",
            "iteration 1069 :train_loss:1.1702708005905151 val_loss1.1245543956756592\n",
            "iteration 1070 :train_loss:1.1702605485916138 val_loss1.1245431900024414\n",
            "iteration 1071 :train_loss:1.1702500581741333 val_loss1.1245319843292236\n",
            "iteration 1072 :train_loss:1.1702396869659424 val_loss1.1245207786560059\n",
            "iteration 1073 :train_loss:1.170229434967041 val_loss1.1245096921920776\n",
            "iteration 1074 :train_loss:1.1702189445495605 val_loss1.1244984865188599\n",
            "iteration 1075 :train_loss:1.1702085733413696 val_loss1.124487280845642\n",
            "iteration 1076 :train_loss:1.1701982021331787 val_loss1.1244760751724243\n",
            "iteration 1077 :train_loss:1.1701878309249878 val_loss1.124464988708496\n",
            "iteration 1078 :train_loss:1.1701773405075073 val_loss1.1244537830352783\n",
            "iteration 1079 :train_loss:1.1701672077178955 val_loss1.124442458152771\n",
            "iteration 1080 :train_loss:1.170156717300415 val_loss1.1244313716888428\n",
            "iteration 1081 :train_loss:1.1701464653015137 val_loss1.124420404434204\n",
            "iteration 1082 :train_loss:1.1701359748840332 val_loss1.1244090795516968\n",
            "iteration 1083 :train_loss:1.1701256036758423 val_loss1.124397873878479\n",
            "iteration 1084 :train_loss:1.170115351676941 val_loss1.1243866682052612\n",
            "iteration 1085 :train_loss:1.17010498046875 val_loss1.124375581741333\n",
            "iteration 1086 :train_loss:1.1700944900512695 val_loss1.1243643760681152\n",
            "iteration 1087 :train_loss:1.1700841188430786 val_loss1.1243531703948975\n",
            "iteration 1088 :train_loss:1.1700738668441772 val_loss1.1243419647216797\n",
            "iteration 1089 :train_loss:1.1700634956359863 val_loss1.124330759048462\n",
            "iteration 1090 :train_loss:1.170053243637085 val_loss1.1243199110031128\n",
            "iteration 1091 :train_loss:1.1700427532196045 val_loss1.124308705329895\n",
            "iteration 1092 :train_loss:1.1700326204299927 val_loss1.1242974996566772\n",
            "iteration 1093 :train_loss:1.1700221300125122 val_loss1.124286413192749\n",
            "iteration 1094 :train_loss:1.1700118780136108 val_loss1.1242750883102417\n",
            "iteration 1095 :train_loss:1.17000150680542 val_loss1.1242640018463135\n",
            "iteration 1096 :train_loss:1.1699912548065186 val_loss1.1242529153823853\n",
            "iteration 1097 :train_loss:1.1699808835983276 val_loss1.1242417097091675\n",
            "iteration 1098 :train_loss:1.1699705123901367 val_loss1.1242306232452393\n",
            "iteration 1099 :train_loss:1.1699600219726562 val_loss1.1242194175720215\n",
            "iteration 1100 :train_loss:1.169950008392334 val_loss1.1242084503173828\n",
            "iteration 1101 :train_loss:1.1699395179748535 val_loss1.124197244644165\n",
            "iteration 1102 :train_loss:1.1699291467666626 val_loss1.1241860389709473\n",
            "iteration 1103 :train_loss:1.1699188947677612 val_loss1.1241748332977295\n",
            "iteration 1104 :train_loss:1.1699086427688599 val_loss1.1241637468338013\n",
            "iteration 1105 :train_loss:1.1698981523513794 val_loss1.124152660369873\n",
            "iteration 1106 :train_loss:1.1698880195617676 val_loss1.1241414546966553\n",
            "iteration 1107 :train_loss:1.169877529144287 val_loss1.1241304874420166\n",
            "iteration 1108 :train_loss:1.1698672771453857 val_loss1.1241194009780884\n",
            "iteration 1109 :train_loss:1.1698569059371948 val_loss1.1241083145141602\n",
            "iteration 1110 :train_loss:1.169846773147583 val_loss1.1240971088409424\n",
            "iteration 1111 :train_loss:1.1698362827301025 val_loss1.1240860223770142\n",
            "iteration 1112 :train_loss:1.1698260307312012 val_loss1.1240750551223755\n",
            "iteration 1113 :train_loss:1.1698158979415894 val_loss1.1240638494491577\n",
            "iteration 1114 :train_loss:1.1698054075241089 val_loss1.12405264377594\n",
            "iteration 1115 :train_loss:1.1697951555252075 val_loss1.1240415573120117\n",
            "iteration 1116 :train_loss:1.1697849035263062 val_loss1.124030590057373\n",
            "iteration 1117 :train_loss:1.1697744131088257 val_loss1.1240193843841553\n",
            "iteration 1118 :train_loss:1.1697641611099243 val_loss1.124008297920227\n",
            "iteration 1119 :train_loss:1.1697537899017334 val_loss1.1239972114562988\n",
            "iteration 1120 :train_loss:1.169743537902832 val_loss1.1239861249923706\n",
            "iteration 1121 :train_loss:1.1697331666946411 val_loss1.1239749193191528\n",
            "iteration 1122 :train_loss:1.1697229146957397 val_loss1.1239638328552246\n",
            "iteration 1123 :train_loss:1.1697126626968384 val_loss1.1239526271820068\n",
            "iteration 1124 :train_loss:1.1697025299072266 val_loss1.1239417791366577\n",
            "iteration 1125 :train_loss:1.169692039489746 val_loss1.12393057346344\n",
            "iteration 1126 :train_loss:1.1696819067001343 val_loss1.1239194869995117\n",
            "iteration 1127 :train_loss:1.1696715354919434 val_loss1.1239084005355835\n",
            "iteration 1128 :train_loss:1.1696611642837524 val_loss1.1238973140716553\n",
            "iteration 1129 :train_loss:1.169650912284851 val_loss1.1238863468170166\n",
            "iteration 1130 :train_loss:1.1696406602859497 val_loss1.1238751411437988\n",
            "iteration 1131 :train_loss:1.169630527496338 val_loss1.1238641738891602\n",
            "iteration 1132 :train_loss:1.169620156288147 val_loss1.123853087425232\n",
            "iteration 1133 :train_loss:1.169609785079956 val_loss1.1238420009613037\n",
            "iteration 1134 :train_loss:1.1695996522903442 val_loss1.123831033706665\n",
            "iteration 1135 :train_loss:1.1695894002914429 val_loss1.1238198280334473\n",
            "iteration 1136 :train_loss:1.169579029083252 val_loss1.123808741569519\n",
            "iteration 1137 :train_loss:1.1695687770843506 val_loss1.1237976551055908\n",
            "iteration 1138 :train_loss:1.1695584058761597 val_loss1.1237865686416626\n",
            "iteration 1139 :train_loss:1.1695481538772583 val_loss1.123775601387024\n",
            "iteration 1140 :train_loss:1.1695380210876465 val_loss1.1237643957138062\n",
            "iteration 1141 :train_loss:1.1695276498794556 val_loss1.1237534284591675\n",
            "iteration 1142 :train_loss:1.1695175170898438 val_loss1.1237423419952393\n",
            "iteration 1143 :train_loss:1.1695072650909424 val_loss1.1237311363220215\n",
            "iteration 1144 :train_loss:1.1694968938827515 val_loss1.1237202882766724\n",
            "iteration 1145 :train_loss:1.16948664188385 val_loss1.1237092018127441\n",
            "iteration 1146 :train_loss:1.1694762706756592 val_loss1.1236982345581055\n",
            "iteration 1147 :train_loss:1.1694661378860474 val_loss1.1236871480941772\n",
            "iteration 1148 :train_loss:1.1694557666778564 val_loss1.123676061630249\n",
            "iteration 1149 :train_loss:1.169445514678955 val_loss1.1236649751663208\n",
            "iteration 1150 :train_loss:1.1694352626800537 val_loss1.1236540079116821\n",
            "iteration 1151 :train_loss:1.1694250106811523 val_loss1.123642921447754\n",
            "iteration 1152 :train_loss:1.1694148778915405 val_loss1.1236318349838257\n",
            "iteration 1153 :train_loss:1.1694046258926392 val_loss1.123620867729187\n",
            "iteration 1154 :train_loss:1.1693943738937378 val_loss1.1236096620559692\n",
            "iteration 1155 :train_loss:1.1693840026855469 val_loss1.1235988140106201\n",
            "iteration 1156 :train_loss:1.1693737506866455 val_loss1.123587727546692\n",
            "iteration 1157 :train_loss:1.1693634986877441 val_loss1.1235767602920532\n",
            "iteration 1158 :train_loss:1.1693532466888428 val_loss1.1235655546188354\n",
            "iteration 1159 :train_loss:1.1693429946899414 val_loss1.1235545873641968\n",
            "iteration 1160 :train_loss:1.16933274269104 val_loss1.1235435009002686\n",
            "iteration 1161 :train_loss:1.1693226099014282 val_loss1.1235326528549194\n",
            "iteration 1162 :train_loss:1.1693121194839478 val_loss1.1235215663909912\n",
            "iteration 1163 :train_loss:1.169301986694336 val_loss1.123510479927063\n",
            "iteration 1164 :train_loss:1.1692917346954346 val_loss1.1234995126724243\n",
            "iteration 1165 :train_loss:1.1692816019058228 val_loss1.123488426208496\n",
            "iteration 1166 :train_loss:1.1692713499069214 val_loss1.1234773397445679\n",
            "iteration 1167 :train_loss:1.1692609786987305 val_loss1.1234664916992188\n",
            "iteration 1168 :train_loss:1.1692509651184082 val_loss1.1234554052352905\n",
            "iteration 1169 :train_loss:1.1692404747009277 val_loss1.1234441995620728\n",
            "iteration 1170 :train_loss:1.1692302227020264 val_loss1.1234333515167236\n",
            "iteration 1171 :train_loss:1.1692200899124146 val_loss1.123422384262085\n",
            "iteration 1172 :train_loss:1.1692099571228027 val_loss1.1234111785888672\n",
            "iteration 1173 :train_loss:1.169199824333191 val_loss1.1234002113342285\n",
            "iteration 1174 :train_loss:1.1691895723342896 val_loss1.1233893632888794\n",
            "iteration 1175 :train_loss:1.1691792011260986 val_loss1.1233782768249512\n",
            "iteration 1176 :train_loss:1.1691690683364868 val_loss1.1233673095703125\n",
            "iteration 1177 :train_loss:1.169158697128296 val_loss1.1233562231063843\n",
            "iteration 1178 :train_loss:1.1691484451293945 val_loss1.123345136642456\n",
            "iteration 1179 :train_loss:1.1691383123397827 val_loss1.1233340501785278\n",
            "iteration 1180 :train_loss:1.1691280603408813 val_loss1.1233230829238892\n",
            "iteration 1181 :train_loss:1.1691179275512695 val_loss1.12331223487854\n",
            "iteration 1182 :train_loss:1.1691075563430786 val_loss1.123301386833191\n",
            "iteration 1183 :train_loss:1.1690974235534668 val_loss1.1232901811599731\n",
            "iteration 1184 :train_loss:1.1690871715545654 val_loss1.123279333114624\n",
            "iteration 1185 :train_loss:1.169076919555664 val_loss1.1232682466506958\n",
            "iteration 1186 :train_loss:1.1690667867660522 val_loss1.1232572793960571\n",
            "iteration 1187 :train_loss:1.1690564155578613 val_loss1.123246192932129\n",
            "iteration 1188 :train_loss:1.1690462827682495 val_loss1.1232353448867798\n",
            "iteration 1189 :train_loss:1.1690362691879272 val_loss1.1232243776321411\n",
            "iteration 1190 :train_loss:1.1690258979797363 val_loss1.123213291168213\n",
            "iteration 1191 :train_loss:1.1690157651901245 val_loss1.1232024431228638\n",
            "iteration 1192 :train_loss:1.1690055131912231 val_loss1.1231913566589355\n",
            "iteration 1193 :train_loss:1.1689953804016113 val_loss1.1231802701950073\n",
            "iteration 1194 :train_loss:1.1689848899841309 val_loss1.1231693029403687\n",
            "iteration 1195 :train_loss:1.168974757194519 val_loss1.1231584548950195\n",
            "iteration 1196 :train_loss:1.1689647436141968 val_loss1.1231473684310913\n",
            "iteration 1197 :train_loss:1.1689543724060059 val_loss1.1231364011764526\n",
            "iteration 1198 :train_loss:1.1689443588256836 val_loss1.1231255531311035\n",
            "iteration 1199 :train_loss:1.1689339876174927 val_loss1.1231144666671753\n",
            "iteration 1200 :train_loss:1.1689238548278809 val_loss1.123103380203247\n",
            "iteration 1201 :train_loss:1.168913722038269 val_loss1.123092532157898\n",
            "iteration 1202 :train_loss:1.1689034700393677 val_loss1.1230814456939697\n",
            "iteration 1203 :train_loss:1.1688932180404663 val_loss1.1230705976486206\n",
            "iteration 1204 :train_loss:1.1688830852508545 val_loss1.1230595111846924\n",
            "iteration 1205 :train_loss:1.1688728332519531 val_loss1.1230485439300537\n",
            "iteration 1206 :train_loss:1.1688625812530518 val_loss1.1230378150939941\n",
            "iteration 1207 :train_loss:1.16885244846344 val_loss1.123026728630066\n",
            "iteration 1208 :train_loss:1.1688423156738281 val_loss1.1230157613754272\n",
            "iteration 1209 :train_loss:1.1688321828842163 val_loss1.1230049133300781\n",
            "iteration 1210 :train_loss:1.168821930885315 val_loss1.1229937076568604\n",
            "iteration 1211 :train_loss:1.1688117980957031 val_loss1.1229827404022217\n",
            "iteration 1212 :train_loss:1.1688015460968018 val_loss1.1229718923568726\n",
            "iteration 1213 :train_loss:1.1687915325164795 val_loss1.1229610443115234\n",
            "iteration 1214 :train_loss:1.1687811613082886 val_loss1.1229499578475952\n",
            "iteration 1215 :train_loss:1.1687709093093872 val_loss1.1229389905929565\n",
            "iteration 1216 :train_loss:1.1687607765197754 val_loss1.1229281425476074\n",
            "iteration 1217 :train_loss:1.1687506437301636 val_loss1.1229171752929688\n",
            "iteration 1218 :train_loss:1.1687405109405518 val_loss1.1229060888290405\n",
            "iteration 1219 :train_loss:1.1687302589416504 val_loss1.1228951215744019\n",
            "iteration 1220 :train_loss:1.1687201261520386 val_loss1.1228841543197632\n",
            "iteration 1221 :train_loss:1.1687098741531372 val_loss1.1228734254837036\n",
            "iteration 1222 :train_loss:1.1686996221542358 val_loss1.122862458229065\n",
            "iteration 1223 :train_loss:1.168689489364624 val_loss1.1228512525558472\n",
            "iteration 1224 :train_loss:1.1686793565750122 val_loss1.1228405237197876\n",
            "iteration 1225 :train_loss:1.1686691045761108 val_loss1.122829556465149\n",
            "iteration 1226 :train_loss:1.1686590909957886 val_loss1.1228187084197998\n",
            "iteration 1227 :train_loss:1.1686488389968872 val_loss1.1228076219558716\n",
            "iteration 1228 :train_loss:1.168638825416565 val_loss1.122796654701233\n",
            "iteration 1229 :train_loss:1.168628454208374 val_loss1.1227856874465942\n",
            "iteration 1230 :train_loss:1.1686183214187622 val_loss1.1227748394012451\n",
            "iteration 1231 :train_loss:1.1686080694198608 val_loss1.1227638721466064\n",
            "iteration 1232 :train_loss:1.168597936630249 val_loss1.1227529048919678\n",
            "iteration 1233 :train_loss:1.1685878038406372 val_loss1.1227421760559082\n",
            "iteration 1234 :train_loss:1.1685776710510254 val_loss1.1227309703826904\n",
            "iteration 1235 :train_loss:1.1685676574707031 val_loss1.1227201223373413\n",
            "iteration 1236 :train_loss:1.1685575246810913 val_loss1.1227092742919922\n",
            "iteration 1237 :train_loss:1.16854727268219 val_loss1.1226983070373535\n",
            "iteration 1238 :train_loss:1.1685370206832886 val_loss1.1226872205734253\n",
            "iteration 1239 :train_loss:1.1685267686843872 val_loss1.1226764917373657\n",
            "iteration 1240 :train_loss:1.1685166358947754 val_loss1.1226656436920166\n",
            "iteration 1241 :train_loss:1.1685066223144531 val_loss1.122654676437378\n",
            "iteration 1242 :train_loss:1.1684964895248413 val_loss1.1226435899734497\n",
            "iteration 1243 :train_loss:1.16848623752594 val_loss1.1226327419281006\n",
            "iteration 1244 :train_loss:1.1684761047363281 val_loss1.122621774673462\n",
            "iteration 1245 :train_loss:1.1684659719467163 val_loss1.1226108074188232\n",
            "iteration 1246 :train_loss:1.168455719947815 val_loss1.1225999593734741\n",
            "iteration 1247 :train_loss:1.1684455871582031 val_loss1.1225889921188354\n",
            "iteration 1248 :train_loss:1.1684354543685913 val_loss1.1225780248641968\n",
            "iteration 1249 :train_loss:1.1684253215789795 val_loss1.1225671768188477\n",
            "iteration 1250 :train_loss:1.1684151887893677 val_loss1.1225563287734985\n",
            "iteration 1251 :train_loss:1.1684049367904663 val_loss1.1225453615188599\n",
            "iteration 1252 :train_loss:1.168394923210144 val_loss1.1225342750549316\n",
            "iteration 1253 :train_loss:1.1683847904205322 val_loss1.122523546218872\n",
            "iteration 1254 :train_loss:1.1683745384216309 val_loss1.1225125789642334\n",
            "iteration 1255 :train_loss:1.1683645248413086 val_loss1.1225017309188843\n",
            "iteration 1256 :train_loss:1.1683543920516968 val_loss1.1224907636642456\n",
            "iteration 1257 :train_loss:1.1683441400527954 val_loss1.122479796409607\n",
            "iteration 1258 :train_loss:1.1683340072631836 val_loss1.1224690675735474\n",
            "iteration 1259 :train_loss:1.1683238744735718 val_loss1.1224581003189087\n",
            "iteration 1260 :train_loss:1.16831374168396 val_loss1.12244713306427\n",
            "iteration 1261 :train_loss:1.1683034896850586 val_loss1.122436285018921\n",
            "iteration 1262 :train_loss:1.1682933568954468 val_loss1.1224253177642822\n",
            "iteration 1263 :train_loss:1.168283224105835 val_loss1.122414469718933\n",
            "iteration 1264 :train_loss:1.1682730913162231 val_loss1.1224035024642944\n",
            "iteration 1265 :train_loss:1.1682629585266113 val_loss1.1223926544189453\n",
            "iteration 1266 :train_loss:1.168252944946289 val_loss1.1223818063735962\n",
            "iteration 1267 :train_loss:1.1682428121566772 val_loss1.1223708391189575\n",
            "iteration 1268 :train_loss:1.1682326793670654 val_loss1.1223599910736084\n",
            "iteration 1269 :train_loss:1.1682225465774536 val_loss1.1223491430282593\n",
            "iteration 1270 :train_loss:1.1682122945785522 val_loss1.1223381757736206\n",
            "iteration 1271 :train_loss:1.1682024002075195 val_loss1.122327208518982\n",
            "iteration 1272 :train_loss:1.1681920289993286 val_loss1.1223163604736328\n",
            "iteration 1273 :train_loss:1.1681820154190063 val_loss1.1223055124282837\n",
            "iteration 1274 :train_loss:1.168171763420105 val_loss1.1222946643829346\n",
            "iteration 1275 :train_loss:1.1681616306304932 val_loss1.122283697128296\n",
            "iteration 1276 :train_loss:1.168151617050171 val_loss1.1222728490829468\n",
            "iteration 1277 :train_loss:1.1681416034698486 val_loss1.1222620010375977\n",
            "iteration 1278 :train_loss:1.1681313514709473 val_loss1.122251033782959\n",
            "iteration 1279 :train_loss:1.1681212186813354 val_loss1.1222401857376099\n",
            "iteration 1280 :train_loss:1.168110966682434 val_loss1.1222290992736816\n",
            "iteration 1281 :train_loss:1.1681010723114014 val_loss1.1222184896469116\n",
            "iteration 1282 :train_loss:1.1680909395217896 val_loss1.1222074031829834\n",
            "iteration 1283 :train_loss:1.1680806875228882 val_loss1.1221966743469238\n",
            "iteration 1284 :train_loss:1.168070673942566 val_loss1.1221858263015747\n",
            "iteration 1285 :train_loss:1.168060541152954 val_loss1.122174859046936\n",
            "iteration 1286 :train_loss:1.1680504083633423 val_loss1.1221638917922974\n",
            "iteration 1287 :train_loss:1.1680402755737305 val_loss1.1221531629562378\n",
            "iteration 1288 :train_loss:1.1680301427841187 val_loss1.1221423149108887\n",
            "iteration 1289 :train_loss:1.1680198907852173 val_loss1.12213134765625\n",
            "iteration 1290 :train_loss:1.168009877204895 val_loss1.1221206188201904\n",
            "iteration 1291 :train_loss:1.1679997444152832 val_loss1.1221095323562622\n",
            "iteration 1292 :train_loss:1.1679896116256714 val_loss1.1220988035202026\n",
            "iteration 1293 :train_loss:1.1679794788360596 val_loss1.122087836265564\n",
            "iteration 1294 :train_loss:1.1679694652557373 val_loss1.1220769882202148\n",
            "iteration 1295 :train_loss:1.1679593324661255 val_loss1.1220662593841553\n",
            "iteration 1296 :train_loss:1.1679491996765137 val_loss1.1220552921295166\n",
            "iteration 1297 :train_loss:1.1679390668869019 val_loss1.122044324874878\n",
            "iteration 1298 :train_loss:1.16792893409729 val_loss1.1220335960388184\n",
            "iteration 1299 :train_loss:1.1679189205169678 val_loss1.1220226287841797\n",
            "iteration 1300 :train_loss:1.1679089069366455 val_loss1.1220117807388306\n",
            "iteration 1301 :train_loss:1.1678987741470337 val_loss1.1220009326934814\n",
            "iteration 1302 :train_loss:1.1678886413574219 val_loss1.1219900846481323\n",
            "iteration 1303 :train_loss:1.16787850856781 val_loss1.121978998184204\n",
            "iteration 1304 :train_loss:1.1678683757781982 val_loss1.1219682693481445\n",
            "iteration 1305 :train_loss:1.1678581237792969 val_loss1.1219573020935059\n",
            "iteration 1306 :train_loss:1.1678482294082642 val_loss1.1219465732574463\n",
            "iteration 1307 :train_loss:1.1678379774093628 val_loss1.1219357252120972\n",
            "iteration 1308 :train_loss:1.1678279638290405 val_loss1.121924877166748\n",
            "iteration 1309 :train_loss:1.1678178310394287 val_loss1.121914029121399\n",
            "iteration 1310 :train_loss:1.167807698249817 val_loss1.1219030618667603\n",
            "iteration 1311 :train_loss:1.1677976846694946 val_loss1.1218923330307007\n",
            "iteration 1312 :train_loss:1.1677875518798828 val_loss1.1218814849853516\n",
            "iteration 1313 :train_loss:1.167777419090271 val_loss1.121870517730713\n",
            "iteration 1314 :train_loss:1.1677674055099487 val_loss1.1218597888946533\n",
            "iteration 1315 :train_loss:1.1677573919296265 val_loss1.1218489408493042\n",
            "iteration 1316 :train_loss:1.167747139930725 val_loss1.121838092803955\n",
            "iteration 1317 :train_loss:1.1677371263504028 val_loss1.1218273639678955\n",
            "iteration 1318 :train_loss:1.167726993560791 val_loss1.1218162775039673\n",
            "iteration 1319 :train_loss:1.1677169799804688 val_loss1.1218055486679077\n",
            "iteration 1320 :train_loss:1.167706847190857 val_loss1.1217947006225586\n",
            "iteration 1321 :train_loss:1.1676967144012451 val_loss1.1217838525772095\n",
            "iteration 1322 :train_loss:1.1676867008209229 val_loss1.12177312374115\n",
            "iteration 1323 :train_loss:1.1676764488220215 val_loss1.1217621564865112\n",
            "iteration 1324 :train_loss:1.1676664352416992 val_loss1.121751308441162\n",
            "iteration 1325 :train_loss:1.167656421661377 val_loss1.121740460395813\n",
            "iteration 1326 :train_loss:1.1676462888717651 val_loss1.1217294931411743\n",
            "iteration 1327 :train_loss:1.1676361560821533 val_loss1.1217186450958252\n",
            "iteration 1328 :train_loss:1.1676262617111206 val_loss1.1217080354690552\n",
            "iteration 1329 :train_loss:1.1676160097122192 val_loss1.121696949005127\n",
            "iteration 1330 :train_loss:1.167605996131897 val_loss1.121686339378357\n",
            "iteration 1331 :train_loss:1.1675959825515747 val_loss1.1216754913330078\n",
            "iteration 1332 :train_loss:1.1675857305526733 val_loss1.1216646432876587\n",
            "iteration 1333 :train_loss:1.167575716972351 val_loss1.1216537952423096\n",
            "iteration 1334 :train_loss:1.1675655841827393 val_loss1.1216429471969604\n",
            "iteration 1335 :train_loss:1.167555570602417 val_loss1.1216319799423218\n",
            "iteration 1336 :train_loss:1.1675455570220947 val_loss1.1216211318969727\n",
            "iteration 1337 :train_loss:1.1675355434417725 val_loss1.121610403060913\n",
            "iteration 1338 :train_loss:1.1675254106521606 val_loss1.121599555015564\n",
            "iteration 1339 :train_loss:1.1675152778625488 val_loss1.1215887069702148\n",
            "iteration 1340 :train_loss:1.167505145072937 val_loss1.1215778589248657\n",
            "iteration 1341 :train_loss:1.1674951314926147 val_loss1.1215670108795166\n",
            "iteration 1342 :train_loss:1.167484998703003 val_loss1.121556282043457\n",
            "iteration 1343 :train_loss:1.1674749851226807 val_loss1.1215453147888184\n",
            "iteration 1344 :train_loss:1.1674649715423584 val_loss1.1215344667434692\n",
            "iteration 1345 :train_loss:1.167454719543457 val_loss1.1215237379074097\n",
            "iteration 1346 :train_loss:1.1674447059631348 val_loss1.1215128898620605\n",
            "iteration 1347 :train_loss:1.1674346923828125 val_loss1.121502161026001\n",
            "iteration 1348 :train_loss:1.1674245595932007 val_loss1.1214911937713623\n",
            "iteration 1349 :train_loss:1.167414665222168 val_loss1.1214803457260132\n",
            "iteration 1350 :train_loss:1.1674045324325562 val_loss1.121469497680664\n",
            "iteration 1351 :train_loss:1.1673943996429443 val_loss1.1214587688446045\n",
            "iteration 1352 :train_loss:1.1673842668533325 val_loss1.121448040008545\n",
            "iteration 1353 :train_loss:1.1673743724822998 val_loss1.1214370727539062\n",
            "iteration 1354 :train_loss:1.167364239692688 val_loss1.1214263439178467\n",
            "iteration 1355 :train_loss:1.1673541069030762 val_loss1.121415615081787\n",
            "iteration 1356 :train_loss:1.167344093322754 val_loss1.1214046478271484\n",
            "iteration 1357 :train_loss:1.167333960533142 val_loss1.1213937997817993\n",
            "iteration 1358 :train_loss:1.1673239469528198 val_loss1.1213829517364502\n",
            "iteration 1359 :train_loss:1.1673139333724976 val_loss1.1213723421096802\n",
            "iteration 1360 :train_loss:1.1673038005828857 val_loss1.121361494064331\n",
            "iteration 1361 :train_loss:1.1672937870025635 val_loss1.1213505268096924\n",
            "iteration 1362 :train_loss:1.1672837734222412 val_loss1.1213397979736328\n",
            "iteration 1363 :train_loss:1.1672736406326294 val_loss1.1213289499282837\n",
            "iteration 1364 :train_loss:1.1672636270523071 val_loss1.1213182210922241\n",
            "iteration 1365 :train_loss:1.1672536134719849 val_loss1.121307373046875\n",
            "iteration 1366 :train_loss:1.167243480682373 val_loss1.1212965250015259\n",
            "iteration 1367 :train_loss:1.1672334671020508 val_loss1.1212859153747559\n",
            "iteration 1368 :train_loss:1.1672234535217285 val_loss1.1212750673294067\n",
            "iteration 1369 :train_loss:1.1672132015228271 val_loss1.121264100074768\n",
            "iteration 1370 :train_loss:1.1672031879425049 val_loss1.121253252029419\n",
            "iteration 1371 :train_loss:1.1671934127807617 val_loss1.1212425231933594\n",
            "iteration 1372 :train_loss:1.1671831607818604 val_loss1.1212315559387207\n",
            "iteration 1373 :train_loss:1.1671730279922485 val_loss1.1212208271026611\n",
            "iteration 1374 :train_loss:1.1671630144119263 val_loss1.121209979057312\n",
            "iteration 1375 :train_loss:1.1671528816223145 val_loss1.1211992502212524\n",
            "iteration 1376 :train_loss:1.1671428680419922 val_loss1.1211884021759033\n",
            "iteration 1377 :train_loss:1.1671329736709595 val_loss1.1211776733398438\n",
            "iteration 1378 :train_loss:1.1671228408813477 val_loss1.1211668252944946\n",
            "iteration 1379 :train_loss:1.1671128273010254 val_loss1.1211559772491455\n",
            "iteration 1380 :train_loss:1.167102575302124 val_loss1.121145248413086\n",
            "iteration 1381 :train_loss:1.1670926809310913 val_loss1.1211345195770264\n",
            "iteration 1382 :train_loss:1.1670825481414795 val_loss1.1211236715316772\n",
            "iteration 1383 :train_loss:1.1670725345611572 val_loss1.1211129426956177\n",
            "iteration 1384 :train_loss:1.167062520980835 val_loss1.121101975440979\n",
            "iteration 1385 :train_loss:1.1670523881912231 val_loss1.1210912466049194\n",
            "iteration 1386 :train_loss:1.1670424938201904 val_loss1.1210805177688599\n",
            "iteration 1387 :train_loss:1.1670323610305786 val_loss1.1210696697235107\n",
            "iteration 1388 :train_loss:1.1670223474502563 val_loss1.1210588216781616\n",
            "iteration 1389 :train_loss:1.1670122146606445 val_loss1.121048092842102\n",
            "iteration 1390 :train_loss:1.1670023202896118 val_loss1.1210373640060425\n",
            "iteration 1391 :train_loss:1.1669921875 val_loss1.1210262775421143\n",
            "iteration 1392 :train_loss:1.1669821739196777 val_loss1.1210156679153442\n",
            "iteration 1393 :train_loss:1.1669721603393555 val_loss1.1210047006607056\n",
            "iteration 1394 :train_loss:1.1669620275497437 val_loss1.1209940910339355\n",
            "iteration 1395 :train_loss:1.1669520139694214 val_loss1.1209831237792969\n",
            "iteration 1396 :train_loss:1.1669421195983887 val_loss1.1209725141525269\n",
            "iteration 1397 :train_loss:1.1669319868087769 val_loss1.1209616661071777\n",
            "iteration 1398 :train_loss:1.166921854019165 val_loss1.1209508180618286\n",
            "iteration 1399 :train_loss:1.1669118404388428 val_loss1.1209402084350586\n",
            "iteration 1400 :train_loss:1.1669018268585205 val_loss1.1209291219711304\n",
            "iteration 1401 :train_loss:1.1668919324874878 val_loss1.12091863155365\n",
            "iteration 1402 :train_loss:1.1668816804885864 val_loss1.1209077835083008\n",
            "iteration 1403 :train_loss:1.1668717861175537 val_loss1.1208969354629517\n",
            "iteration 1404 :train_loss:1.1668617725372314 val_loss1.120886206626892\n",
            "iteration 1405 :train_loss:1.1668516397476196 val_loss1.120875358581543\n",
            "iteration 1406 :train_loss:1.1668416261672974 val_loss1.1208646297454834\n",
            "iteration 1407 :train_loss:1.166831612586975 val_loss1.1208539009094238\n",
            "iteration 1408 :train_loss:1.1668215990066528 val_loss1.1208430528640747\n",
            "iteration 1409 :train_loss:1.1668115854263306 val_loss1.1208323240280151\n",
            "iteration 1410 :train_loss:1.1668014526367188 val_loss1.120821475982666\n",
            "iteration 1411 :train_loss:1.1667914390563965 val_loss1.1208107471466064\n",
            "iteration 1412 :train_loss:1.1667815446853638 val_loss1.1207998991012573\n",
            "iteration 1413 :train_loss:1.166771411895752 val_loss1.1207891702651978\n",
            "iteration 1414 :train_loss:1.1667613983154297 val_loss1.120778203010559\n",
            "iteration 1415 :train_loss:1.1667513847351074 val_loss1.120767593383789\n",
            "iteration 1416 :train_loss:1.1667414903640747 val_loss1.1207568645477295\n",
            "iteration 1417 :train_loss:1.166731357574463 val_loss1.1207458972930908\n",
            "iteration 1418 :train_loss:1.166721224784851 val_loss1.1207351684570312\n",
            "iteration 1419 :train_loss:1.1667113304138184 val_loss1.1207245588302612\n",
            "iteration 1420 :train_loss:1.166701316833496 val_loss1.120713710784912\n",
            "iteration 1421 :train_loss:1.1666913032531738 val_loss1.1207029819488525\n",
            "iteration 1422 :train_loss:1.166681170463562 val_loss1.1206920146942139\n",
            "iteration 1423 :train_loss:1.1666711568832397 val_loss1.1206812858581543\n",
            "iteration 1424 :train_loss:1.166661262512207 val_loss1.1206705570220947\n",
            "iteration 1425 :train_loss:1.1666511297225952 val_loss1.1206598281860352\n",
            "iteration 1426 :train_loss:1.166641116142273 val_loss1.1206490993499756\n",
            "iteration 1427 :train_loss:1.1666311025619507 val_loss1.120638370513916\n",
            "iteration 1428 :train_loss:1.1666210889816284 val_loss1.1206276416778564\n",
            "iteration 1429 :train_loss:1.1666110754013062 val_loss1.1206167936325073\n",
            "iteration 1430 :train_loss:1.1666009426116943 val_loss1.1206059455871582\n",
            "iteration 1431 :train_loss:1.1665910482406616 val_loss1.120595097541809\n",
            "iteration 1432 :train_loss:1.1665809154510498 val_loss1.120584487915039\n",
            "iteration 1433 :train_loss:1.166571021080017 val_loss1.1205737590789795\n",
            "iteration 1434 :train_loss:1.1665610074996948 val_loss1.1205629110336304\n",
            "iteration 1435 :train_loss:1.166550874710083 val_loss1.1205521821975708\n",
            "iteration 1436 :train_loss:1.1665409803390503 val_loss1.1205412149429321\n",
            "iteration 1437 :train_loss:1.1665308475494385 val_loss1.1205307245254517\n",
            "iteration 1438 :train_loss:1.1665207147598267 val_loss1.120519995689392\n",
            "iteration 1439 :train_loss:1.1665109395980835 val_loss1.120509147644043\n",
            "iteration 1440 :train_loss:1.1665009260177612 val_loss1.1204982995986938\n",
            "iteration 1441 :train_loss:1.1664907932281494 val_loss1.1204875707626343\n",
            "iteration 1442 :train_loss:1.1664807796478271 val_loss1.1204769611358643\n",
            "iteration 1443 :train_loss:1.1664707660675049 val_loss1.1204659938812256\n",
            "iteration 1444 :train_loss:1.1664608716964722 val_loss1.120455265045166\n",
            "iteration 1445 :train_loss:1.16645085811615 val_loss1.1204445362091064\n",
            "iteration 1446 :train_loss:1.166440725326538 val_loss1.1204336881637573\n",
            "iteration 1447 :train_loss:1.1664307117462158 val_loss1.1204229593276978\n",
            "iteration 1448 :train_loss:1.1664206981658936 val_loss1.1204121112823486\n",
            "iteration 1449 :train_loss:1.1664106845855713 val_loss1.1204015016555786\n",
            "iteration 1450 :train_loss:1.166400671005249 val_loss1.120390772819519\n",
            "iteration 1451 :train_loss:1.1663906574249268 val_loss1.1203798055648804\n",
            "iteration 1452 :train_loss:1.166380763053894 val_loss1.1203691959381104\n",
            "iteration 1453 :train_loss:1.1663707494735718 val_loss1.1203584671020508\n",
            "iteration 1454 :train_loss:1.1663607358932495 val_loss1.1203477382659912\n",
            "iteration 1455 :train_loss:1.1663507223129272 val_loss1.120336890220642\n",
            "iteration 1456 :train_loss:1.166340947151184 val_loss1.1203261613845825\n",
            "iteration 1457 :train_loss:1.1663309335708618 val_loss1.1203153133392334\n",
            "iteration 1458 :train_loss:1.1663206815719604 val_loss1.1203047037124634\n",
            "iteration 1459 :train_loss:1.1663106679916382 val_loss1.1202938556671143\n",
            "iteration 1460 :train_loss:1.166300654411316 val_loss1.1202832460403442\n",
            "iteration 1461 :train_loss:1.1662907600402832 val_loss1.1202723979949951\n",
            "iteration 1462 :train_loss:1.166280746459961 val_loss1.120261549949646\n",
            "iteration 1463 :train_loss:1.1662707328796387 val_loss1.120250940322876\n",
            "iteration 1464 :train_loss:1.1662607192993164 val_loss1.1202402114868164\n",
            "iteration 1465 :train_loss:1.1662507057189941 val_loss1.1202293634414673\n",
            "iteration 1466 :train_loss:1.1662405729293823 val_loss1.1202187538146973\n",
            "iteration 1467 :train_loss:1.1662304401397705 val_loss1.1202077865600586\n",
            "iteration 1468 :train_loss:1.1662206649780273 val_loss1.120197057723999\n",
            "iteration 1469 :train_loss:1.166210651397705 val_loss1.120186448097229\n",
            "iteration 1470 :train_loss:1.1662006378173828 val_loss1.1201756000518799\n",
            "iteration 1471 :train_loss:1.16619074344635 val_loss1.1201647520065308\n",
            "iteration 1472 :train_loss:1.1661807298660278 val_loss1.1201541423797607\n",
            "iteration 1473 :train_loss:1.166170597076416 val_loss1.1201432943344116\n",
            "iteration 1474 :train_loss:1.1661605834960938 val_loss1.1201326847076416\n",
            "iteration 1475 :train_loss:1.1661505699157715 val_loss1.1201220750808716\n",
            "iteration 1476 :train_loss:1.1661406755447388 val_loss1.120111107826233\n",
            "iteration 1477 :train_loss:1.1661306619644165 val_loss1.1201003789901733\n",
            "iteration 1478 :train_loss:1.1661206483840942 val_loss1.1200897693634033\n",
            "iteration 1479 :train_loss:1.1661107540130615 val_loss1.1200790405273438\n",
            "iteration 1480 :train_loss:1.1661006212234497 val_loss1.120068073272705\n",
            "iteration 1481 :train_loss:1.1660906076431274 val_loss1.120057463645935\n",
            "iteration 1482 :train_loss:1.1660807132720947 val_loss1.1200467348098755\n",
            "iteration 1483 :train_loss:1.166070580482483 val_loss1.120036005973816\n",
            "iteration 1484 :train_loss:1.1660606861114502 val_loss1.1200251579284668\n",
            "iteration 1485 :train_loss:1.166050672531128 val_loss1.1200145483016968\n",
            "iteration 1486 :train_loss:1.1660407781600952 val_loss1.1200037002563477\n",
            "iteration 1487 :train_loss:1.1660306453704834 val_loss1.1199930906295776\n",
            "iteration 1488 :train_loss:1.1660206317901611 val_loss1.119982361793518\n",
            "iteration 1489 :train_loss:1.1660106182098389 val_loss1.119971513748169\n",
            "iteration 1490 :train_loss:1.1660007238388062 val_loss1.119960904121399\n",
            "iteration 1491 :train_loss:1.1659908294677734 val_loss1.1199501752853394\n",
            "iteration 1492 :train_loss:1.1659808158874512 val_loss1.1199394464492798\n",
            "iteration 1493 :train_loss:1.165970802307129 val_loss1.1199285984039307\n",
            "iteration 1494 :train_loss:1.1659607887268066 val_loss1.119917869567871\n",
            "iteration 1495 :train_loss:1.1659506559371948 val_loss1.1199071407318115\n",
            "iteration 1496 :train_loss:1.1659406423568726 val_loss1.119896411895752\n",
            "iteration 1497 :train_loss:1.1659307479858398 val_loss1.1198855638504028\n",
            "iteration 1498 :train_loss:1.1659207344055176 val_loss1.1198748350143433\n",
            "iteration 1499 :train_loss:1.1659108400344849 val_loss1.1198641061782837\n",
            "iteration 1500 :train_loss:1.165900707244873 val_loss1.1198534965515137\n",
            "iteration 1501 :train_loss:1.1658909320831299 val_loss1.1198426485061646\n",
            "iteration 1502 :train_loss:1.165880799293518 val_loss1.119831919670105\n",
            "iteration 1503 :train_loss:1.1658709049224854 val_loss1.1198211908340454\n",
            "iteration 1504 :train_loss:1.1658607721328735 val_loss1.1198104619979858\n",
            "iteration 1505 :train_loss:1.1658507585525513 val_loss1.1197998523712158\n",
            "iteration 1506 :train_loss:1.1658408641815186 val_loss1.1197891235351562\n",
            "iteration 1507 :train_loss:1.1658309698104858 val_loss1.1197783946990967\n",
            "iteration 1508 :train_loss:1.165820837020874 val_loss1.1197675466537476\n",
            "iteration 1509 :train_loss:1.1658108234405518 val_loss1.1197569370269775\n",
            "iteration 1510 :train_loss:1.165800929069519 val_loss1.119746208190918\n",
            "iteration 1511 :train_loss:1.1657909154891968 val_loss1.1197354793548584\n",
            "iteration 1512 :train_loss:1.165780782699585 val_loss1.1197246313095093\n",
            "iteration 1513 :train_loss:1.1657708883285522 val_loss1.1197140216827393\n",
            "iteration 1514 :train_loss:1.1657609939575195 val_loss1.1197031736373901\n",
            "iteration 1515 :train_loss:1.1657509803771973 val_loss1.119692325592041\n",
            "iteration 1516 :train_loss:1.165740966796875 val_loss1.119681715965271\n",
            "iteration 1517 :train_loss:1.1657309532165527 val_loss1.1196709871292114\n",
            "iteration 1518 :train_loss:1.16572105884552 val_loss1.1196602582931519\n",
            "iteration 1519 :train_loss:1.1657110452651978 val_loss1.1196495294570923\n",
            "iteration 1520 :train_loss:1.165700912475586 val_loss1.1196388006210327\n",
            "iteration 1521 :train_loss:1.1656910181045532 val_loss1.1196280717849731\n",
            "iteration 1522 :train_loss:1.165681004524231 val_loss1.1196173429489136\n",
            "iteration 1523 :train_loss:1.1656711101531982 val_loss1.1196064949035645\n",
            "iteration 1524 :train_loss:1.1656612157821655 val_loss1.119596004486084\n",
            "iteration 1525 :train_loss:1.1656510829925537 val_loss1.1195851564407349\n",
            "iteration 1526 :train_loss:1.1656410694122314 val_loss1.1195745468139648\n",
            "iteration 1527 :train_loss:1.1656312942504883 val_loss1.1195638179779053\n",
            "iteration 1528 :train_loss:1.165621280670166 val_loss1.1195529699325562\n",
            "iteration 1529 :train_loss:1.1656112670898438 val_loss1.1195422410964966\n",
            "iteration 1530 :train_loss:1.1656012535095215 val_loss1.119531512260437\n",
            "iteration 1531 :train_loss:1.1655912399291992 val_loss1.119520902633667\n",
            "iteration 1532 :train_loss:1.1655813455581665 val_loss1.1195101737976074\n",
            "iteration 1533 :train_loss:1.1655714511871338 val_loss1.1194995641708374\n",
            "iteration 1534 :train_loss:1.1655614376068115 val_loss1.1194887161254883\n",
            "iteration 1535 :train_loss:1.1655514240264893 val_loss1.1194779872894287\n",
            "iteration 1536 :train_loss:1.1655415296554565 val_loss1.1194672584533691\n",
            "iteration 1537 :train_loss:1.1655313968658447 val_loss1.1194565296173096\n",
            "iteration 1538 :train_loss:1.165521502494812 val_loss1.1194456815719604\n",
            "iteration 1539 :train_loss:1.1655113697052002 val_loss1.1194350719451904\n",
            "iteration 1540 :train_loss:1.1655014753341675 val_loss1.1194242238998413\n",
            "iteration 1541 :train_loss:1.1654914617538452 val_loss1.1194137334823608\n",
            "iteration 1542 :train_loss:1.1654815673828125 val_loss1.1194027662277222\n",
            "iteration 1543 :train_loss:1.1654715538024902 val_loss1.1193922758102417\n",
            "iteration 1544 :train_loss:1.1654616594314575 val_loss1.1193815469741821\n",
            "iteration 1545 :train_loss:1.1654517650604248 val_loss1.119370698928833\n",
            "iteration 1546 :train_loss:1.1654417514801025 val_loss1.119360089302063\n",
            "iteration 1547 :train_loss:1.1654317378997803 val_loss1.1193493604660034\n",
            "iteration 1548 :train_loss:1.165421724319458 val_loss1.1193386316299438\n",
            "iteration 1549 :train_loss:1.1654118299484253 val_loss1.1193280220031738\n",
            "iteration 1550 :train_loss:1.165401816368103 val_loss1.1193172931671143\n",
            "iteration 1551 :train_loss:1.1653916835784912 val_loss1.1193064451217651\n",
            "iteration 1552 :train_loss:1.165381908416748 val_loss1.1192958354949951\n",
            "iteration 1553 :train_loss:1.1653720140457153 val_loss1.119284987449646\n",
            "iteration 1554 :train_loss:1.165362000465393 val_loss1.119274377822876\n",
            "iteration 1555 :train_loss:1.1653519868850708 val_loss1.1192636489868164\n",
            "iteration 1556 :train_loss:1.165342092514038 val_loss1.1192530393600464\n",
            "iteration 1557 :train_loss:1.1653320789337158 val_loss1.1192423105239868\n",
            "iteration 1558 :train_loss:1.1653220653533936 val_loss1.1192313432693481\n",
            "iteration 1559 :train_loss:1.1653120517730713 val_loss1.1192207336425781\n",
            "iteration 1560 :train_loss:1.165302038192749 val_loss1.119210124015808\n",
            "iteration 1561 :train_loss:1.1652921438217163 val_loss1.1191993951797485\n",
            "iteration 1562 :train_loss:1.1652822494506836 val_loss1.1191885471343994\n",
            "iteration 1563 :train_loss:1.1652722358703613 val_loss1.1191779375076294\n",
            "iteration 1564 :train_loss:1.165262222290039 val_loss1.1191672086715698\n",
            "iteration 1565 :train_loss:1.1652522087097168 val_loss1.1191564798355103\n",
            "iteration 1566 :train_loss:1.165242314338684 val_loss1.1191457509994507\n",
            "iteration 1567 :train_loss:1.1652324199676514 val_loss1.1191351413726807\n",
            "iteration 1568 :train_loss:1.1652222871780396 val_loss1.119124412536621\n",
            "iteration 1569 :train_loss:1.1652123928070068 val_loss1.1191136837005615\n",
            "iteration 1570 :train_loss:1.1652023792266846 val_loss1.119102954864502\n",
            "iteration 1571 :train_loss:1.1651926040649414 val_loss1.1190922260284424\n",
            "iteration 1572 :train_loss:1.1651824712753296 val_loss1.1190816164016724\n",
            "iteration 1573 :train_loss:1.1651725769042969 val_loss1.1190708875656128\n",
            "iteration 1574 :train_loss:1.1651625633239746 val_loss1.1190601587295532\n",
            "iteration 1575 :train_loss:1.1651527881622314 val_loss1.1190495491027832\n",
            "iteration 1576 :train_loss:1.1651427745819092 val_loss1.119038701057434\n",
            "iteration 1577 :train_loss:1.1651326417922974 val_loss1.119027853012085\n",
            "iteration 1578 :train_loss:1.1651227474212646 val_loss1.1190173625946045\n",
            "iteration 1579 :train_loss:1.165112853050232 val_loss1.119006633758545\n",
            "iteration 1580 :train_loss:1.1651028394699097 val_loss1.118996024131775\n",
            "iteration 1581 :train_loss:1.1650928258895874 val_loss1.1189851760864258\n",
            "iteration 1582 :train_loss:1.1650829315185547 val_loss1.1189745664596558\n",
            "iteration 1583 :train_loss:1.1650729179382324 val_loss1.1189639568328857\n",
            "iteration 1584 :train_loss:1.1650630235671997 val_loss1.1189531087875366\n",
            "iteration 1585 :train_loss:1.165052890777588 val_loss1.1189422607421875\n",
            "iteration 1586 :train_loss:1.1650431156158447 val_loss1.1189316511154175\n",
            "iteration 1587 :train_loss:1.165033221244812 val_loss1.118920922279358\n",
            "iteration 1588 :train_loss:1.1650230884552002 val_loss1.118910312652588\n",
            "iteration 1589 :train_loss:1.1650131940841675 val_loss1.1188997030258179\n",
            "iteration 1590 :train_loss:1.1650032997131348 val_loss1.1188888549804688\n",
            "iteration 1591 :train_loss:1.1649932861328125 val_loss1.1188781261444092\n",
            "iteration 1592 :train_loss:1.1649831533432007 val_loss1.1188673973083496\n",
            "iteration 1593 :train_loss:1.164973258972168 val_loss1.1188569068908691\n",
            "iteration 1594 :train_loss:1.1649633646011353 val_loss1.11884605884552\n",
            "iteration 1595 :train_loss:1.164953351020813 val_loss1.118835210800171\n",
            "iteration 1596 :train_loss:1.1649432182312012 val_loss1.1188246011734009\n",
            "iteration 1597 :train_loss:1.164933443069458 val_loss1.1188139915466309\n",
            "iteration 1598 :train_loss:1.1649235486984253 val_loss1.1188032627105713\n",
            "iteration 1599 :train_loss:1.164913535118103 val_loss1.1187925338745117\n",
            "iteration 1600 :train_loss:1.1649035215377808 val_loss1.1187816858291626\n",
            "iteration 1601 :train_loss:1.164893627166748 val_loss1.1187711954116821\n",
            "iteration 1602 :train_loss:1.1648836135864258 val_loss1.1187604665756226\n",
            "iteration 1603 :train_loss:1.164873719215393 val_loss1.1187498569488525\n",
            "iteration 1604 :train_loss:1.1648635864257812 val_loss1.1187390089035034\n",
            "iteration 1605 :train_loss:1.164853811264038 val_loss1.1187282800674438\n",
            "iteration 1606 :train_loss:1.1648437976837158 val_loss1.1187176704406738\n",
            "iteration 1607 :train_loss:1.1648337841033936 val_loss1.1187069416046143\n",
            "iteration 1608 :train_loss:1.1648238897323608 val_loss1.1186962127685547\n",
            "iteration 1609 :train_loss:1.1648139953613281 val_loss1.1186854839324951\n",
            "iteration 1610 :train_loss:1.1648039817810059 val_loss1.118674874305725\n",
            "iteration 1611 :train_loss:1.1647940874099731 val_loss1.118664264678955\n",
            "iteration 1612 :train_loss:1.1647841930389404 val_loss1.1186532974243164\n",
            "iteration 1613 :train_loss:1.1647741794586182 val_loss1.118642807006836\n",
            "iteration 1614 :train_loss:1.164764165878296 val_loss1.1186319589614868\n",
            "iteration 1615 :train_loss:1.1647541522979736 val_loss1.1186213493347168\n",
            "iteration 1616 :train_loss:1.164744257926941 val_loss1.1186106204986572\n",
            "iteration 1617 :train_loss:1.1647343635559082 val_loss1.1186000108718872\n",
            "iteration 1618 :train_loss:1.164724349975586 val_loss1.1185894012451172\n",
            "iteration 1619 :train_loss:1.1647144556045532 val_loss1.118578553199768\n",
            "iteration 1620 :train_loss:1.164704442024231 val_loss1.1185678243637085\n",
            "iteration 1621 :train_loss:1.1646944284439087 val_loss1.118557095527649\n",
            "iteration 1622 :train_loss:1.164684534072876 val_loss1.118546485900879\n",
            "iteration 1623 :train_loss:1.1646745204925537 val_loss1.1185358762741089\n",
            "iteration 1624 :train_loss:1.1646647453308105 val_loss1.1185250282287598\n",
            "iteration 1625 :train_loss:1.1646546125411987 val_loss1.1185144186019897\n",
            "iteration 1626 :train_loss:1.164644718170166 val_loss1.1185036897659302\n",
            "iteration 1627 :train_loss:1.1646348237991333 val_loss1.1184930801391602\n",
            "iteration 1628 :train_loss:1.1646246910095215 val_loss1.1184824705123901\n",
            "iteration 1629 :train_loss:1.1646147966384888 val_loss1.118471622467041\n",
            "iteration 1630 :train_loss:1.164604902267456 val_loss1.1184608936309814\n",
            "iteration 1631 :train_loss:1.1645948886871338 val_loss1.1184501647949219\n",
            "iteration 1632 :train_loss:1.164584994316101 val_loss1.1184395551681519\n",
            "iteration 1633 :train_loss:1.1645750999450684 val_loss1.1184288263320923\n",
            "iteration 1634 :train_loss:1.1645649671554565 val_loss1.1184180974960327\n",
            "iteration 1635 :train_loss:1.1645550727844238 val_loss1.1184076070785522\n",
            "iteration 1636 :train_loss:1.1645451784133911 val_loss1.1183968782424927\n",
            "iteration 1637 :train_loss:1.1645352840423584 val_loss1.1183860301971436\n",
            "iteration 1638 :train_loss:1.1645253896713257 val_loss1.1183754205703735\n",
            "iteration 1639 :train_loss:1.1645153760910034 val_loss1.118364691734314\n",
            "iteration 1640 :train_loss:1.1645053625106812 val_loss1.118354082107544\n",
            "iteration 1641 :train_loss:1.1644953489303589 val_loss1.1183433532714844\n",
            "iteration 1642 :train_loss:1.1644854545593262 val_loss1.1183326244354248\n",
            "iteration 1643 :train_loss:1.1644755601882935 val_loss1.1183220148086548\n",
            "iteration 1644 :train_loss:1.1644654273986816 val_loss1.1183112859725952\n",
            "iteration 1645 :train_loss:1.1644556522369385 val_loss1.1183005571365356\n",
            "iteration 1646 :train_loss:1.1644456386566162 val_loss1.118289828300476\n",
            "iteration 1647 :train_loss:1.1644357442855835 val_loss1.118279218673706\n",
            "iteration 1648 :train_loss:1.1644257307052612 val_loss1.1182684898376465\n",
            "iteration 1649 :train_loss:1.1644158363342285 val_loss1.118257761001587\n",
            "iteration 1650 :train_loss:1.1644058227539062 val_loss1.1182470321655273\n",
            "iteration 1651 :train_loss:1.1643959283828735 val_loss1.1182364225387573\n",
            "iteration 1652 :train_loss:1.1643859148025513 val_loss1.1182258129119873\n",
            "iteration 1653 :train_loss:1.1643760204315186 val_loss1.1182150840759277\n",
            "iteration 1654 :train_loss:1.1643661260604858 val_loss1.1182044744491577\n",
            "iteration 1655 :train_loss:1.1643561124801636 val_loss1.1181936264038086\n",
            "iteration 1656 :train_loss:1.1643460988998413 val_loss1.1181830167770386\n",
            "iteration 1657 :train_loss:1.1643362045288086 val_loss1.118172287940979\n",
            "iteration 1658 :train_loss:1.1643261909484863 val_loss1.118161678314209\n",
            "iteration 1659 :train_loss:1.1643162965774536 val_loss1.1181508302688599\n",
            "iteration 1660 :train_loss:1.164306402206421 val_loss1.1181401014328003\n",
            "iteration 1661 :train_loss:1.1642963886260986 val_loss1.1181294918060303\n",
            "iteration 1662 :train_loss:1.1642863750457764 val_loss1.1181188821792603\n",
            "iteration 1663 :train_loss:1.1642765998840332 val_loss1.1181080341339111\n",
            "iteration 1664 :train_loss:1.1642664670944214 val_loss1.1180974245071411\n",
            "iteration 1665 :train_loss:1.1642566919326782 val_loss1.118086814880371\n",
            "iteration 1666 :train_loss:1.1642465591430664 val_loss1.1180760860443115\n",
            "iteration 1667 :train_loss:1.1642366647720337 val_loss1.118065357208252\n",
            "iteration 1668 :train_loss:1.164226770401001 val_loss1.1180546283721924\n",
            "iteration 1669 :train_loss:1.1642168760299683 val_loss1.1180440187454224\n",
            "iteration 1670 :train_loss:1.164206862449646 val_loss1.1180334091186523\n",
            "iteration 1671 :train_loss:1.1641969680786133 val_loss1.1180225610733032\n",
            "iteration 1672 :train_loss:1.1641868352890015 val_loss1.1180118322372437\n",
            "iteration 1673 :train_loss:1.1641769409179688 val_loss1.1180012226104736\n",
            "iteration 1674 :train_loss:1.164167046546936 val_loss1.117990493774414\n",
            "iteration 1675 :train_loss:1.1641571521759033 val_loss1.1179797649383545\n",
            "iteration 1676 :train_loss:1.1641472578048706 val_loss1.1179691553115845\n",
            "iteration 1677 :train_loss:1.1641372442245483 val_loss1.1179585456848145\n",
            "iteration 1678 :train_loss:1.164127230644226 val_loss1.1179478168487549\n",
            "iteration 1679 :train_loss:1.1641173362731934 val_loss1.1179372072219849\n",
            "iteration 1680 :train_loss:1.1641074419021606 val_loss1.1179264783859253\n",
            "iteration 1681 :train_loss:1.1640974283218384 val_loss1.1179158687591553\n",
            "iteration 1682 :train_loss:1.1640874147415161 val_loss1.1179049015045166\n",
            "iteration 1683 :train_loss:1.1640774011611938 val_loss1.1178944110870361\n",
            "iteration 1684 :train_loss:1.1640676259994507 val_loss1.117883563041687\n",
            "iteration 1685 :train_loss:1.164057731628418 val_loss1.1178728342056274\n",
            "iteration 1686 :train_loss:1.1640475988388062 val_loss1.1178622245788574\n",
            "iteration 1687 :train_loss:1.1640377044677734 val_loss1.1178516149520874\n",
            "iteration 1688 :train_loss:1.1640279293060303 val_loss1.1178408861160278\n",
            "iteration 1689 :train_loss:1.164017915725708 val_loss1.1178302764892578\n",
            "iteration 1690 :train_loss:1.1640079021453857 val_loss1.1178195476531982\n",
            "iteration 1691 :train_loss:1.1639978885650635 val_loss1.1178088188171387\n",
            "iteration 1692 :train_loss:1.1639879941940308 val_loss1.1177983283996582\n",
            "iteration 1693 :train_loss:1.163978099822998 val_loss1.117787480354309\n",
            "iteration 1694 :train_loss:1.1639680862426758 val_loss1.117776870727539\n",
            "iteration 1695 :train_loss:1.1639580726623535 val_loss1.11776602268219\n",
            "iteration 1696 :train_loss:1.1639482975006104 val_loss1.1177555322647095\n",
            "iteration 1697 :train_loss:1.163938283920288 val_loss1.1177446842193604\n",
            "iteration 1698 :train_loss:1.1639282703399658 val_loss1.1177340745925903\n",
            "iteration 1699 :train_loss:1.1639182567596436 val_loss1.1177234649658203\n",
            "iteration 1700 :train_loss:1.1639083623886108 val_loss1.1177127361297607\n",
            "iteration 1701 :train_loss:1.1638984680175781 val_loss1.1177018880844116\n",
            "iteration 1702 :train_loss:1.1638885736465454 val_loss1.1176912784576416\n",
            "iteration 1703 :train_loss:1.1638786792755127 val_loss1.1176806688308716\n",
            "iteration 1704 :train_loss:1.1638686656951904 val_loss1.1176700592041016\n",
            "iteration 1705 :train_loss:1.1638585329055786 val_loss1.1176592111587524\n",
            "iteration 1706 :train_loss:1.1638487577438354 val_loss1.1176486015319824\n",
            "iteration 1707 :train_loss:1.1638387441635132 val_loss1.1176378726959229\n",
            "iteration 1708 :train_loss:1.1638288497924805 val_loss1.1176272630691528\n",
            "iteration 1709 :train_loss:1.1638188362121582 val_loss1.1176165342330933\n",
            "iteration 1710 :train_loss:1.163809061050415 val_loss1.1176058053970337\n",
            "iteration 1711 :train_loss:1.1637991666793823 val_loss1.1175951957702637\n",
            "iteration 1712 :train_loss:1.16378915309906 val_loss1.1175845861434937\n",
            "iteration 1713 :train_loss:1.1637791395187378 val_loss1.117573857307434\n",
            "iteration 1714 :train_loss:1.163769245147705 val_loss1.1175631284713745\n",
            "iteration 1715 :train_loss:1.1637592315673828 val_loss1.117552399635315\n",
            "iteration 1716 :train_loss:1.16374933719635 val_loss1.1175416707992554\n",
            "iteration 1717 :train_loss:1.1637393236160278 val_loss1.117531180381775\n",
            "iteration 1718 :train_loss:1.1637294292449951 val_loss1.1175204515457153\n",
            "iteration 1719 :train_loss:1.1637195348739624 val_loss1.1175097227096558\n",
            "iteration 1720 :train_loss:1.1637095212936401 val_loss1.1174988746643066\n",
            "iteration 1721 :train_loss:1.1636995077133179 val_loss1.1174882650375366\n",
            "iteration 1722 :train_loss:1.1636894941329956 val_loss1.1174776554107666\n",
            "iteration 1723 :train_loss:1.163679599761963 val_loss1.117466926574707\n",
            "iteration 1724 :train_loss:1.1636697053909302 val_loss1.1174561977386475\n",
            "iteration 1725 :train_loss:1.163659691810608 val_loss1.1174455881118774\n",
            "iteration 1726 :train_loss:1.1636497974395752 val_loss1.1174348592758179\n",
            "iteration 1727 :train_loss:1.1636399030685425 val_loss1.1174241304397583\n",
            "iteration 1728 :train_loss:1.1636298894882202 val_loss1.1174135208129883\n",
            "iteration 1729 :train_loss:1.1636199951171875 val_loss1.1174027919769287\n",
            "iteration 1730 :train_loss:1.1636099815368652 val_loss1.1173921823501587\n",
            "iteration 1731 :train_loss:1.163599967956543 val_loss1.1173815727233887\n",
            "iteration 1732 :train_loss:1.1635903120040894 val_loss1.1173707246780396\n",
            "iteration 1733 :train_loss:1.163580298423767 val_loss1.11735999584198\n",
            "iteration 1734 :train_loss:1.1635702848434448 val_loss1.1173495054244995\n",
            "iteration 1735 :train_loss:1.163560390472412 val_loss1.1173386573791504\n",
            "iteration 1736 :train_loss:1.1635504961013794 val_loss1.1173279285430908\n",
            "iteration 1737 :train_loss:1.1635403633117676 val_loss1.1173174381256104\n",
            "iteration 1738 :train_loss:1.1635304689407349 val_loss1.1173067092895508\n",
            "iteration 1739 :train_loss:1.1635204553604126 val_loss1.1172960996627808\n",
            "iteration 1740 :train_loss:1.1635105609893799 val_loss1.1172853708267212\n",
            "iteration 1741 :train_loss:1.1635005474090576 val_loss1.1172746419906616\n",
            "iteration 1742 :train_loss:1.1634907722473145 val_loss1.117263913154602\n",
            "iteration 1743 :train_loss:1.1634806394577026 val_loss1.117253303527832\n",
            "iteration 1744 :train_loss:1.1634706258773804 val_loss1.117242693901062\n",
            "iteration 1745 :train_loss:1.1634608507156372 val_loss1.117231845855713\n",
            "iteration 1746 :train_loss:1.1634509563446045 val_loss1.1172211170196533\n",
            "iteration 1747 :train_loss:1.1634410619735718 val_loss1.1172105073928833\n",
            "iteration 1748 :train_loss:1.163431167602539 val_loss1.1171998977661133\n",
            "iteration 1749 :train_loss:1.1634210348129272 val_loss1.1171891689300537\n",
            "iteration 1750 :train_loss:1.163411021232605 val_loss1.1171785593032837\n",
            "iteration 1751 :train_loss:1.1634011268615723 val_loss1.1171677112579346\n",
            "iteration 1752 :train_loss:1.1633912324905396 val_loss1.117157220840454\n",
            "iteration 1753 :train_loss:1.1633812189102173 val_loss1.117146611213684\n",
            "iteration 1754 :train_loss:1.1633713245391846 val_loss1.1171356439590454\n",
            "iteration 1755 :train_loss:1.1633614301681519 val_loss1.117125153541565\n",
            "iteration 1756 :train_loss:1.1633514165878296 val_loss1.1171144247055054\n",
            "iteration 1757 :train_loss:1.1633415222167969 val_loss1.1171035766601562\n",
            "iteration 1758 :train_loss:1.1633315086364746 val_loss1.1170932054519653\n",
            "iteration 1759 :train_loss:1.1633214950561523 val_loss1.1170823574066162\n",
            "iteration 1760 :train_loss:1.1633116006851196 val_loss1.117071509361267\n",
            "iteration 1761 :train_loss:1.163301706314087 val_loss1.1170611381530762\n",
            "iteration 1762 :train_loss:1.1632916927337646 val_loss1.1170504093170166\n",
            "iteration 1763 :train_loss:1.1632819175720215 val_loss1.117039680480957\n",
            "iteration 1764 :train_loss:1.1632719039916992 val_loss1.1170289516448975\n",
            "iteration 1765 :train_loss:1.163261890411377 val_loss1.117018222808838\n",
            "iteration 1766 :train_loss:1.1632519960403442 val_loss1.1170076131820679\n",
            "iteration 1767 :train_loss:1.1632418632507324 val_loss1.1169968843460083\n",
            "iteration 1768 :train_loss:1.1632320880889893 val_loss1.1169861555099487\n",
            "iteration 1769 :train_loss:1.163222074508667 val_loss1.1169755458831787\n",
            "iteration 1770 :train_loss:1.1632121801376343 val_loss1.1169646978378296\n",
            "iteration 1771 :train_loss:1.163202166557312 val_loss1.1169540882110596\n",
            "iteration 1772 :train_loss:1.1631921529769897 val_loss1.116943359375\n",
            "iteration 1773 :train_loss:1.163182258605957 val_loss1.1169328689575195\n",
            "iteration 1774 :train_loss:1.1631723642349243 val_loss1.1169222593307495\n",
            "iteration 1775 :train_loss:1.1631624698638916 val_loss1.1169114112854004\n",
            "iteration 1776 :train_loss:1.1631524562835693 val_loss1.1169006824493408\n",
            "iteration 1777 :train_loss:1.163142442703247 val_loss1.1168900728225708\n",
            "iteration 1778 :train_loss:1.163132667541504 val_loss1.1168792247772217\n",
            "iteration 1779 :train_loss:1.1631226539611816 val_loss1.1168687343597412\n",
            "iteration 1780 :train_loss:1.163112759590149 val_loss1.1168580055236816\n",
            "iteration 1781 :train_loss:1.163102626800537 val_loss1.116847276687622\n",
            "iteration 1782 :train_loss:1.1630927324295044 val_loss1.1168365478515625\n",
            "iteration 1783 :train_loss:1.1630827188491821 val_loss1.1168259382247925\n",
            "iteration 1784 :train_loss:1.1630730628967285 val_loss1.116815209388733\n",
            "iteration 1785 :train_loss:1.1630628108978271 val_loss1.116804599761963\n",
            "iteration 1786 :train_loss:1.163053035736084 val_loss1.1167939901351929\n",
            "iteration 1787 :train_loss:1.1630430221557617 val_loss1.1167831420898438\n",
            "iteration 1788 :train_loss:1.163033127784729 val_loss1.1167725324630737\n",
            "iteration 1789 :train_loss:1.1630229949951172 val_loss1.1167618036270142\n",
            "iteration 1790 :train_loss:1.1630131006240845 val_loss1.1167511940002441\n",
            "iteration 1791 :train_loss:1.1630033254623413 val_loss1.1167404651641846\n",
            "iteration 1792 :train_loss:1.162993311882019 val_loss1.116729736328125\n",
            "iteration 1793 :train_loss:1.1629834175109863 val_loss1.1167190074920654\n",
            "iteration 1794 :train_loss:1.162973403930664 val_loss1.1167083978652954\n",
            "iteration 1795 :train_loss:1.1629633903503418 val_loss1.1166976690292358\n",
            "iteration 1796 :train_loss:1.162953495979309 val_loss1.1166870594024658\n",
            "iteration 1797 :train_loss:1.1629436016082764 val_loss1.1166762113571167\n",
            "iteration 1798 :train_loss:1.162933588027954 val_loss1.1166656017303467\n",
            "iteration 1799 :train_loss:1.1629234552383423 val_loss1.116654872894287\n",
            "iteration 1800 :train_loss:1.1629136800765991 val_loss1.116644263267517\n",
            "iteration 1801 :train_loss:1.1629036664962769 val_loss1.1166335344314575\n",
            "iteration 1802 :train_loss:1.1628937721252441 val_loss1.1166229248046875\n",
            "iteration 1803 :train_loss:1.1628837585449219 val_loss1.116612195968628\n",
            "iteration 1804 :train_loss:1.1628738641738892 val_loss1.1166014671325684\n",
            "iteration 1805 :train_loss:1.1628637313842773 val_loss1.1165907382965088\n",
            "iteration 1806 :train_loss:1.1628539562225342 val_loss1.1165800094604492\n",
            "iteration 1807 :train_loss:1.162843942642212 val_loss1.1165693998336792\n",
            "iteration 1808 :train_loss:1.1628339290618896 val_loss1.1165587902069092\n",
            "iteration 1809 :train_loss:1.1628241539001465 val_loss1.1165481805801392\n",
            "iteration 1810 :train_loss:1.1628141403198242 val_loss1.11653733253479\n",
            "iteration 1811 :train_loss:1.162804126739502 val_loss1.11652672290802\n",
            "iteration 1812 :train_loss:1.1627942323684692 val_loss1.1165159940719604\n",
            "iteration 1813 :train_loss:1.1627843379974365 val_loss1.1165053844451904\n",
            "iteration 1814 :train_loss:1.1627743244171143 val_loss1.1164945363998413\n",
            "iteration 1815 :train_loss:1.1627644300460815 val_loss1.1164839267730713\n",
            "iteration 1816 :train_loss:1.1627542972564697 val_loss1.1164731979370117\n",
            "iteration 1817 :train_loss:1.162744402885437 val_loss1.1164627075195312\n",
            "iteration 1818 :train_loss:1.1627346277236938 val_loss1.1164518594741821\n",
            "iteration 1819 :train_loss:1.162724494934082 val_loss1.1164411306381226\n",
            "iteration 1820 :train_loss:1.1627146005630493 val_loss1.1164305210113525\n",
            "iteration 1821 :train_loss:1.1627048254013062 val_loss1.1164199113845825\n",
            "iteration 1822 :train_loss:1.1626946926116943 val_loss1.116409182548523\n",
            "iteration 1823 :train_loss:1.1626847982406616 val_loss1.1163983345031738\n",
            "iteration 1824 :train_loss:1.1626747846603394 val_loss1.1163877248764038\n",
            "iteration 1825 :train_loss:1.1626648902893066 val_loss1.1163769960403442\n",
            "iteration 1826 :train_loss:1.162654995918274 val_loss1.1163663864135742\n",
            "iteration 1827 :train_loss:1.1626449823379517 val_loss1.1163558959960938\n",
            "iteration 1828 :train_loss:1.162635087966919 val_loss1.1163450479507446\n",
            "iteration 1829 :train_loss:1.1626251935958862 val_loss1.1163344383239746\n",
            "iteration 1830 :train_loss:1.1626150608062744 val_loss1.116323709487915\n",
            "iteration 1831 :train_loss:1.1626051664352417 val_loss1.116312861442566\n",
            "iteration 1832 :train_loss:1.162595272064209 val_loss1.116302251815796\n",
            "iteration 1833 :train_loss:1.1625851392745972 val_loss1.1162915229797363\n",
            "iteration 1834 :train_loss:1.1625752449035645 val_loss1.1162807941436768\n",
            "iteration 1835 :train_loss:1.1625652313232422 val_loss1.1162701845169067\n",
            "iteration 1836 :train_loss:1.16255521774292 val_loss1.1162594556808472\n",
            "iteration 1837 :train_loss:1.1625454425811768 val_loss1.1162487268447876\n",
            "iteration 1838 :train_loss:1.1625354290008545 val_loss1.1162381172180176\n",
            "iteration 1839 :train_loss:1.1625254154205322 val_loss1.1162275075912476\n",
            "iteration 1840 :train_loss:1.16251540184021 val_loss1.116216778755188\n",
            "iteration 1841 :train_loss:1.1625056266784668 val_loss1.1162059307098389\n",
            "iteration 1842 :train_loss:1.162495493888855 val_loss1.1161953210830688\n",
            "iteration 1843 :train_loss:1.1624855995178223 val_loss1.1161845922470093\n",
            "iteration 1844 :train_loss:1.1624757051467896 val_loss1.1161738634109497\n",
            "iteration 1845 :train_loss:1.1624658107757568 val_loss1.1161632537841797\n",
            "iteration 1846 :train_loss:1.1624557971954346 val_loss1.1161526441574097\n",
            "iteration 1847 :train_loss:1.1624457836151123 val_loss1.11614191532135\n",
            "iteration 1848 :train_loss:1.16243577003479 val_loss1.11613130569458\n",
            "iteration 1849 :train_loss:1.1624258756637573 val_loss1.1161205768585205\n",
            "iteration 1850 :train_loss:1.162415862083435 val_loss1.1161097288131714\n",
            "iteration 1851 :train_loss:1.1624062061309814 val_loss1.1160991191864014\n",
            "iteration 1852 :train_loss:1.1623960733413696 val_loss1.1160882711410522\n",
            "iteration 1853 :train_loss:1.1623860597610474 val_loss1.1160777807235718\n",
            "iteration 1854 :train_loss:1.1623761653900146 val_loss1.1160670518875122\n",
            "iteration 1855 :train_loss:1.1623660326004028 val_loss1.1160563230514526\n",
            "iteration 1856 :train_loss:1.1623561382293701 val_loss1.116045594215393\n",
            "iteration 1857 :train_loss:1.1623462438583374 val_loss1.1160348653793335\n",
            "iteration 1858 :train_loss:1.1623362302780151 val_loss1.1160242557525635\n",
            "iteration 1859 :train_loss:1.1623263359069824 val_loss1.116013526916504\n",
            "iteration 1860 :train_loss:1.1623163223266602 val_loss1.1160026788711548\n",
            "iteration 1861 :train_loss:1.1623064279556274 val_loss1.1159921884536743\n",
            "iteration 1862 :train_loss:1.1622964143753052 val_loss1.1159813404083252\n",
            "iteration 1863 :train_loss:1.1622865200042725 val_loss1.1159707307815552\n",
            "iteration 1864 :train_loss:1.1622765064239502 val_loss1.115959882736206\n",
            "iteration 1865 :train_loss:1.1622666120529175 val_loss1.115949273109436\n",
            "iteration 1866 :train_loss:1.1622564792633057 val_loss1.115938663482666\n",
            "iteration 1867 :train_loss:1.1622467041015625 val_loss1.115928053855896\n",
            "iteration 1868 :train_loss:1.1622365713119507 val_loss1.1159172058105469\n",
            "iteration 1869 :train_loss:1.162226676940918 val_loss1.1159065961837769\n",
            "iteration 1870 :train_loss:1.1622166633605957 val_loss1.1158958673477173\n",
            "iteration 1871 :train_loss:1.162206768989563 val_loss1.1158852577209473\n",
            "iteration 1872 :train_loss:1.1621966361999512 val_loss1.1158746480941772\n",
            "iteration 1873 :train_loss:1.162186861038208 val_loss1.1158638000488281\n",
            "iteration 1874 :train_loss:1.1621768474578857 val_loss1.115853190422058\n",
            "iteration 1875 :train_loss:1.1621670722961426 val_loss1.1158424615859985\n",
            "iteration 1876 :train_loss:1.1621569395065308 val_loss1.1158318519592285\n",
            "iteration 1877 :train_loss:1.162146806716919 val_loss1.1158210039138794\n",
            "iteration 1878 :train_loss:1.1621371507644653 val_loss1.1158102750778198\n",
            "iteration 1879 :train_loss:1.1621270179748535 val_loss1.1157996654510498\n",
            "iteration 1880 :train_loss:1.1621170043945312 val_loss1.1157889366149902\n",
            "iteration 1881 :train_loss:1.1621071100234985 val_loss1.1157782077789307\n",
            "iteration 1882 :train_loss:1.1620972156524658 val_loss1.115767478942871\n",
            "iteration 1883 :train_loss:1.1620872020721436 val_loss1.115756869316101\n",
            "iteration 1884 :train_loss:1.1620773077011108 val_loss1.1157461404800415\n",
            "iteration 1885 :train_loss:1.1620672941207886 val_loss1.115735411643982\n",
            "iteration 1886 :train_loss:1.1620572805404663 val_loss1.1157246828079224\n",
            "iteration 1887 :train_loss:1.1620473861694336 val_loss1.1157140731811523\n",
            "iteration 1888 :train_loss:1.1620372533798218 val_loss1.1157032251358032\n",
            "iteration 1889 :train_loss:1.162027359008789 val_loss1.1156926155090332\n",
            "iteration 1890 :train_loss:1.1620174646377563 val_loss1.1156820058822632\n",
            "iteration 1891 :train_loss:1.162007451057434 val_loss1.115671157836914\n",
            "iteration 1892 :train_loss:1.1619975566864014 val_loss1.115660548210144\n",
            "iteration 1893 :train_loss:1.1619874238967896 val_loss1.115649700164795\n",
            "iteration 1894 :train_loss:1.1619776487350464 val_loss1.1156392097473145\n",
            "iteration 1895 :train_loss:1.1619676351547241 val_loss1.1156284809112549\n",
            "iteration 1896 :train_loss:1.1619576215744019 val_loss1.1156176328659058\n",
            "iteration 1897 :train_loss:1.1619477272033691 val_loss1.1156070232391357\n",
            "iteration 1898 :train_loss:1.1619375944137573 val_loss1.1155962944030762\n",
            "iteration 1899 :train_loss:1.1619277000427246 val_loss1.1155855655670166\n",
            "iteration 1900 :train_loss:1.161917805671692 val_loss1.115574836730957\n",
            "iteration 1901 :train_loss:1.1619077920913696 val_loss1.115564227104187\n",
            "iteration 1902 :train_loss:1.1618977785110474 val_loss1.1155534982681274\n",
            "iteration 1903 :train_loss:1.1618878841400146 val_loss1.1155428886413574\n",
            "iteration 1904 :train_loss:1.1618778705596924 val_loss1.1155320405960083\n",
            "iteration 1905 :train_loss:1.1618677377700806 val_loss1.1155214309692383\n",
            "iteration 1906 :train_loss:1.1618579626083374 val_loss1.1155107021331787\n",
            "iteration 1907 :train_loss:1.1618479490280151 val_loss1.1155000925064087\n",
            "iteration 1908 :train_loss:1.1618379354476929 val_loss1.1154893636703491\n",
            "iteration 1909 :train_loss:1.1618280410766602 val_loss1.1154783964157104\n",
            "iteration 1910 :train_loss:1.161818027496338 val_loss1.11546790599823\n",
            "iteration 1911 :train_loss:1.1618080139160156 val_loss1.11545729637146\n",
            "iteration 1912 :train_loss:1.1617980003356934 val_loss1.1154464483261108\n",
            "iteration 1913 :train_loss:1.1617881059646606 val_loss1.1154357194900513\n",
            "iteration 1914 :train_loss:1.1617780923843384 val_loss1.1154249906539917\n",
            "iteration 1915 :train_loss:1.1617683172225952 val_loss1.1154142618179321\n",
            "iteration 1916 :train_loss:1.1617580652236938 val_loss1.115403652191162\n",
            "iteration 1917 :train_loss:1.1617481708526611 val_loss1.1153929233551025\n",
            "iteration 1918 :train_loss:1.1617380380630493 val_loss1.1153823137283325\n",
            "iteration 1919 :train_loss:1.1617282629013062 val_loss1.115371584892273\n",
            "iteration 1920 :train_loss:1.1617182493209839 val_loss1.1153607368469238\n",
            "iteration 1921 :train_loss:1.1617082357406616 val_loss1.1153500080108643\n",
            "iteration 1922 :train_loss:1.161698341369629 val_loss1.1153393983840942\n",
            "iteration 1923 :train_loss:1.1616884469985962 val_loss1.1153286695480347\n",
            "iteration 1924 :train_loss:1.1616781949996948 val_loss1.1153180599212646\n",
            "iteration 1925 :train_loss:1.1616684198379517 val_loss1.115307331085205\n",
            "iteration 1926 :train_loss:1.1616584062576294 val_loss1.1152966022491455\n",
            "iteration 1927 :train_loss:1.1616483926773071 val_loss1.115285873413086\n",
            "iteration 1928 :train_loss:1.1616383790969849 val_loss1.1152750253677368\n",
            "iteration 1929 :train_loss:1.1616286039352417 val_loss1.1152645349502563\n",
            "iteration 1930 :train_loss:1.1616184711456299 val_loss1.1152538061141968\n",
            "iteration 1931 :train_loss:1.161608338356018 val_loss1.1152430772781372\n",
            "iteration 1932 :train_loss:1.1615984439849854 val_loss1.115232229232788\n",
            "iteration 1933 :train_loss:1.1615885496139526 val_loss1.115221619606018\n",
            "iteration 1934 :train_loss:1.1615785360336304 val_loss1.115211009979248\n",
            "iteration 1935 :train_loss:1.1615686416625977 val_loss1.115200161933899\n",
            "iteration 1936 :train_loss:1.1615585088729858 val_loss1.1151894330978394\n",
            "iteration 1937 :train_loss:1.1615486145019531 val_loss1.1151788234710693\n",
            "iteration 1938 :train_loss:1.1615386009216309 val_loss1.1151680946350098\n",
            "iteration 1939 :train_loss:1.1615285873413086 val_loss1.1151573657989502\n",
            "iteration 1940 :train_loss:1.1615186929702759 val_loss1.1151466369628906\n",
            "iteration 1941 :train_loss:1.1615086793899536 val_loss1.115135908126831\n",
            "iteration 1942 :train_loss:1.161498785018921 val_loss1.1151251792907715\n",
            "iteration 1943 :train_loss:1.161488652229309 val_loss1.115114450454712\n",
            "iteration 1944 :train_loss:1.1614787578582764 val_loss1.1151037216186523\n",
            "iteration 1945 :train_loss:1.1614688634872437 val_loss1.1150929927825928\n",
            "iteration 1946 :train_loss:1.1614586114883423 val_loss1.1150823831558228\n",
            "iteration 1947 :train_loss:1.1614489555358887 val_loss1.1150715351104736\n",
            "iteration 1948 :train_loss:1.1614388227462769 val_loss1.115060806274414\n",
            "iteration 1949 :train_loss:1.1614289283752441 val_loss1.1150500774383545\n",
            "iteration 1950 :train_loss:1.1614189147949219 val_loss1.1150394678115845\n",
            "iteration 1951 :train_loss:1.16140878200531 val_loss1.115028738975525\n",
            "iteration 1952 :train_loss:1.1613988876342773 val_loss1.1150180101394653\n",
            "iteration 1953 :train_loss:1.161388874053955 val_loss1.1150072813034058\n",
            "iteration 1954 :train_loss:1.1613788604736328 val_loss1.1149966716766357\n",
            "iteration 1955 :train_loss:1.1613689661026 val_loss1.1149858236312866\n",
            "iteration 1956 :train_loss:1.1613589525222778 val_loss1.1149752140045166\n",
            "iteration 1957 :train_loss:1.161348819732666 val_loss1.114964485168457\n",
            "iteration 1958 :train_loss:1.1613390445709229 val_loss1.1149537563323975\n",
            "iteration 1959 :train_loss:1.1613290309906006 val_loss1.1149431467056274\n",
            "iteration 1960 :train_loss:1.1613191366195679 val_loss1.1149324178695679\n",
            "iteration 1961 :train_loss:1.161309003829956 val_loss1.1149215698242188\n",
            "iteration 1962 :train_loss:1.1612989902496338 val_loss1.1149108409881592\n",
            "iteration 1963 :train_loss:1.161289095878601 val_loss1.1149002313613892\n",
            "iteration 1964 :train_loss:1.1612789630889893 val_loss1.11488938331604\n",
            "iteration 1965 :train_loss:1.1612690687179565 val_loss1.114878535270691\n",
            "iteration 1966 :train_loss:1.1612590551376343 val_loss1.1148680448532104\n",
            "iteration 1967 :train_loss:1.1612491607666016 val_loss1.1148573160171509\n",
            "iteration 1968 :train_loss:1.1612391471862793 val_loss1.1148465871810913\n",
            "iteration 1969 :train_loss:1.1612290143966675 val_loss1.1148358583450317\n",
            "iteration 1970 :train_loss:1.1612190008163452 val_loss1.1148251295089722\n",
            "iteration 1971 :train_loss:1.1612088680267334 val_loss1.1148144006729126\n",
            "iteration 1972 :train_loss:1.1611990928649902 val_loss1.1148035526275635\n",
            "iteration 1973 :train_loss:1.1611889600753784 val_loss1.1147929430007935\n",
            "iteration 1974 :train_loss:1.1611789464950562 val_loss1.1147820949554443\n",
            "iteration 1975 :train_loss:1.1611690521240234 val_loss1.1147713661193848\n",
            "iteration 1976 :train_loss:1.1611590385437012 val_loss1.1147607564926147\n",
            "iteration 1977 :train_loss:1.1611491441726685 val_loss1.1147500276565552\n",
            "iteration 1978 :train_loss:1.1611391305923462 val_loss1.1147392988204956\n",
            "iteration 1979 :train_loss:1.161129117012024 val_loss1.1147286891937256\n",
            "iteration 1980 :train_loss:1.1611192226409912 val_loss1.1147178411483765\n",
            "iteration 1981 :train_loss:1.1611089706420898 val_loss1.1147072315216064\n",
            "iteration 1982 :train_loss:1.1610990762710571 val_loss1.1146963834762573\n",
            "iteration 1983 :train_loss:1.1610890626907349 val_loss1.1146856546401978\n",
            "iteration 1984 :train_loss:1.1610790491104126 val_loss1.1146750450134277\n",
            "iteration 1985 :train_loss:1.1610691547393799 val_loss1.1146643161773682\n",
            "iteration 1986 :train_loss:1.1610591411590576 val_loss1.114653468132019\n",
            "iteration 1987 :train_loss:1.161049246788025 val_loss1.1146427392959595\n",
            "iteration 1988 :train_loss:1.161039113998413 val_loss1.1146320104599\n",
            "iteration 1989 :train_loss:1.1610291004180908 val_loss1.1146211624145508\n",
            "iteration 1990 :train_loss:1.1610190868377686 val_loss1.1146105527877808\n",
            "iteration 1991 :train_loss:1.1610090732574463 val_loss1.1145997047424316\n",
            "iteration 1992 :train_loss:1.160999059677124 val_loss1.1145890951156616\n",
            "iteration 1993 :train_loss:1.1609890460968018 val_loss1.1145782470703125\n",
            "iteration 1994 :train_loss:1.160979151725769 val_loss1.1145676374435425\n",
            "iteration 1995 :train_loss:1.1609691381454468 val_loss1.114556908607483\n",
            "iteration 1996 :train_loss:1.160959005355835 val_loss1.1145461797714233\n",
            "iteration 1997 :train_loss:1.1609491109848022 val_loss1.1145354509353638\n",
            "iteration 1998 :train_loss:1.16093909740448 val_loss1.1145248413085938\n",
            "iteration 1999 :train_loss:1.1609292030334473 val_loss1.1145139932632446\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(0,iterations),history[\"train_loss\"],'b')\n",
        "plt.plot(range(0,iterations),history[\"val_loss\"],'r')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('iterations')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "SbmgxG54dhnR",
        "outputId": "916fda9e-17a2-4a4b-cf89-e6caa1872e0c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d3278321dea9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iterations'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iterations=1000\n",
        "parameters, history=create_nn_model(train_X,train_Y,50, val_X, val_Y, iterations, 1e-2)\n",
        "plt.plot(range(0,iterations),history[\"train_loss\"],'b')\n",
        "plt.plot(range(0,iterations),history[\"val_loss\"],'r')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('iterations')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNGDrbZwhC8v",
        "outputId": "4be6fa94-130e-440f-8936-815b665016dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 0 :train_loss:5.6630377769470215 val_loss5.55766487121582\n",
            "iteration 1 :train_loss:5.487386226654053 val_loss5.3834547996521\n",
            "iteration 2 :train_loss:5.318718910217285 val_loss5.216200828552246\n",
            "iteration 3 :train_loss:5.156501770019531 val_loss5.055371284484863\n",
            "iteration 4 :train_loss:5.000255584716797 val_loss4.90048885345459\n",
            "iteration 5 :train_loss:4.849534511566162 val_loss4.751113414764404\n",
            "iteration 6 :train_loss:4.703921794891357 val_loss4.6068267822265625\n",
            "iteration 7 :train_loss:4.563014507293701 val_loss4.467231273651123\n",
            "iteration 8 :train_loss:4.4264235496521 val_loss4.331939220428467\n",
            "iteration 9 :train_loss:4.293766498565674 val_loss4.2005720138549805\n",
            "iteration 10 :train_loss:4.164668083190918 val_loss4.072756767272949\n",
            "iteration 11 :train_loss:4.038759231567383 val_loss3.9481279850006104\n",
            "iteration 12 :train_loss:3.915675640106201 val_loss3.8263251781463623\n",
            "iteration 13 :train_loss:3.7950644493103027 val_loss3.7069990634918213\n",
            "iteration 14 :train_loss:3.6765849590301514 val_loss3.589812755584717\n",
            "iteration 15 :train_loss:3.5599160194396973 val_loss3.4744479656219482\n",
            "iteration 16 :train_loss:3.444761276245117 val_loss3.3606152534484863\n",
            "iteration 17 :train_loss:3.330859422683716 val_loss3.2480556964874268\n",
            "iteration 18 :train_loss:3.2179946899414062 val_loss3.1365575790405273\n",
            "iteration 19 :train_loss:3.1060070991516113 val_loss3.025965929031372\n",
            "iteration 20 :train_loss:2.994807481765747 val_loss2.916194200515747\n",
            "iteration 21 :train_loss:2.8843870162963867 val_loss2.807236909866333\n",
            "iteration 22 :train_loss:2.774832248687744 val_loss2.6991822719573975\n",
            "iteration 23 :train_loss:2.6663315296173096 val_loss2.5922210216522217\n",
            "iteration 24 :train_loss:2.559183120727539 val_loss2.4866509437561035\n",
            "iteration 25 :train_loss:2.45379376411438 val_loss2.3828768730163574\n",
            "iteration 26 :train_loss:2.350672960281372 val_loss2.2814059257507324\n",
            "iteration 27 :train_loss:2.2504165172576904 val_loss2.182827949523926\n",
            "iteration 28 :train_loss:2.1536834239959717 val_loss2.0877957344055176\n",
            "iteration 29 :train_loss:2.0611627101898193 val_loss1.996990442276001\n",
            "iteration 30 :train_loss:1.9735361337661743 val_loss1.9110827445983887\n",
            "iteration 31 :train_loss:1.8914344310760498 val_loss1.830692172050476\n",
            "iteration 32 :train_loss:1.8153969049453735 val_loss1.7563459873199463\n",
            "iteration 33 :train_loss:1.745835304260254 val_loss1.688442349433899\n",
            "iteration 34 :train_loss:1.6830048561096191 val_loss1.6272251605987549\n",
            "iteration 35 :train_loss:1.6269912719726562 val_loss1.5727671384811401\n",
            "iteration 36 :train_loss:1.5777084827423096 val_loss1.5249710083007812\n",
            "iteration 37 :train_loss:1.5349106788635254 val_loss1.4835830926895142\n",
            "iteration 38 :train_loss:1.4982174634933472 val_loss1.4482146501541138\n",
            "iteration 39 :train_loss:1.4671435356140137 val_loss1.4183745384216309\n",
            "iteration 40 :train_loss:1.4411325454711914 val_loss1.3935054540634155\n",
            "iteration 41 :train_loss:1.4195955991744995 val_loss1.3730155229568481\n",
            "iteration 42 :train_loss:1.4019384384155273 val_loss1.35631263256073\n",
            "iteration 43 :train_loss:1.3875890970230103 val_loss1.3428258895874023\n",
            "iteration 44 :train_loss:1.376015067100525 val_loss1.3320271968841553\n",
            "iteration 45 :train_loss:1.3667364120483398 val_loss1.3234416246414185\n",
            "iteration 46 :train_loss:1.359331488609314 val_loss1.3166524171829224\n",
            "iteration 47 :train_loss:1.3534375429153442 val_loss1.3113031387329102\n",
            "iteration 48 :train_loss:1.3487498760223389 val_loss1.307094693183899\n",
            "iteration 49 :train_loss:1.3450161218643188 val_loss1.303780436515808\n",
            "iteration 50 :train_loss:1.3420295715332031 val_loss1.3011603355407715\n",
            "iteration 51 :train_loss:1.3396246433258057 val_loss1.2990742921829224\n",
            "iteration 52 :train_loss:1.3376694917678833 val_loss1.29739511013031\n",
            "iteration 53 :train_loss:1.3360600471496582 val_loss1.2960237264633179\n",
            "iteration 54 :train_loss:1.3347148895263672 val_loss1.294883370399475\n",
            "iteration 55 :train_loss:1.3335710763931274 val_loss1.2939153909683228\n",
            "iteration 56 :train_loss:1.3325804471969604 val_loss1.2930744886398315\n",
            "iteration 57 :train_loss:1.331705093383789 val_loss1.2923266887664795\n",
            "iteration 58 :train_loss:1.3309175968170166 val_loss1.2916470766067505\n",
            "iteration 59 :train_loss:1.3301957845687866 val_loss1.2910162210464478\n",
            "iteration 60 :train_loss:1.3295234441757202 val_loss1.2904201745986938\n",
            "iteration 61 :train_loss:1.3288887739181519 val_loss1.2898491621017456\n",
            "iteration 62 :train_loss:1.328282356262207 val_loss1.289294958114624\n",
            "iteration 63 :train_loss:1.3276971578598022 val_loss1.2887526750564575\n",
            "iteration 64 :train_loss:1.3271280527114868 val_loss1.288218379020691\n",
            "iteration 65 :train_loss:1.3265711069107056 val_loss1.287688970565796\n",
            "iteration 66 :train_loss:1.3260236978530884 val_loss1.2871628999710083\n",
            "iteration 67 :train_loss:1.3254834413528442 val_loss1.286638855934143\n",
            "iteration 68 :train_loss:1.324948787689209 val_loss1.2861155271530151\n",
            "iteration 69 :train_loss:1.3244186639785767 val_loss1.2855931520462036\n",
            "iteration 70 :train_loss:1.3238918781280518 val_loss1.2850706577301025\n",
            "iteration 71 :train_loss:1.3233681917190552 val_loss1.284548282623291\n",
            "iteration 72 :train_loss:1.3228466510772705 val_loss1.28402578830719\n",
            "iteration 73 :train_loss:1.3223271369934082 val_loss1.2835031747817993\n",
            "iteration 74 :train_loss:1.3218095302581787 val_loss1.2829805612564087\n",
            "iteration 75 :train_loss:1.3212932348251343 val_loss1.2824574708938599\n",
            "iteration 76 :train_loss:1.320778250694275 val_loss1.2819347381591797\n",
            "iteration 77 :train_loss:1.3202648162841797 val_loss1.2814120054244995\n",
            "iteration 78 :train_loss:1.3197520971298218 val_loss1.2808892726898193\n",
            "iteration 79 :train_loss:1.3192408084869385 val_loss1.2803670167922974\n",
            "iteration 80 :train_loss:1.3187304735183716 val_loss1.279844880104065\n",
            "iteration 81 :train_loss:1.3182209730148315 val_loss1.2793229818344116\n",
            "iteration 82 :train_loss:1.3177125453948975 val_loss1.278801679611206\n",
            "iteration 83 :train_loss:1.3172047138214111 val_loss1.2782808542251587\n",
            "iteration 84 :train_loss:1.3166981935501099 val_loss1.27776038646698\n",
            "iteration 85 :train_loss:1.3161925077438354 val_loss1.2772403955459595\n",
            "iteration 86 :train_loss:1.3156874179840088 val_loss1.2767210006713867\n",
            "iteration 87 :train_loss:1.315183401107788 val_loss1.2762020826339722\n",
            "iteration 88 :train_loss:1.3146799802780151 val_loss1.275684118270874\n",
            "iteration 89 :train_loss:1.3141775131225586 val_loss1.275166630744934\n",
            "iteration 90 :train_loss:1.313675880432129 val_loss1.2746496200561523\n",
            "iteration 91 :train_loss:1.3131749629974365 val_loss1.274133324623108\n",
            "iteration 92 :train_loss:1.3126747608184814 val_loss1.2736176252365112\n",
            "iteration 93 :train_loss:1.3121753931045532 val_loss1.2731026411056519\n",
            "iteration 94 :train_loss:1.3116768598556519 val_loss1.2725883722305298\n",
            "iteration 95 :train_loss:1.3111788034439087 val_loss1.272074580192566\n",
            "iteration 96 :train_loss:1.310681700706482 val_loss1.271561622619629\n",
            "iteration 97 :train_loss:1.3101853132247925 val_loss1.2710493803024292\n",
            "iteration 98 :train_loss:1.3096897602081299 val_loss1.2705377340316772\n",
            "iteration 99 :train_loss:1.309194803237915 val_loss1.270026683807373\n",
            "iteration 100 :train_loss:1.308700442314148 val_loss1.2695164680480957\n",
            "iteration 101 :train_loss:1.3082070350646973 val_loss1.2690067291259766\n",
            "iteration 102 :train_loss:1.3077139854431152 val_loss1.2684978246688843\n",
            "iteration 103 :train_loss:1.3072220087051392 val_loss1.2679893970489502\n",
            "iteration 104 :train_loss:1.3067303895950317 val_loss1.2674815654754639\n",
            "iteration 105 :train_loss:1.306239366531372 val_loss1.266974687576294\n",
            "iteration 106 :train_loss:1.3057494163513184 val_loss1.2664682865142822\n",
            "iteration 107 :train_loss:1.3052597045898438 val_loss1.2659624814987183\n",
            "iteration 108 :train_loss:1.3047709465026855 val_loss1.265457272529602\n",
            "iteration 109 :train_loss:1.304282546043396 val_loss1.2649527788162231\n",
            "iteration 110 :train_loss:1.3037948608398438 val_loss1.2644487619400024\n",
            "iteration 111 :train_loss:1.303308129310608 val_loss1.2639455795288086\n",
            "iteration 112 :train_loss:1.302821397781372 val_loss1.2634426355361938\n",
            "iteration 113 :train_loss:1.3023358583450317 val_loss1.2629406452178955\n",
            "iteration 114 :train_loss:1.3018505573272705 val_loss1.2624391317367554\n",
            "iteration 115 :train_loss:1.3013660907745361 val_loss1.2619383335113525\n",
            "iteration 116 :train_loss:1.300882339477539 val_loss1.2614378929138184\n",
            "iteration 117 :train_loss:1.3003989458084106 val_loss1.2609381675720215\n",
            "iteration 118 :train_loss:1.29991614818573 val_loss1.2604389190673828\n",
            "iteration 119 :train_loss:1.2994341850280762 val_loss1.259940505027771\n",
            "iteration 120 :train_loss:1.298952341079712 val_loss1.2594424486160278\n",
            "iteration 121 :train_loss:1.298471450805664 val_loss1.2589449882507324\n",
            "iteration 122 :train_loss:1.2979909181594849 val_loss1.2584480047225952\n",
            "iteration 123 :train_loss:1.2975112199783325 val_loss1.2579518556594849\n",
            "iteration 124 :train_loss:1.2970318794250488 val_loss1.257455825805664\n",
            "iteration 125 :train_loss:1.2965528964996338 val_loss1.2569606304168701\n",
            "iteration 126 :train_loss:1.2960747480392456 val_loss1.2564657926559448\n",
            "iteration 127 :train_loss:1.2955971956253052 val_loss1.2559716701507568\n",
            "iteration 128 :train_loss:1.2951200008392334 val_loss1.2554781436920166\n",
            "iteration 129 :train_loss:1.294643521308899 val_loss1.2549850940704346\n",
            "iteration 130 :train_loss:1.2941675186157227 val_loss1.254492163658142\n",
            "iteration 131 :train_loss:1.2936919927597046 val_loss1.2540003061294556\n",
            "iteration 132 :train_loss:1.2932170629501343 val_loss1.2535086870193481\n",
            "iteration 133 :train_loss:1.2927427291870117 val_loss1.2530179023742676\n",
            "iteration 134 :train_loss:1.2922686338424683 val_loss1.2525274753570557\n",
            "iteration 135 :train_loss:1.2917954921722412 val_loss1.2520372867584229\n",
            "iteration 136 :train_loss:1.2913228273391724 val_loss1.2515480518341064\n",
            "iteration 137 :train_loss:1.2908505201339722 val_loss1.2510590553283691\n",
            "iteration 138 :train_loss:1.2903786897659302 val_loss1.2505707740783691\n",
            "iteration 139 :train_loss:1.289907455444336 val_loss1.2500827312469482\n",
            "iteration 140 :train_loss:1.2894366979599 val_loss1.2495954036712646\n",
            "iteration 141 :train_loss:1.2889665365219116 val_loss1.2491084337234497\n",
            "iteration 142 :train_loss:1.2884966135025024 val_loss1.2486220598220825\n",
            "iteration 143 :train_loss:1.2880276441574097 val_loss1.2481361627578735\n",
            "iteration 144 :train_loss:1.287559151649475 val_loss1.2476507425308228\n",
            "iteration 145 :train_loss:1.2870908975601196 val_loss1.2471659183502197\n",
            "iteration 146 :train_loss:1.286623239517212 val_loss1.2466816902160645\n",
            "iteration 147 :train_loss:1.2861560583114624 val_loss1.2461977005004883\n",
            "iteration 148 :train_loss:1.2856894731521606 val_loss1.2457143068313599\n",
            "iteration 149 :train_loss:1.2852234840393066 val_loss1.2452313899993896\n",
            "iteration 150 :train_loss:1.2847577333450317 val_loss1.2447490692138672\n",
            "iteration 151 :train_loss:1.2842926979064941 val_loss1.2442671060562134\n",
            "iteration 152 :train_loss:1.2838281393051147 val_loss1.2437858581542969\n",
            "iteration 153 :train_loss:1.283363938331604 val_loss1.243304967880249\n",
            "iteration 154 :train_loss:1.2829006910324097 val_loss1.2428245544433594\n",
            "iteration 155 :train_loss:1.2824374437332153 val_loss1.242344617843628\n",
            "iteration 156 :train_loss:1.2819749116897583 val_loss1.2418652772903442\n",
            "iteration 157 :train_loss:1.2815128564834595 val_loss1.2413864135742188\n",
            "iteration 158 :train_loss:1.2810513973236084 val_loss1.240907907485962\n",
            "iteration 159 :train_loss:1.280590534210205 val_loss1.2404299974441528\n",
            "iteration 160 :train_loss:1.2801297903060913 val_loss1.239952564239502\n",
            "iteration 161 :train_loss:1.2796698808670044 val_loss1.2394758462905884\n",
            "iteration 162 :train_loss:1.2792103290557861 val_loss1.238999366760254\n",
            "iteration 163 :train_loss:1.2787514925003052 val_loss1.2385234832763672\n",
            "iteration 164 :train_loss:1.2782930135726929 val_loss1.2380481958389282\n",
            "iteration 165 :train_loss:1.2778348922729492 val_loss1.237573266029358\n",
            "iteration 166 :train_loss:1.2773776054382324 val_loss1.2370986938476562\n",
            "iteration 167 :train_loss:1.2769206762313843 val_loss1.236625075340271\n",
            "iteration 168 :train_loss:1.2764642238616943 val_loss1.2361515760421753\n",
            "iteration 169 :train_loss:1.2760084867477417 val_loss1.2356786727905273\n",
            "iteration 170 :train_loss:1.2755532264709473 val_loss1.2352064847946167\n",
            "iteration 171 :train_loss:1.275098204612732 val_loss1.2347347736358643\n",
            "iteration 172 :train_loss:1.274644136428833 val_loss1.23426353931427\n",
            "iteration 173 :train_loss:1.2741903066635132 val_loss1.233792781829834\n",
            "iteration 174 :train_loss:1.2737370729446411 val_loss1.2333226203918457\n",
            "iteration 175 :train_loss:1.2732844352722168 val_loss1.2328526973724365\n",
            "iteration 176 :train_loss:1.2728322744369507 val_loss1.2323837280273438\n",
            "iteration 177 :train_loss:1.2723808288574219 val_loss1.2319151163101196\n",
            "iteration 178 :train_loss:1.2719297409057617 val_loss1.2314468622207642\n",
            "iteration 179 :train_loss:1.2714792490005493 val_loss1.2309794425964355\n",
            "iteration 180 :train_loss:1.2710293531417847 val_loss1.2305123805999756\n",
            "iteration 181 :train_loss:1.2705800533294678 val_loss1.2300461530685425\n",
            "iteration 182 :train_loss:1.270131230354309 val_loss1.229580044746399\n",
            "iteration 183 :train_loss:1.2696828842163086 val_loss1.2291147708892822\n",
            "iteration 184 :train_loss:1.2692351341247559 val_loss1.2286498546600342\n",
            "iteration 185 :train_loss:1.2687878608703613 val_loss1.228185772895813\n",
            "iteration 186 :train_loss:1.2683414220809937 val_loss1.2277220487594604\n",
            "iteration 187 :train_loss:1.2678954601287842 val_loss1.2272590398788452\n",
            "iteration 188 :train_loss:1.2674503326416016 val_loss1.2267966270446777\n",
            "iteration 189 :train_loss:1.267005443572998 val_loss1.2263346910476685\n",
            "iteration 190 :train_loss:1.2665610313415527 val_loss1.2258734703063965\n",
            "iteration 191 :train_loss:1.2661173343658447 val_loss1.2254124879837036\n",
            "iteration 192 :train_loss:1.265674352645874 val_loss1.2249523401260376\n",
            "iteration 193 :train_loss:1.2652320861816406 val_loss1.2244927883148193\n",
            "iteration 194 :train_loss:1.2647901773452759 val_loss1.2240338325500488\n",
            "iteration 195 :train_loss:1.2643488645553589 val_loss1.2235753536224365\n",
            "iteration 196 :train_loss:1.2639082670211792 val_loss1.2231178283691406\n",
            "iteration 197 :train_loss:1.2634681463241577 val_loss1.2226605415344238\n",
            "iteration 198 :train_loss:1.2630289793014526 val_loss1.2222042083740234\n",
            "iteration 199 :train_loss:1.2625902891159058 val_loss1.2217479944229126\n",
            "iteration 200 :train_loss:1.2621521949768066 val_loss1.2212929725646973\n",
            "iteration 201 :train_loss:1.2617145776748657 val_loss1.2208384275436401\n",
            "iteration 202 :train_loss:1.261277675628662 val_loss1.2203843593597412\n",
            "iteration 203 :train_loss:1.2608416080474854 val_loss1.2199311256408691\n",
            "iteration 204 :train_loss:1.2604061365127563 val_loss1.2194783687591553\n",
            "iteration 205 :train_loss:1.259971261024475 val_loss1.2190264463424683\n",
            "iteration 206 :train_loss:1.2595369815826416 val_loss1.218575119972229\n",
            "iteration 207 :train_loss:1.259103536605835 val_loss1.2181241512298584\n",
            "iteration 208 :train_loss:1.258670687675476 val_loss1.2176743745803833\n",
            "iteration 209 :train_loss:1.258238434791565 val_loss1.2172249555587769\n",
            "iteration 210 :train_loss:1.2578070163726807 val_loss1.2167764902114868\n",
            "iteration 211 :train_loss:1.2573760747909546 val_loss1.2163283824920654\n",
            "iteration 212 :train_loss:1.256946086883545 val_loss1.2158812284469604\n",
            "iteration 213 :train_loss:1.2565168142318726 val_loss1.2154345512390137\n",
            "iteration 214 :train_loss:1.2560880184173584 val_loss1.2149888277053833\n",
            "iteration 215 :train_loss:1.255660057067871 val_loss1.2145439386367798\n",
            "iteration 216 :train_loss:1.255232810974121 val_loss1.2140992879867554\n",
            "iteration 217 :train_loss:1.2548062801361084 val_loss1.2136558294296265\n",
            "iteration 218 :train_loss:1.254380464553833 val_loss1.2132128477096558\n",
            "iteration 219 :train_loss:1.253955364227295 val_loss1.212770700454712\n",
            "iteration 220 :train_loss:1.2535310983657837 val_loss1.2123292684555054\n",
            "iteration 221 :train_loss:1.2531074285507202 val_loss1.2118885517120361\n",
            "iteration 222 :train_loss:1.2526847124099731 val_loss1.2114487886428833\n",
            "iteration 223 :train_loss:1.252263069152832 val_loss1.2110096216201782\n",
            "iteration 224 :train_loss:1.2518415451049805 val_loss1.2105712890625\n",
            "iteration 225 :train_loss:1.2514210939407349 val_loss1.210133671760559\n",
            "iteration 226 :train_loss:1.2510015964508057 val_loss1.2096971273422241\n",
            "iteration 227 :train_loss:1.2505825757980347 val_loss1.2092612981796265\n",
            "iteration 228 :train_loss:1.250164270401001 val_loss1.208825945854187\n",
            "iteration 229 :train_loss:1.2497472763061523 val_loss1.2083916664123535\n",
            "iteration 230 :train_loss:1.249330759048462 val_loss1.2079582214355469\n",
            "iteration 231 :train_loss:1.248915195465088 val_loss1.207525610923767\n",
            "iteration 232 :train_loss:1.2485003471374512 val_loss1.2070938348770142\n",
            "iteration 233 :train_loss:1.2480862140655518 val_loss1.2066627740859985\n",
            "iteration 234 :train_loss:1.2476731538772583 val_loss1.2062326669692993\n",
            "iteration 235 :train_loss:1.2472609281539917 val_loss1.205803394317627\n",
            "iteration 236 :train_loss:1.246849536895752 val_loss1.2053749561309814\n",
            "iteration 237 :train_loss:1.2464388608932495 val_loss1.2049474716186523\n",
            "iteration 238 :train_loss:1.246029257774353 val_loss1.2045209407806396\n",
            "iteration 239 :train_loss:1.2456203699111938 val_loss1.2040950059890747\n",
            "iteration 240 :train_loss:1.2452125549316406 val_loss1.2036700248718262\n",
            "iteration 241 :train_loss:1.2448055744171143 val_loss1.2032462358474731\n",
            "iteration 242 :train_loss:1.2443994283676147 val_loss1.2028230428695679\n",
            "iteration 243 :train_loss:1.243994116783142 val_loss1.202401041984558\n",
            "iteration 244 :train_loss:1.2435898780822754 val_loss1.2019797563552856\n",
            "iteration 245 :train_loss:1.2431864738464355 val_loss1.2015595436096191\n",
            "iteration 246 :train_loss:1.242783784866333 val_loss1.201140284538269\n",
            "iteration 247 :train_loss:1.242382526397705 val_loss1.2007219791412354\n",
            "iteration 248 :train_loss:1.241981863975525 val_loss1.2003045082092285\n",
            "iteration 249 :train_loss:1.2415822744369507 val_loss1.199887990951538\n",
            "iteration 250 :train_loss:1.2411835193634033 val_loss1.1994726657867432\n",
            "iteration 251 :train_loss:1.2407855987548828 val_loss1.1990580558776855\n",
            "iteration 252 :train_loss:1.2403889894485474 val_loss1.1986446380615234\n",
            "iteration 253 :train_loss:1.2399934530258179 val_loss1.1982320547103882\n",
            "iteration 254 :train_loss:1.2395986318588257 val_loss1.1978206634521484\n",
            "iteration 255 :train_loss:1.2392046451568604 val_loss1.197410225868225\n",
            "iteration 256 :train_loss:1.2388120889663696 val_loss1.1970007419586182\n",
            "iteration 257 :train_loss:1.2384202480316162 val_loss1.1965923309326172\n",
            "iteration 258 :train_loss:1.2380295991897583 val_loss1.1961849927902222\n",
            "iteration 259 :train_loss:1.2376397848129272 val_loss1.1957786083221436\n",
            "iteration 260 :train_loss:1.2372512817382812 val_loss1.1953734159469604\n",
            "iteration 261 :train_loss:1.2368634939193726 val_loss1.1949691772460938\n",
            "iteration 262 :train_loss:1.236477017402649 val_loss1.1945661306381226\n",
            "iteration 263 :train_loss:1.2360914945602417 val_loss1.1941639184951782\n",
            "iteration 264 :train_loss:1.23570716381073 val_loss1.1937628984451294\n",
            "iteration 265 :train_loss:1.235323429107666 val_loss1.1933633089065552\n",
            "iteration 266 :train_loss:1.2349413633346558 val_loss1.1929643154144287\n",
            "iteration 267 :train_loss:1.2345600128173828 val_loss1.1925666332244873\n",
            "iteration 268 :train_loss:1.234179973602295 val_loss1.1921700239181519\n",
            "iteration 269 :train_loss:1.2338008880615234 val_loss1.191774606704712\n",
            "iteration 270 :train_loss:1.233422875404358 val_loss1.191380262374878\n",
            "iteration 271 :train_loss:1.233046054840088 val_loss1.1909871101379395\n",
            "iteration 272 :train_loss:1.2326704263687134 val_loss1.1905951499938965\n",
            "iteration 273 :train_loss:1.2322957515716553 val_loss1.1902042627334595\n",
            "iteration 274 :train_loss:1.2319223880767822 val_loss1.189814567565918\n",
            "iteration 275 :train_loss:1.2315500974655151 val_loss1.1894261837005615\n",
            "iteration 276 :train_loss:1.2311787605285645 val_loss1.1890387535095215\n",
            "iteration 277 :train_loss:1.2308090925216675 val_loss1.188652515411377\n",
            "iteration 278 :train_loss:1.2304402589797974 val_loss1.188267707824707\n",
            "iteration 279 :train_loss:1.2300724983215332 val_loss1.187883973121643\n",
            "iteration 280 :train_loss:1.2297061681747437 val_loss1.187501311302185\n",
            "iteration 281 :train_loss:1.2293407917022705 val_loss1.1871198415756226\n",
            "iteration 282 :train_loss:1.2289767265319824 val_loss1.1867398023605347\n",
            "iteration 283 :train_loss:1.2286138534545898 val_loss1.1863608360290527\n",
            "iteration 284 :train_loss:1.2282522916793823 val_loss1.1859831809997559\n",
            "iteration 285 :train_loss:1.2278918027877808 val_loss1.1856069564819336\n",
            "iteration 286 :train_loss:1.2275325059890747 val_loss1.1852316856384277\n",
            "iteration 287 :train_loss:1.2271745204925537 val_loss1.1848578453063965\n",
            "iteration 288 :train_loss:1.2268177270889282 val_loss1.1844851970672607\n",
            "iteration 289 :train_loss:1.2264620065689087 val_loss1.1841137409210205\n",
            "iteration 290 :train_loss:1.2261079549789429 val_loss1.1837437152862549\n",
            "iteration 291 :train_loss:1.225754976272583 val_loss1.1833750009536743\n",
            "iteration 292 :train_loss:1.2254033088684082 val_loss1.1830073595046997\n",
            "iteration 293 :train_loss:1.2250525951385498 val_loss1.1826411485671997\n",
            "iteration 294 :train_loss:1.2247034311294556 val_loss1.1822763681411743\n",
            "iteration 295 :train_loss:1.2243554592132568 val_loss1.1819127798080444\n",
            "iteration 296 :train_loss:1.2240087985992432 val_loss1.18155038356781\n",
            "iteration 297 :train_loss:1.2236632108688354 val_loss1.1811896562576294\n",
            "iteration 298 :train_loss:1.223319172859192 val_loss1.1808301210403442\n",
            "iteration 299 :train_loss:1.2229764461517334 val_loss1.1804717779159546\n",
            "iteration 300 :train_loss:1.22263503074646 val_loss1.1801148653030396\n",
            "iteration 301 :train_loss:1.2222949266433716 val_loss1.1797592639923096\n",
            "iteration 302 :train_loss:1.2219561338424683 val_loss1.179404854774475\n",
            "iteration 303 :train_loss:1.2216185331344604 val_loss1.1790521144866943\n",
            "iteration 304 :train_loss:1.2212824821472168 val_loss1.1787006855010986\n",
            "iteration 305 :train_loss:1.220947504043579 val_loss1.1783509254455566\n",
            "iteration 306 :train_loss:1.2206140756607056 val_loss1.1780019998550415\n",
            "iteration 307 :train_loss:1.2202820777893066 val_loss1.1776546239852905\n",
            "iteration 308 :train_loss:1.2199511528015137 val_loss1.1773086786270142\n",
            "iteration 309 :train_loss:1.2196214199066162 val_loss1.176964282989502\n",
            "iteration 310 :train_loss:1.2192937135696411 val_loss1.1766210794448853\n",
            "iteration 311 :train_loss:1.2189671993255615 val_loss1.1762794256210327\n",
            "iteration 312 :train_loss:1.218641757965088 val_loss1.1759392023086548\n",
            "iteration 313 :train_loss:1.2183177471160889 val_loss1.175600290298462\n",
            "iteration 314 :train_loss:1.2179951667785645 val_loss1.1752628087997437\n",
            "iteration 315 :train_loss:1.2176740169525146 val_loss1.1749268770217896\n",
            "iteration 316 :train_loss:1.21735417842865 val_loss1.1745922565460205\n",
            "iteration 317 :train_loss:1.2170358896255493 val_loss1.1742589473724365\n",
            "iteration 318 :train_loss:1.2167189121246338 val_loss1.1739273071289062\n",
            "iteration 319 :train_loss:1.2164032459259033 val_loss1.1735972166061401\n",
            "iteration 320 :train_loss:1.216089129447937 val_loss1.1732683181762695\n",
            "iteration 321 :train_loss:1.2157765626907349 val_loss1.172940969467163\n",
            "iteration 322 :train_loss:1.2154650688171387 val_loss1.1726150512695312\n",
            "iteration 323 :train_loss:1.2151552438735962 val_loss1.1722908020019531\n",
            "iteration 324 :train_loss:1.2148467302322388 val_loss1.17196786403656\n",
            "iteration 325 :train_loss:1.2145395278930664 val_loss1.1716463565826416\n",
            "iteration 326 :train_loss:1.2142341136932373 val_loss1.1713262796401978\n",
            "iteration 327 :train_loss:1.2139301300048828 val_loss1.171007752418518\n",
            "iteration 328 :train_loss:1.2136270999908447 val_loss1.1706907749176025\n",
            "iteration 329 :train_loss:1.21332585811615 val_loss1.1703753471374512\n",
            "iteration 330 :train_loss:1.2130260467529297 val_loss1.1700613498687744\n",
            "iteration 331 :train_loss:1.212727665901184 val_loss1.1697487831115723\n",
            "iteration 332 :train_loss:1.2124308347702026 val_loss1.1694377660751343\n",
            "iteration 333 :train_loss:1.2121353149414062 val_loss1.1691282987594604\n",
            "iteration 334 :train_loss:1.2118412256240845 val_loss1.1688202619552612\n",
            "iteration 335 :train_loss:1.2115488052368164 val_loss1.1685136556625366\n",
            "iteration 336 :train_loss:1.2112575769424438 val_loss1.1682088375091553\n",
            "iteration 337 :train_loss:1.210968255996704 val_loss1.167905330657959\n",
            "iteration 338 :train_loss:1.2106801271438599 val_loss1.1676033735275269\n",
            "iteration 339 :train_loss:1.2103931903839111 val_loss1.1673029661178589\n",
            "iteration 340 :train_loss:1.2101081609725952 val_loss1.167004108428955\n",
            "iteration 341 :train_loss:1.209824562072754 val_loss1.1667065620422363\n",
            "iteration 342 :train_loss:1.209542155265808 val_loss1.1664108037948608\n",
            "iteration 343 :train_loss:1.209261417388916 val_loss1.16611647605896\n",
            "iteration 344 :train_loss:1.2089821100234985 val_loss1.1658238172531128\n",
            "iteration 345 :train_loss:1.2087044715881348 val_loss1.1655324697494507\n",
            "iteration 346 :train_loss:1.208428144454956 val_loss1.1652427911758423\n",
            "iteration 347 :train_loss:1.208153486251831 val_loss1.164954662322998\n",
            "iteration 348 :train_loss:1.2078800201416016 val_loss1.164668083190918\n",
            "iteration 349 :train_loss:1.2076083421707153 val_loss1.1643829345703125\n",
            "iteration 350 :train_loss:1.207337737083435 val_loss1.1640994548797607\n",
            "iteration 351 :train_loss:1.2070690393447876 val_loss1.1638174057006836\n",
            "iteration 352 :train_loss:1.2068017721176147 val_loss1.1635370254516602\n",
            "iteration 353 :train_loss:1.2065359354019165 val_loss1.1632579565048218\n",
            "iteration 354 :train_loss:1.2062715291976929 val_loss1.1629806756973267\n",
            "iteration 355 :train_loss:1.2060086727142334 val_loss1.1627049446105957\n",
            "iteration 356 :train_loss:1.205747365951538 val_loss1.1624306440353394\n",
            "iteration 357 :train_loss:1.205487608909607 val_loss1.1621577739715576\n",
            "iteration 358 :train_loss:1.2052291631698608 val_loss1.1618868112564087\n",
            "iteration 359 :train_loss:1.204972267150879 val_loss1.1616171598434448\n",
            "iteration 360 :train_loss:1.2047169208526611 val_loss1.1613489389419556\n",
            "iteration 361 :train_loss:1.204463005065918 val_loss1.1610825061798096\n",
            "iteration 362 :train_loss:1.204210638999939 val_loss1.1608175039291382\n",
            "iteration 363 :train_loss:1.2039598226547241 val_loss1.1605541706085205\n",
            "iteration 364 :train_loss:1.2037105560302734 val_loss1.160292387008667\n",
            "iteration 365 :train_loss:1.2034626007080078 val_loss1.160032033920288\n",
            "iteration 366 :train_loss:1.2032161951065063 val_loss1.1597731113433838\n",
            "iteration 367 :train_loss:1.202971339225769 val_loss1.1595159769058228\n",
            "iteration 368 :train_loss:1.202728033065796 val_loss1.1592603921890259\n",
            "iteration 369 :train_loss:1.2024861574172974 val_loss1.1590062379837036\n",
            "iteration 370 :train_loss:1.2022457122802734 val_loss1.1587536334991455\n",
            "iteration 371 :train_loss:1.2020068168640137 val_loss1.1585025787353516\n",
            "iteration 372 :train_loss:1.2017693519592285 val_loss1.1582531929016113\n",
            "iteration 373 :train_loss:1.201533317565918 val_loss1.1580053567886353\n",
            "iteration 374 :train_loss:1.2012989521026611 val_loss1.1577588319778442\n",
            "iteration 375 :train_loss:1.2010658979415894 val_loss1.157513976097107\n",
            "iteration 376 :train_loss:1.2008345127105713 val_loss1.1572706699371338\n",
            "iteration 377 :train_loss:1.2006045579910278 val_loss1.1570287942886353\n",
            "iteration 378 :train_loss:1.2003761529922485 val_loss1.1567885875701904\n",
            "iteration 379 :train_loss:1.2001490592956543 val_loss1.1565498113632202\n",
            "iteration 380 :train_loss:1.1999236345291138 val_loss1.1563125848770142\n",
            "iteration 381 :train_loss:1.1996994018554688 val_loss1.1560770273208618\n",
            "iteration 382 :train_loss:1.199476957321167 val_loss1.155842900276184\n",
            "iteration 383 :train_loss:1.1992557048797607 val_loss1.155610203742981\n",
            "iteration 384 :train_loss:1.1990361213684082 val_loss1.1553791761398315\n",
            "iteration 385 :train_loss:1.1988179683685303 val_loss1.1551496982574463\n",
            "iteration 386 :train_loss:1.198601245880127 val_loss1.1549214124679565\n",
            "iteration 387 :train_loss:1.1983859539031982 val_loss1.1546951532363892\n",
            "iteration 388 :train_loss:1.1981720924377441 val_loss1.1544698476791382\n",
            "iteration 389 :train_loss:1.1979596614837646 val_loss1.1542465686798096\n",
            "iteration 390 :train_loss:1.1977487802505493 val_loss1.154024600982666\n",
            "iteration 391 :train_loss:1.1975393295288086 val_loss1.1538039445877075\n",
            "iteration 392 :train_loss:1.1973313093185425 val_loss1.1535848379135132\n",
            "iteration 393 :train_loss:1.1971246004104614 val_loss1.1533674001693726\n",
            "iteration 394 :train_loss:1.196919560432434 val_loss1.1531513929367065\n",
            "iteration 395 :train_loss:1.1967158317565918 val_loss1.1529370546340942\n",
            "iteration 396 :train_loss:1.1965135335922241 val_loss1.152723789215088\n",
            "iteration 397 :train_loss:1.1963127851486206 val_loss1.1525123119354248\n",
            "iteration 398 :train_loss:1.1961132287979126 val_loss1.1523021459579468\n",
            "iteration 399 :train_loss:1.1959153413772583 val_loss1.1520934104919434\n",
            "iteration 400 :train_loss:1.195718765258789 val_loss1.1518864631652832\n",
            "iteration 401 :train_loss:1.1955235004425049 val_loss1.1516807079315186\n",
            "iteration 402 :train_loss:1.1953297853469849 val_loss1.151476502418518\n",
            "iteration 403 :train_loss:1.19513738155365 val_loss1.1512737274169922\n",
            "iteration 404 :train_loss:1.194946527481079 val_loss1.151072382926941\n",
            "iteration 405 :train_loss:1.1947567462921143 val_loss1.1508725881576538\n",
            "iteration 406 :train_loss:1.1945687532424927 val_loss1.1506739854812622\n",
            "iteration 407 :train_loss:1.1943819522857666 val_loss1.1504771709442139\n",
            "iteration 408 :train_loss:1.1941964626312256 val_loss1.150281548500061\n",
            "iteration 409 :train_loss:1.1940125226974487 val_loss1.1500874757766724\n",
            "iteration 410 :train_loss:1.193829894065857 val_loss1.1498948335647583\n",
            "iteration 411 :train_loss:1.1936485767364502 val_loss1.1497033834457397\n",
            "iteration 412 :train_loss:1.193468689918518 val_loss1.1495137214660645\n",
            "iteration 413 :train_loss:1.1932899951934814 val_loss1.1493251323699951\n",
            "iteration 414 :train_loss:1.193112850189209 val_loss1.149138331413269\n",
            "iteration 415 :train_loss:1.1929370164871216 val_loss1.148952603340149\n",
            "iteration 416 :train_loss:1.1927624940872192 val_loss1.148768424987793\n",
            "iteration 417 :train_loss:1.192589282989502 val_loss1.148585557937622\n",
            "iteration 418 :train_loss:1.1924173831939697 val_loss1.1484042406082153\n",
            "iteration 419 :train_loss:1.192246913909912 val_loss1.1482239961624146\n",
            "iteration 420 :train_loss:1.1920775175094604 val_loss1.148045301437378\n",
            "iteration 421 :train_loss:1.1919097900390625 val_loss1.1478679180145264\n",
            "iteration 422 :train_loss:1.1917431354522705 val_loss1.1476919651031494\n",
            "iteration 423 :train_loss:1.1915779113769531 val_loss1.1475173234939575\n",
            "iteration 424 :train_loss:1.1914138793945312 val_loss1.1473439931869507\n",
            "iteration 425 :train_loss:1.1912513971328735 val_loss1.147172212600708\n",
            "iteration 426 :train_loss:1.1910897493362427 val_loss1.1470016241073608\n",
            "iteration 427 :train_loss:1.1909297704696655 val_loss1.1468323469161987\n",
            "iteration 428 :train_loss:1.1907708644866943 val_loss1.1466643810272217\n",
            "iteration 429 :train_loss:1.1906133890151978 val_loss1.1464978456497192\n",
            "iteration 430 :train_loss:1.1904571056365967 val_loss1.1463326215744019\n",
            "iteration 431 :train_loss:1.1903021335601807 val_loss1.14616858959198\n",
            "iteration 432 :train_loss:1.1901483535766602 val_loss1.1460059881210327\n",
            "iteration 433 :train_loss:1.1899956464767456 val_loss1.145844578742981\n",
            "iteration 434 :train_loss:1.1898444890975952 val_loss1.1456844806671143\n",
            "iteration 435 :train_loss:1.1896944046020508 val_loss1.1455258131027222\n",
            "iteration 436 :train_loss:1.1895455121994019 val_loss1.145368218421936\n",
            "iteration 437 :train_loss:1.1893980503082275 val_loss1.145211935043335\n",
            "iteration 438 :train_loss:1.1892515420913696 val_loss1.145056962966919\n",
            "iteration 439 :train_loss:1.1891064643859863 val_loss1.144903302192688\n",
            "iteration 440 :train_loss:1.188962459564209 val_loss1.144750952720642\n",
            "iteration 441 :train_loss:1.1888196468353271 val_loss1.1445995569229126\n",
            "iteration 442 :train_loss:1.18867826461792 val_loss1.1444498300552368\n",
            "iteration 443 :train_loss:1.188537836074829 val_loss1.1443010568618774\n",
            "iteration 444 :train_loss:1.1883984804153442 val_loss1.1441535949707031\n",
            "iteration 445 :train_loss:1.1882603168487549 val_loss1.1440073251724243\n",
            "iteration 446 :train_loss:1.1881235837936401 val_loss1.143862247467041\n",
            "iteration 447 :train_loss:1.1879878044128418 val_loss1.1437183618545532\n",
            "iteration 448 :train_loss:1.1878533363342285 val_loss1.1435757875442505\n",
            "iteration 449 :train_loss:1.1877198219299316 val_loss1.1434341669082642\n",
            "iteration 450 :train_loss:1.1875876188278198 val_loss1.143293857574463\n",
            "iteration 451 :train_loss:1.1874563694000244 val_loss1.1431547403335571\n",
            "iteration 452 :train_loss:1.187326431274414 val_loss1.1430168151855469\n",
            "iteration 453 :train_loss:1.1871975660324097 val_loss1.1428800821304321\n",
            "iteration 454 :train_loss:1.1870697736740112 val_loss1.142744541168213\n",
            "iteration 455 :train_loss:1.1869430541992188 val_loss1.1426101922988892\n",
            "iteration 456 :train_loss:1.1868174076080322 val_loss1.1424769163131714\n",
            "iteration 457 :train_loss:1.1866929531097412 val_loss1.14234459400177\n",
            "iteration 458 :train_loss:1.1865695714950562 val_loss1.1422134637832642\n",
            "iteration 459 :train_loss:1.1864471435546875 val_loss1.1420836448669434\n",
            "iteration 460 :train_loss:1.186326026916504 val_loss1.141955018043518\n",
            "iteration 461 :train_loss:1.1862058639526367 val_loss1.1418272256851196\n",
            "iteration 462 :train_loss:1.1860865354537964 val_loss1.1417007446289062\n",
            "iteration 463 :train_loss:1.1859685182571411 val_loss1.1415753364562988\n",
            "iteration 464 :train_loss:1.1858515739440918 val_loss1.1414508819580078\n",
            "iteration 465 :train_loss:1.1857355833053589 val_loss1.1413273811340332\n",
            "iteration 466 :train_loss:1.1856204271316528 val_loss1.1412053108215332\n",
            "iteration 467 :train_loss:1.1855064630508423 val_loss1.14108407497406\n",
            "iteration 468 :train_loss:1.1853934526443481 val_loss1.1409640312194824\n",
            "iteration 469 :train_loss:1.18528151512146 val_loss1.1408448219299316\n",
            "iteration 470 :train_loss:1.1851706504821777 val_loss1.140726923942566\n",
            "iteration 471 :train_loss:1.185060739517212 val_loss1.1406099796295166\n",
            "iteration 472 :train_loss:1.184951901435852 val_loss1.1404939889907837\n",
            "iteration 473 :train_loss:1.184843897819519 val_loss1.1403790712356567\n",
            "iteration 474 :train_loss:1.1847368478775024 val_loss1.1402652263641357\n",
            "iteration 475 :train_loss:1.1846308708190918 val_loss1.1401523351669312\n",
            "iteration 476 :train_loss:1.184525728225708 val_loss1.140040397644043\n",
            "iteration 477 :train_loss:1.1844216585159302 val_loss1.1399296522140503\n",
            "iteration 478 :train_loss:1.1843184232711792 val_loss1.1398197412490845\n",
            "iteration 479 :train_loss:1.1842162609100342 val_loss1.1397109031677246\n",
            "iteration 480 :train_loss:1.1841150522232056 val_loss1.1396030187606812\n",
            "iteration 481 :train_loss:1.1840146780014038 val_loss1.1394959688186646\n",
            "iteration 482 :train_loss:1.183915138244629 val_loss1.1393898725509644\n",
            "iteration 483 :train_loss:1.18381667137146 val_loss1.1392848491668701\n",
            "iteration 484 :train_loss:1.1837188005447388 val_loss1.1391806602478027\n",
            "iteration 485 :train_loss:1.1836223602294922 val_loss1.1390775442123413\n",
            "iteration 486 :train_loss:1.1835265159606934 val_loss1.1389752626419067\n",
            "iteration 487 :train_loss:1.1834315061569214 val_loss1.1388740539550781\n",
            "iteration 488 :train_loss:1.1833375692367554 val_loss1.1387736797332764\n",
            "iteration 489 :train_loss:1.183244228363037 val_loss1.1386741399765015\n",
            "iteration 490 :train_loss:1.1831519603729248 val_loss1.138575553894043\n",
            "iteration 491 :train_loss:1.1830604076385498 val_loss1.1384780406951904\n",
            "iteration 492 :train_loss:1.1829699277877808 val_loss1.1383812427520752\n",
            "iteration 493 :train_loss:1.1828800439834595 val_loss1.1382852792739868\n",
            "iteration 494 :train_loss:1.1827911138534546 val_loss1.1381901502609253\n",
            "iteration 495 :train_loss:1.182702898979187 val_loss1.1380959749221802\n",
            "iteration 496 :train_loss:1.1826156377792358 val_loss1.1380027532577515\n",
            "iteration 497 :train_loss:1.1825292110443115 val_loss1.1379104852676392\n",
            "iteration 498 :train_loss:1.182443618774414 val_loss1.137818694114685\n",
            "iteration 499 :train_loss:1.182358741760254 val_loss1.1377280950546265\n",
            "iteration 500 :train_loss:1.182274580001831 val_loss1.1376380920410156\n",
            "iteration 501 :train_loss:1.1821913719177246 val_loss1.1375489234924316\n",
            "iteration 502 :train_loss:1.182108759880066 val_loss1.1374608278274536\n",
            "iteration 503 :train_loss:1.1820271015167236 val_loss1.1373733282089233\n",
            "iteration 504 :train_loss:1.1819462776184082 val_loss1.1372867822647095\n",
            "iteration 505 :train_loss:1.1818660497665405 val_loss1.1372005939483643\n",
            "iteration 506 :train_loss:1.1817866563796997 val_loss1.1371158361434937\n",
            "iteration 507 :train_loss:1.1817080974578857 val_loss1.1370316743850708\n",
            "iteration 508 :train_loss:1.18163001537323 val_loss1.1369479894638062\n",
            "iteration 509 :train_loss:1.181552767753601 val_loss1.136865258216858\n",
            "iteration 510 :train_loss:1.181476354598999 val_loss1.1367833614349365\n",
            "iteration 511 :train_loss:1.1814007759094238 val_loss1.1367021799087524\n",
            "iteration 512 :train_loss:1.1813256740570068 val_loss1.1366218328475952\n",
            "iteration 513 :train_loss:1.1812512874603271 val_loss1.1365420818328857\n",
            "iteration 514 :train_loss:1.1811777353286743 val_loss1.1364630460739136\n",
            "iteration 515 :train_loss:1.1811048984527588 val_loss1.1363848447799683\n",
            "iteration 516 :train_loss:1.1810325384140015 val_loss1.1363074779510498\n",
            "iteration 517 :train_loss:1.18096125125885 val_loss1.136230707168579\n",
            "iteration 518 :train_loss:1.180890440940857 val_loss1.1361546516418457\n",
            "iteration 519 :train_loss:1.180820345878601 val_loss1.1360793113708496\n",
            "iteration 520 :train_loss:1.180750846862793 val_loss1.1360045671463013\n",
            "iteration 521 :train_loss:1.180681824684143 val_loss1.1359307765960693\n",
            "iteration 522 :train_loss:1.1806137561798096 val_loss1.1358574628829956\n",
            "iteration 523 :train_loss:1.1805462837219238 val_loss1.1357851028442383\n",
            "iteration 524 :train_loss:1.1804794073104858 val_loss1.13571298122406\n",
            "iteration 525 :train_loss:1.1804132461547852 val_loss1.1356418132781982\n",
            "iteration 526 :train_loss:1.1803475618362427 val_loss1.1355712413787842\n",
            "iteration 527 :train_loss:1.1802825927734375 val_loss1.1355013847351074\n",
            "iteration 528 :train_loss:1.1802183389663696 val_loss1.1354320049285889\n",
            "iteration 529 :train_loss:1.1801543235778809 val_loss1.1353635787963867\n",
            "iteration 530 :train_loss:1.180091381072998 val_loss1.1352956295013428\n",
            "iteration 531 :train_loss:1.1800289154052734 val_loss1.1352282762527466\n",
            "iteration 532 :train_loss:1.1799668073654175 val_loss1.1351615190505981\n",
            "iteration 533 :train_loss:1.179905652999878 val_loss1.1350955963134766\n",
            "iteration 534 :train_loss:1.179844856262207 val_loss1.1350302696228027\n",
            "iteration 535 :train_loss:1.1797845363616943 val_loss1.134965419769287\n",
            "iteration 536 :train_loss:1.179724931716919 val_loss1.1349010467529297\n",
            "iteration 537 :train_loss:1.1796659231185913 val_loss1.1348373889923096\n",
            "iteration 538 :train_loss:1.1796075105667114 val_loss1.1347744464874268\n",
            "iteration 539 :train_loss:1.1795496940612793 val_loss1.1347118616104126\n",
            "iteration 540 :train_loss:1.1794923543930054 val_loss1.1346499919891357\n",
            "iteration 541 :train_loss:1.1794354915618896 val_loss1.1345888376235962\n",
            "iteration 542 :train_loss:1.1793793439865112 val_loss1.1345279216766357\n",
            "iteration 543 :train_loss:1.1793235540390015 val_loss1.1344677209854126\n",
            "iteration 544 :train_loss:1.179268479347229 val_loss1.1344083547592163\n",
            "iteration 545 :train_loss:1.1792137622833252 val_loss1.1343493461608887\n",
            "iteration 546 :train_loss:1.1791596412658691 val_loss1.1342906951904297\n",
            "iteration 547 :train_loss:1.1791058778762817 val_loss1.1342326402664185\n",
            "iteration 548 :train_loss:1.1790528297424316 val_loss1.134175419807434\n",
            "iteration 549 :train_loss:1.1790001392364502 val_loss1.1341183185577393\n",
            "iteration 550 :train_loss:1.178947925567627 val_loss1.1340621709823608\n",
            "iteration 551 :train_loss:1.178896427154541 val_loss1.1340062618255615\n",
            "iteration 552 :train_loss:1.1788451671600342 val_loss1.13395094871521\n",
            "iteration 553 :train_loss:1.1787946224212646 val_loss1.1338961124420166\n",
            "iteration 554 :train_loss:1.1787444353103638 val_loss1.1338417530059814\n",
            "iteration 555 :train_loss:1.178694725036621 val_loss1.1337881088256836\n",
            "iteration 556 :train_loss:1.1786454916000366 val_loss1.1337345838546753\n",
            "iteration 557 :train_loss:1.1785967350006104 val_loss1.1336816549301147\n",
            "iteration 558 :train_loss:1.1785483360290527 val_loss1.133629560470581\n",
            "iteration 559 :train_loss:1.1785005331039429 val_loss1.133577585220337\n",
            "iteration 560 :train_loss:1.1784532070159912 val_loss1.133526086807251\n",
            "iteration 561 :train_loss:1.1784062385559082 val_loss1.1334753036499023\n",
            "iteration 562 :train_loss:1.1783596277236938 val_loss1.1334248781204224\n",
            "iteration 563 :train_loss:1.1783136129379272 val_loss1.133374810218811\n",
            "iteration 564 :train_loss:1.1782678365707397 val_loss1.133325219154358\n",
            "iteration 565 :train_loss:1.17822265625 val_loss1.1332762241363525\n",
            "iteration 566 :train_loss:1.178177833557129 val_loss1.1332275867462158\n",
            "iteration 567 :train_loss:1.1781336069107056 val_loss1.1331793069839478\n",
            "iteration 568 :train_loss:1.1780894994735718 val_loss1.133131504058838\n",
            "iteration 569 :train_loss:1.1780459880828857 val_loss1.1330841779708862\n",
            "iteration 570 :train_loss:1.1780028343200684 val_loss1.1330372095108032\n",
            "iteration 571 :train_loss:1.1779600381851196 val_loss1.1329909563064575\n",
            "iteration 572 :train_loss:1.1779175996780396 val_loss1.132944941520691\n",
            "iteration 573 :train_loss:1.1778757572174072 val_loss1.1328990459442139\n",
            "iteration 574 :train_loss:1.1778340339660645 val_loss1.1328538656234741\n",
            "iteration 575 :train_loss:1.1777927875518799 val_loss1.132809042930603\n",
            "iteration 576 :train_loss:1.177752137184143 val_loss1.1327646970748901\n",
            "iteration 577 :train_loss:1.1777114868164062 val_loss1.1327204704284668\n",
            "iteration 578 :train_loss:1.1776714324951172 val_loss1.1326768398284912\n",
            "iteration 579 :train_loss:1.1776317358016968 val_loss1.1326336860656738\n",
            "iteration 580 :train_loss:1.177592396736145 val_loss1.132590889930725\n",
            "iteration 581 :train_loss:1.1775532960891724 val_loss1.132548213005066\n",
            "iteration 582 :train_loss:1.177514672279358 val_loss1.1325061321258545\n",
            "iteration 583 :train_loss:1.177476406097412 val_loss1.1324644088745117\n",
            "iteration 584 :train_loss:1.177438497543335 val_loss1.1324230432510376\n",
            "iteration 585 :train_loss:1.1774009466171265 val_loss1.1323819160461426\n",
            "iteration 586 :train_loss:1.1773635149002075 val_loss1.1323412656784058\n",
            "iteration 587 :train_loss:1.1773265600204468 val_loss1.1323009729385376\n",
            "iteration 588 :train_loss:1.1772898435592651 val_loss1.132261037826538\n",
            "iteration 589 :train_loss:1.1772537231445312 val_loss1.1322214603424072\n",
            "iteration 590 :train_loss:1.177217721939087 val_loss1.132182240486145\n",
            "iteration 591 :train_loss:1.1771820783615112 val_loss1.132143259048462\n",
            "iteration 592 :train_loss:1.1771469116210938 val_loss1.132104516029358\n",
            "iteration 593 :train_loss:1.1771118640899658 val_loss1.1320664882659912\n",
            "iteration 594 :train_loss:1.177077054977417 val_loss1.132028341293335\n",
            "iteration 595 :train_loss:1.1770427227020264 val_loss1.1319907903671265\n",
            "iteration 596 :train_loss:1.1770085096359253 val_loss1.1319535970687866\n",
            "iteration 597 :train_loss:1.1769746541976929 val_loss1.1319165229797363\n",
            "iteration 598 :train_loss:1.176941156387329 val_loss1.1318799257278442\n",
            "iteration 599 :train_loss:1.176908016204834 val_loss1.1318435668945312\n",
            "iteration 600 :train_loss:1.176875114440918 val_loss1.1318074464797974\n",
            "iteration 601 :train_loss:1.176842451095581 val_loss1.1317715644836426\n",
            "iteration 602 :train_loss:1.1768101453781128 val_loss1.1317362785339355\n",
            "iteration 603 :train_loss:1.176777958869934 val_loss1.1317009925842285\n",
            "iteration 604 :train_loss:1.176746129989624 val_loss1.1316663026809692\n",
            "iteration 605 :train_loss:1.1767144203186035 val_loss1.1316317319869995\n",
            "iteration 606 :train_loss:1.1766831874847412 val_loss1.1315975189208984\n",
            "iteration 607 :train_loss:1.1766523122787476 val_loss1.1315633058547974\n",
            "iteration 608 :train_loss:1.1766215562820435 val_loss1.1315295696258545\n",
            "iteration 609 :train_loss:1.1765910387039185 val_loss1.1314963102340698\n",
            "iteration 610 :train_loss:1.1765607595443726 val_loss1.1314631700515747\n",
            "iteration 611 :train_loss:1.1765308380126953 val_loss1.1314302682876587\n",
            "iteration 612 :train_loss:1.176500916481018 val_loss1.1313976049423218\n",
            "iteration 613 :train_loss:1.176471471786499 val_loss1.1313650608062744\n",
            "iteration 614 :train_loss:1.176442265510559 val_loss1.1313331127166748\n",
            "iteration 615 :train_loss:1.1764132976531982 val_loss1.1313011646270752\n",
            "iteration 616 :train_loss:1.1763845682144165 val_loss1.1312695741653442\n",
            "iteration 617 :train_loss:1.1763560771942139 val_loss1.1312382221221924\n",
            "iteration 618 :train_loss:1.1763278245925903 val_loss1.1312071084976196\n",
            "iteration 619 :train_loss:1.1762994527816772 val_loss1.131176233291626\n",
            "iteration 620 :train_loss:1.176271915435791 val_loss1.1311455965042114\n",
            "iteration 621 :train_loss:1.1762441396713257 val_loss1.1311153173446655\n",
            "iteration 622 :train_loss:1.176216721534729 val_loss1.1310851573944092\n",
            "iteration 623 :train_loss:1.176189661026001 val_loss1.1310551166534424\n",
            "iteration 624 :train_loss:1.1761624813079834 val_loss1.1310254335403442\n",
            "iteration 625 :train_loss:1.176135778427124 val_loss1.1309959888458252\n",
            "iteration 626 :train_loss:1.1761090755462646 val_loss1.1309667825698853\n",
            "iteration 627 :train_loss:1.176082968711853 val_loss1.1309378147125244\n",
            "iteration 628 :train_loss:1.1760568618774414 val_loss1.1309090852737427\n",
            "iteration 629 :train_loss:1.1760308742523193 val_loss1.130880355834961\n",
            "iteration 630 :train_loss:1.1760050058364868 val_loss1.130852222442627\n",
            "iteration 631 :train_loss:1.1759796142578125 val_loss1.1308239698410034\n",
            "iteration 632 :train_loss:1.1759543418884277 val_loss1.1307960748672485\n",
            "iteration 633 :train_loss:1.1759291887283325 val_loss1.1307682991027832\n",
            "iteration 634 :train_loss:1.1759041547775269 val_loss1.1307408809661865\n",
            "iteration 635 :train_loss:1.1758792400360107 val_loss1.1307133436203003\n",
            "iteration 636 :train_loss:1.1758548021316528 val_loss1.1306862831115723\n",
            "iteration 637 :train_loss:1.175830364227295 val_loss1.1306594610214233\n",
            "iteration 638 :train_loss:1.1758061647415161 val_loss1.1306326389312744\n",
            "iteration 639 :train_loss:1.1757820844650269 val_loss1.1306061744689941\n",
            "iteration 640 :train_loss:1.1757582426071167 val_loss1.1305797100067139\n",
            "iteration 641 :train_loss:1.175734519958496 val_loss1.1305537223815918\n",
            "iteration 642 :train_loss:1.1757110357284546 val_loss1.1305277347564697\n",
            "iteration 643 :train_loss:1.175687551498413 val_loss1.1305019855499268\n",
            "iteration 644 :train_loss:1.1756645441055298 val_loss1.130476474761963\n",
            "iteration 645 :train_loss:1.1756415367126465 val_loss1.1304508447647095\n",
            "iteration 646 :train_loss:1.1756185293197632 val_loss1.1304256916046143\n",
            "iteration 647 :train_loss:1.175595998764038 val_loss1.1304006576538086\n",
            "iteration 648 :train_loss:1.175573468208313 val_loss1.130375623703003\n",
            "iteration 649 :train_loss:1.1755510568618774 val_loss1.130350947380066\n",
            "iteration 650 :train_loss:1.175528883934021 val_loss1.1303263902664185\n",
            "iteration 651 :train_loss:1.1755067110061646 val_loss1.130301833152771\n",
            "iteration 652 :train_loss:1.1754847764968872 val_loss1.1302778720855713\n",
            "iteration 653 :train_loss:1.1754629611968994 val_loss1.1302536725997925\n",
            "iteration 654 :train_loss:1.1754415035247803 val_loss1.1302298307418823\n",
            "iteration 655 :train_loss:1.1754200458526611 val_loss1.1302061080932617\n",
            "iteration 656 :train_loss:1.175398826599121 val_loss1.1301823854446411\n",
            "iteration 657 :train_loss:1.175377368927002 val_loss1.1301590204238892\n",
            "iteration 658 :train_loss:1.175356388092041 val_loss1.1301356554031372\n",
            "iteration 659 :train_loss:1.1753356456756592 val_loss1.130112648010254\n",
            "iteration 660 :train_loss:1.1753147840499878 val_loss1.1300896406173706\n",
            "iteration 661 :train_loss:1.1752941608428955 val_loss1.1300667524337769\n",
            "iteration 662 :train_loss:1.1752736568450928 val_loss1.1300441026687622\n",
            "iteration 663 :train_loss:1.17525315284729 val_loss1.130021572113037\n",
            "iteration 664 :train_loss:1.1752331256866455 val_loss1.129999041557312\n",
            "iteration 665 :train_loss:1.1752129793167114 val_loss1.129976749420166\n",
            "iteration 666 :train_loss:1.175192952156067 val_loss1.1299546957015991\n",
            "iteration 667 :train_loss:1.1751731634140015 val_loss1.1299327611923218\n",
            "iteration 668 :train_loss:1.175153374671936 val_loss1.1299108266830444\n",
            "iteration 669 :train_loss:1.1751338243484497 val_loss1.1298891305923462\n",
            "iteration 670 :train_loss:1.1751142740249634 val_loss1.129867434501648\n",
            "iteration 671 :train_loss:1.1750949621200562 val_loss1.1298460960388184\n",
            "iteration 672 :train_loss:1.175075650215149 val_loss1.1298246383666992\n",
            "iteration 673 :train_loss:1.1750564575195312 val_loss1.1298034191131592\n",
            "iteration 674 :train_loss:1.1750375032424927 val_loss1.1297823190689087\n",
            "iteration 675 :train_loss:1.1750186681747437 val_loss1.1297613382339478\n",
            "iteration 676 :train_loss:1.1749998331069946 val_loss1.1297404766082764\n",
            "iteration 677 :train_loss:1.1749811172485352 val_loss1.129719853401184\n",
            "iteration 678 :train_loss:1.1749624013900757 val_loss1.1296991109848022\n",
            "iteration 679 :train_loss:1.1749441623687744 val_loss1.129678726196289\n",
            "iteration 680 :train_loss:1.1749255657196045 val_loss1.1296583414077759\n",
            "iteration 681 :train_loss:1.1749073266983032 val_loss1.1296379566192627\n",
            "iteration 682 :train_loss:1.1748894453048706 val_loss1.1296179294586182\n",
            "iteration 683 :train_loss:1.1748712062835693 val_loss1.1295979022979736\n",
            "iteration 684 :train_loss:1.1748532056808472 val_loss1.129577875137329\n",
            "iteration 685 :train_loss:1.174835443496704 val_loss1.1295580863952637\n",
            "iteration 686 :train_loss:1.174817681312561 val_loss1.1295384168624878\n",
            "iteration 687 :train_loss:1.174799919128418 val_loss1.1295186281204224\n",
            "iteration 688 :train_loss:1.174782395362854 val_loss1.1294993162155151\n",
            "iteration 689 :train_loss:1.17476487159729 val_loss1.1294798851013184\n",
            "iteration 690 :train_loss:1.1747475862503052 val_loss1.1294605731964111\n",
            "iteration 691 :train_loss:1.1747303009033203 val_loss1.1294413805007935\n",
            "iteration 692 :train_loss:1.1747130155563354 val_loss1.1294223070144653\n",
            "iteration 693 :train_loss:1.1746958494186401 val_loss1.1294032335281372\n",
            "iteration 694 :train_loss:1.174678921699524 val_loss1.1293842792510986\n",
            "iteration 695 :train_loss:1.1746619939804077 val_loss1.1293655633926392\n",
            "iteration 696 :train_loss:1.174645185470581 val_loss1.1293467283248901\n",
            "iteration 697 :train_loss:1.1746283769607544 val_loss1.1293281316757202\n",
            "iteration 698 :train_loss:1.1746116876602173 val_loss1.1293095350265503\n",
            "iteration 699 :train_loss:1.1745949983596802 val_loss1.1292911767959595\n",
            "iteration 700 :train_loss:1.1745785474777222 val_loss1.1292729377746582\n",
            "iteration 701 :train_loss:1.1745622158050537 val_loss1.1292544603347778\n",
            "iteration 702 :train_loss:1.1745456457138062 val_loss1.1292364597320557\n",
            "iteration 703 :train_loss:1.1745295524597168 val_loss1.1292182207107544\n",
            "iteration 704 :train_loss:1.1745132207870483 val_loss1.1292002201080322\n",
            "iteration 705 :train_loss:1.1744970083236694 val_loss1.1291823387145996\n",
            "iteration 706 :train_loss:1.1744810342788696 val_loss1.1291645765304565\n",
            "iteration 707 :train_loss:1.1744650602340698 val_loss1.1291468143463135\n",
            "iteration 708 :train_loss:1.1744492053985596 val_loss1.1291290521621704\n",
            "iteration 709 :train_loss:1.1744333505630493 val_loss1.1291115283966064\n",
            "iteration 710 :train_loss:1.1744176149368286 val_loss1.1290940046310425\n",
            "iteration 711 :train_loss:1.174401879310608 val_loss1.1290764808654785\n",
            "iteration 712 :train_loss:1.1743860244750977 val_loss1.1290591955184937\n",
            "iteration 713 :train_loss:1.1743706464767456 val_loss1.1290417909622192\n",
            "iteration 714 :train_loss:1.174355149269104 val_loss1.129024624824524\n",
            "iteration 715 :train_loss:1.1743396520614624 val_loss1.1290074586868286\n",
            "iteration 716 :train_loss:1.1743242740631104 val_loss1.1289905309677124\n",
            "iteration 717 :train_loss:1.1743091344833374 val_loss1.1289734840393066\n",
            "iteration 718 :train_loss:1.174293875694275 val_loss1.1289565563201904\n",
            "iteration 719 :train_loss:1.1742786169052124 val_loss1.1289397478103638\n",
            "iteration 720 :train_loss:1.17426335811615 val_loss1.1289228200912476\n",
            "iteration 721 :train_loss:1.1742483377456665 val_loss1.1289061307907104\n",
            "iteration 722 :train_loss:1.1742335557937622 val_loss1.1288894414901733\n",
            "iteration 723 :train_loss:1.1742184162139893 val_loss1.1288729906082153\n",
            "iteration 724 :train_loss:1.174203634262085 val_loss1.1288565397262573\n",
            "iteration 725 :train_loss:1.1741888523101807 val_loss1.1288398504257202\n",
            "iteration 726 :train_loss:1.174174189567566 val_loss1.1288233995437622\n",
            "iteration 727 :train_loss:1.1741594076156616 val_loss1.1288071870803833\n",
            "iteration 728 :train_loss:1.1741447448730469 val_loss1.1287909746170044\n",
            "iteration 729 :train_loss:1.1741300821304321 val_loss1.128774642944336\n",
            "iteration 730 :train_loss:1.1741156578063965 val_loss1.1287585496902466\n",
            "iteration 731 :train_loss:1.1741011142730713 val_loss1.1287425756454468\n",
            "iteration 732 :train_loss:1.1740868091583252 val_loss1.1287263631820679\n",
            "iteration 733 :train_loss:1.1740723848342896 val_loss1.1287105083465576\n",
            "iteration 734 :train_loss:1.1740580797195435 val_loss1.1286946535110474\n",
            "iteration 735 :train_loss:1.1740437746047974 val_loss1.1286786794662476\n",
            "iteration 736 :train_loss:1.1740294694900513 val_loss1.1286630630493164\n",
            "iteration 737 :train_loss:1.1740152835845947 val_loss1.1286470890045166\n",
            "iteration 738 :train_loss:1.1740013360977173 val_loss1.1286317110061646\n",
            "iteration 739 :train_loss:1.1739871501922607 val_loss1.1286157369613647\n",
            "iteration 740 :train_loss:1.1739732027053833 val_loss1.1286002397537231\n",
            "iteration 741 :train_loss:1.1739591360092163 val_loss1.128584861755371\n",
            "iteration 742 :train_loss:1.1739451885223389 val_loss1.128569483757019\n",
            "iteration 743 :train_loss:1.1739312410354614 val_loss1.1285539865493774\n",
            "iteration 744 :train_loss:1.173917293548584 val_loss1.1285384893417358\n",
            "iteration 745 :train_loss:1.1739035844802856 val_loss1.1285232305526733\n",
            "iteration 746 :train_loss:1.1738898754119873 val_loss1.1285079717636108\n",
            "iteration 747 :train_loss:1.1738762855529785 val_loss1.1284925937652588\n",
            "iteration 748 :train_loss:1.1738625764846802 val_loss1.1284775733947754\n",
            "iteration 749 :train_loss:1.1738487482070923 val_loss1.1284624338150024\n",
            "iteration 750 :train_loss:1.1738351583480835 val_loss1.1284472942352295\n",
            "iteration 751 :train_loss:1.1738216876983643 val_loss1.1284323930740356\n",
            "iteration 752 :train_loss:1.1738083362579346 val_loss1.1284173727035522\n",
            "iteration 753 :train_loss:1.1737946271896362 val_loss1.1284024715423584\n",
            "iteration 754 :train_loss:1.1737812757492065 val_loss1.1283875703811646\n",
            "iteration 755 :train_loss:1.1737679243087769 val_loss1.1283726692199707\n",
            "iteration 756 :train_loss:1.1737544536590576 val_loss1.1283577680587769\n",
            "iteration 757 :train_loss:1.1737412214279175 val_loss1.1283429861068726\n",
            "iteration 758 :train_loss:1.1737278699874878 val_loss1.1283283233642578\n",
            "iteration 759 :train_loss:1.1737147569656372 val_loss1.128313660621643\n",
            "iteration 760 :train_loss:1.1737016439437866 val_loss1.1282991170883179\n",
            "iteration 761 :train_loss:1.173688292503357 val_loss1.1282844543457031\n",
            "iteration 762 :train_loss:1.1736751794815063 val_loss1.128269910812378\n",
            "iteration 763 :train_loss:1.1736620664596558 val_loss1.1282554864883423\n",
            "iteration 764 :train_loss:1.1736490726470947 val_loss1.128240942955017\n",
            "iteration 765 :train_loss:1.1736359596252441 val_loss1.1282265186309814\n",
            "iteration 766 :train_loss:1.1736228466033936 val_loss1.1282119750976562\n",
            "iteration 767 :train_loss:1.1736098527908325 val_loss1.1281976699829102\n",
            "iteration 768 :train_loss:1.1735972166061401 val_loss1.128183364868164\n",
            "iteration 769 :train_loss:1.1735841035842896 val_loss1.1281689405441284\n",
            "iteration 770 :train_loss:1.1735713481903076 val_loss1.1281547546386719\n",
            "iteration 771 :train_loss:1.1735584735870361 val_loss1.1281405687332153\n",
            "iteration 772 :train_loss:1.1735455989837646 val_loss1.1281262636184692\n",
            "iteration 773 :train_loss:1.1735328435897827 val_loss1.1281123161315918\n",
            "iteration 774 :train_loss:1.1735202074050903 val_loss1.1280981302261353\n",
            "iteration 775 :train_loss:1.1735073328018188 val_loss1.1280841827392578\n",
            "iteration 776 :train_loss:1.1734946966171265 val_loss1.1280698776245117\n",
            "iteration 777 :train_loss:1.173482060432434 val_loss1.1280560493469238\n",
            "iteration 778 :train_loss:1.1734694242477417 val_loss1.1280421018600464\n",
            "iteration 779 :train_loss:1.1734567880630493 val_loss1.128028154373169\n",
            "iteration 780 :train_loss:1.173444151878357 val_loss1.1280142068862915\n",
            "iteration 781 :train_loss:1.173431634902954 val_loss1.1280003786087036\n",
            "iteration 782 :train_loss:1.1734192371368408 val_loss1.1279864311218262\n",
            "iteration 783 :train_loss:1.1734066009521484 val_loss1.1279724836349487\n",
            "iteration 784 :train_loss:1.1733942031860352 val_loss1.12795889377594\n",
            "iteration 785 :train_loss:1.1733816862106323 val_loss1.127945065498352\n",
            "iteration 786 :train_loss:1.173369288444519 val_loss1.1279313564300537\n",
            "iteration 787 :train_loss:1.1733568906784058 val_loss1.1279175281524658\n",
            "iteration 788 :train_loss:1.1733444929122925 val_loss1.127903938293457\n",
            "iteration 789 :train_loss:1.1733322143554688 val_loss1.1278902292251587\n",
            "iteration 790 :train_loss:1.173319935798645 val_loss1.12787663936615\n",
            "iteration 791 :train_loss:1.1733076572418213 val_loss1.1278630495071411\n",
            "iteration 792 :train_loss:1.1732953786849976 val_loss1.1278494596481323\n",
            "iteration 793 :train_loss:1.1732829809188843 val_loss1.127835988998413\n",
            "iteration 794 :train_loss:1.1732709407806396 val_loss1.1278225183486938\n",
            "iteration 795 :train_loss:1.1732585430145264 val_loss1.127808928489685\n",
            "iteration 796 :train_loss:1.1732465028762817 val_loss1.1277955770492554\n",
            "iteration 797 :train_loss:1.1732343435287476 val_loss1.1277822256088257\n",
            "iteration 798 :train_loss:1.1732221841812134 val_loss1.127768635749817\n",
            "iteration 799 :train_loss:1.1732100248336792 val_loss1.1277551651000977\n",
            "iteration 800 :train_loss:1.1731979846954346 val_loss1.127741813659668\n",
            "iteration 801 :train_loss:1.17318594455719 val_loss1.1277285814285278\n",
            "iteration 802 :train_loss:1.1731737852096558 val_loss1.1277152299880981\n",
            "iteration 803 :train_loss:1.1731617450714111 val_loss1.1277018785476685\n",
            "iteration 804 :train_loss:1.173149585723877 val_loss1.1276887655258179\n",
            "iteration 805 :train_loss:1.1731376647949219 val_loss1.1276756525039673\n",
            "iteration 806 :train_loss:1.1731257438659668 val_loss1.1276623010635376\n",
            "iteration 807 :train_loss:1.1731138229370117 val_loss1.127649188041687\n",
            "iteration 808 :train_loss:1.1731019020080566 val_loss1.1276359558105469\n",
            "iteration 809 :train_loss:1.173089861869812 val_loss1.1276229619979858\n",
            "iteration 810 :train_loss:1.173077940940857 val_loss1.1276097297668457\n",
            "iteration 811 :train_loss:1.1730661392211914 val_loss1.1275966167449951\n",
            "iteration 812 :train_loss:1.1730542182922363 val_loss1.127583384513855\n",
            "iteration 813 :train_loss:1.1730424165725708 val_loss1.1275705099105835\n",
            "iteration 814 :train_loss:1.1730304956436157 val_loss1.127557396888733\n",
            "iteration 815 :train_loss:1.1730186939239502 val_loss1.1275445222854614\n",
            "iteration 816 :train_loss:1.1730068922042847 val_loss1.1275314092636108\n",
            "iteration 817 :train_loss:1.1729952096939087 val_loss1.1275184154510498\n",
            "iteration 818 :train_loss:1.1729835271835327 val_loss1.1275054216384888\n",
            "iteration 819 :train_loss:1.1729717254638672 val_loss1.1274925470352173\n",
            "iteration 820 :train_loss:1.1729601621627808 val_loss1.1274797916412354\n",
            "iteration 821 :train_loss:1.1729482412338257 val_loss1.1274667978286743\n",
            "iteration 822 :train_loss:1.1729365587234497 val_loss1.1274539232254028\n",
            "iteration 823 :train_loss:1.1729249954223633 val_loss1.1274410486221313\n",
            "iteration 824 :train_loss:1.1729131937026978 val_loss1.1274281740188599\n",
            "iteration 825 :train_loss:1.1729015111923218 val_loss1.127415418624878\n",
            "iteration 826 :train_loss:1.1728898286819458 val_loss1.1274027824401855\n",
            "iteration 827 :train_loss:1.172878384590149 val_loss1.127389907836914\n",
            "iteration 828 :train_loss:1.172866702079773 val_loss1.1273770332336426\n",
            "iteration 829 :train_loss:1.172855019569397 val_loss1.1273643970489502\n",
            "iteration 830 :train_loss:1.1728435754776 val_loss1.1273517608642578\n",
            "iteration 831 :train_loss:1.1728318929672241 val_loss1.1273386478424072\n",
            "iteration 832 :train_loss:1.1728204488754272 val_loss1.1273261308670044\n",
            "iteration 833 :train_loss:1.1728088855743408 val_loss1.127313494682312\n",
            "iteration 834 :train_loss:1.1727973222732544 val_loss1.1273008584976196\n",
            "iteration 835 :train_loss:1.1727858781814575 val_loss1.1272883415222168\n",
            "iteration 836 :train_loss:1.172774314880371 val_loss1.1272755861282349\n",
            "iteration 837 :train_loss:1.1727628707885742 val_loss1.1272629499435425\n",
            "iteration 838 :train_loss:1.1727514266967773 val_loss1.1272504329681396\n",
            "iteration 839 :train_loss:1.172739863395691 val_loss1.1272377967834473\n",
            "iteration 840 :train_loss:1.1727285385131836 val_loss1.1272251605987549\n",
            "iteration 841 :train_loss:1.1727169752120972 val_loss1.1272127628326416\n",
            "iteration 842 :train_loss:1.1727056503295898 val_loss1.1272000074386597\n",
            "iteration 843 :train_loss:1.172694206237793 val_loss1.1271874904632568\n",
            "iteration 844 :train_loss:1.172682762145996 val_loss1.127174973487854\n",
            "iteration 845 :train_loss:1.1726713180541992 val_loss1.1271624565124512\n",
            "iteration 846 :train_loss:1.1726601123809814 val_loss1.127150058746338\n",
            "iteration 847 :train_loss:1.172648549079895 val_loss1.1271376609802246\n",
            "iteration 848 :train_loss:1.1726373434066772 val_loss1.1271252632141113\n",
            "iteration 849 :train_loss:1.1726258993148804 val_loss1.127112865447998\n",
            "iteration 850 :train_loss:1.1726146936416626 val_loss1.1271001100540161\n",
            "iteration 851 :train_loss:1.1726033687591553 val_loss1.1270878314971924\n",
            "iteration 852 :train_loss:1.172592043876648 val_loss1.1270756721496582\n",
            "iteration 853 :train_loss:1.1725808382034302 val_loss1.1270630359649658\n",
            "iteration 854 :train_loss:1.1725695133209229 val_loss1.127050757408142\n",
            "iteration 855 :train_loss:1.172558307647705 val_loss1.1270384788513184\n",
            "iteration 856 :train_loss:1.1725469827651978 val_loss1.1270262002944946\n",
            "iteration 857 :train_loss:1.17253577709198 val_loss1.1270136833190918\n",
            "iteration 858 :train_loss:1.1725244522094727 val_loss1.1270012855529785\n",
            "iteration 859 :train_loss:1.1725132465362549 val_loss1.1269890069961548\n",
            "iteration 860 :train_loss:1.1725019216537476 val_loss1.1269768476486206\n",
            "iteration 861 :train_loss:1.1724908351898193 val_loss1.1269644498825073\n",
            "iteration 862 :train_loss:1.172479510307312 val_loss1.1269521713256836\n",
            "iteration 863 :train_loss:1.1724684238433838 val_loss1.1269398927688599\n",
            "iteration 864 :train_loss:1.1724570989608765 val_loss1.1269276142120361\n",
            "iteration 865 :train_loss:1.1724460124969482 val_loss1.126915454864502\n",
            "iteration 866 :train_loss:1.17243492603302 val_loss1.1269031763076782\n",
            "iteration 867 :train_loss:1.1724237203598022 val_loss1.1268908977508545\n",
            "iteration 868 :train_loss:1.1724125146865845 val_loss1.1268788576126099\n",
            "iteration 869 :train_loss:1.1724013090133667 val_loss1.1268666982650757\n",
            "iteration 870 :train_loss:1.1723902225494385 val_loss1.126854419708252\n",
            "iteration 871 :train_loss:1.1723791360855103 val_loss1.1268421411514282\n",
            "iteration 872 :train_loss:1.172368049621582 val_loss1.1268301010131836\n",
            "iteration 873 :train_loss:1.1723568439483643 val_loss1.1268178224563599\n",
            "iteration 874 :train_loss:1.1723458766937256 val_loss1.1268057823181152\n",
            "iteration 875 :train_loss:1.1723346710205078 val_loss1.126793622970581\n",
            "iteration 876 :train_loss:1.1723237037658691 val_loss1.1267814636230469\n",
            "iteration 877 :train_loss:1.172312617301941 val_loss1.1267694234848022\n",
            "iteration 878 :train_loss:1.1723014116287231 val_loss1.1267573833465576\n",
            "iteration 879 :train_loss:1.1722904443740845 val_loss1.1267452239990234\n",
            "iteration 880 :train_loss:1.1722793579101562 val_loss1.1267330646514893\n",
            "iteration 881 :train_loss:1.1722681522369385 val_loss1.1267210245132446\n",
            "iteration 882 :train_loss:1.172257423400879 val_loss1.126708984375\n",
            "iteration 883 :train_loss:1.1722462177276611 val_loss1.126697063446045\n",
            "iteration 884 :train_loss:1.172235369682312 val_loss1.1266849040985107\n",
            "iteration 885 :train_loss:1.1722241640090942 val_loss1.1266729831695557\n",
            "iteration 886 :train_loss:1.1722133159637451 val_loss1.1266610622406006\n",
            "iteration 887 :train_loss:1.1722023487091064 val_loss1.1266487836837769\n",
            "iteration 888 :train_loss:1.1721912622451782 val_loss1.1266368627548218\n",
            "iteration 889 :train_loss:1.1721802949905396 val_loss1.1266248226165771\n",
            "iteration 890 :train_loss:1.1721692085266113 val_loss1.126612901687622\n",
            "iteration 891 :train_loss:1.1721582412719727 val_loss1.1266008615493774\n",
            "iteration 892 :train_loss:1.172147274017334 val_loss1.1265889406204224\n",
            "iteration 893 :train_loss:1.1721364259719849 val_loss1.1265769004821777\n",
            "iteration 894 :train_loss:1.1721255779266357 val_loss1.1265650987625122\n",
            "iteration 895 :train_loss:1.1721144914627075 val_loss1.1265531778335571\n",
            "iteration 896 :train_loss:1.1721035242080688 val_loss1.1265411376953125\n",
            "iteration 897 :train_loss:1.1720926761627197 val_loss1.1265292167663574\n",
            "iteration 898 :train_loss:1.172081708908081 val_loss1.1265172958374023\n",
            "iteration 899 :train_loss:1.172070860862732 val_loss1.1265053749084473\n",
            "iteration 900 :train_loss:1.1720597743988037 val_loss1.1264935731887817\n",
            "iteration 901 :train_loss:1.1720489263534546 val_loss1.1264816522598267\n",
            "iteration 902 :train_loss:1.172038197517395 val_loss1.126469612121582\n",
            "iteration 903 :train_loss:1.1720272302627563 val_loss1.1264578104019165\n",
            "iteration 904 :train_loss:1.1720162630081177 val_loss1.1264458894729614\n",
            "iteration 905 :train_loss:1.1720054149627686 val_loss1.126434087753296\n",
            "iteration 906 :train_loss:1.1719945669174194 val_loss1.1264222860336304\n",
            "iteration 907 :train_loss:1.1719835996627808 val_loss1.1264103651046753\n",
            "iteration 908 :train_loss:1.1719728708267212 val_loss1.1263984441757202\n",
            "iteration 909 :train_loss:1.171962022781372 val_loss1.1263867616653442\n",
            "iteration 910 :train_loss:1.1719510555267334 val_loss1.1263749599456787\n",
            "iteration 911 :train_loss:1.1719402074813843 val_loss1.1263630390167236\n",
            "iteration 912 :train_loss:1.1719292402267456 val_loss1.1263511180877686\n",
            "iteration 913 :train_loss:1.1719186305999756 val_loss1.1263394355773926\n",
            "iteration 914 :train_loss:1.1719077825546265 val_loss1.1263277530670166\n",
            "iteration 915 :train_loss:1.1718969345092773 val_loss1.1263158321380615\n",
            "iteration 916 :train_loss:1.1718860864639282 val_loss1.126304030418396\n",
            "iteration 917 :train_loss:1.171875238418579 val_loss1.1262922286987305\n",
            "iteration 918 :train_loss:1.17186439037323 val_loss1.126280426979065\n",
            "iteration 919 :train_loss:1.1718535423278809 val_loss1.1262688636779785\n",
            "iteration 920 :train_loss:1.1718428134918213 val_loss1.126257061958313\n",
            "iteration 921 :train_loss:1.1718319654464722 val_loss1.1262452602386475\n",
            "iteration 922 :train_loss:1.1718212366104126 val_loss1.126233458518982\n",
            "iteration 923 :train_loss:1.1718103885650635 val_loss1.1262216567993164\n",
            "iteration 924 :train_loss:1.171799659729004 val_loss1.12621009349823\n",
            "iteration 925 :train_loss:1.1717891693115234 val_loss1.1261982917785645\n",
            "iteration 926 :train_loss:1.1717780828475952 val_loss1.1261866092681885\n",
            "iteration 927 :train_loss:1.1717674732208252 val_loss1.1261746883392334\n",
            "iteration 928 :train_loss:1.171756625175476 val_loss1.1261630058288574\n",
            "iteration 929 :train_loss:1.171745777130127 val_loss1.126151442527771\n",
            "iteration 930 :train_loss:1.171735167503357 val_loss1.126139760017395\n",
            "iteration 931 :train_loss:1.1717243194580078 val_loss1.126128077507019\n",
            "iteration 932 :train_loss:1.1717135906219482 val_loss1.126116394996643\n",
            "iteration 933 :train_loss:1.1717028617858887 val_loss1.126104712486267\n",
            "iteration 934 :train_loss:1.171692132949829 val_loss1.1260930299758911\n",
            "iteration 935 :train_loss:1.1716814041137695 val_loss1.1260812282562256\n",
            "iteration 936 :train_loss:1.1716707944869995 val_loss1.1260695457458496\n",
            "iteration 937 :train_loss:1.17166006565094 val_loss1.1260579824447632\n",
            "iteration 938 :train_loss:1.1716493368148804 val_loss1.1260462999343872\n",
            "iteration 939 :train_loss:1.1716384887695312 val_loss1.1260346174240112\n",
            "iteration 940 :train_loss:1.1716278791427612 val_loss1.1260230541229248\n",
            "iteration 941 :train_loss:1.1716169118881226 val_loss1.1260113716125488\n",
            "iteration 942 :train_loss:1.1716063022613525 val_loss1.1259998083114624\n",
            "iteration 943 :train_loss:1.171595573425293 val_loss1.125988245010376\n",
            "iteration 944 :train_loss:1.171584963798523 val_loss1.1259765625\n",
            "iteration 945 :train_loss:1.1715742349624634 val_loss1.125964879989624\n",
            "iteration 946 :train_loss:1.1715635061264038 val_loss1.1259534358978271\n",
            "iteration 947 :train_loss:1.1715528964996338 val_loss1.1259417533874512\n",
            "iteration 948 :train_loss:1.1715421676635742 val_loss1.1259300708770752\n",
            "iteration 949 :train_loss:1.1715314388275146 val_loss1.1259185075759888\n",
            "iteration 950 :train_loss:1.1715208292007446 val_loss1.1259069442749023\n",
            "iteration 951 :train_loss:1.1715102195739746 val_loss1.125895380973816\n",
            "iteration 952 :train_loss:1.171499490737915 val_loss1.12588369846344\n",
            "iteration 953 :train_loss:1.171488881111145 val_loss1.1258721351623535\n",
            "iteration 954 :train_loss:1.1714781522750854 val_loss1.1258606910705566\n",
            "iteration 955 :train_loss:1.1714675426483154 val_loss1.1258491277694702\n",
            "iteration 956 :train_loss:1.1714568138122559 val_loss1.1258375644683838\n",
            "iteration 957 :train_loss:1.1714462041854858 val_loss1.1258258819580078\n",
            "iteration 958 :train_loss:1.1714357137680054 val_loss1.125814437866211\n",
            "iteration 959 :train_loss:1.1714248657226562 val_loss1.125802993774414\n",
            "iteration 960 :train_loss:1.1714143753051758 val_loss1.125791311264038\n",
            "iteration 961 :train_loss:1.1714036464691162 val_loss1.1257798671722412\n",
            "iteration 962 :train_loss:1.1713930368423462 val_loss1.1257681846618652\n",
            "iteration 963 :train_loss:1.1713824272155762 val_loss1.1257567405700684\n",
            "iteration 964 :train_loss:1.1713718175888062 val_loss1.1257452964782715\n",
            "iteration 965 :train_loss:1.1713613271713257 val_loss1.1257336139678955\n",
            "iteration 966 :train_loss:1.1713507175445557 val_loss1.1257221698760986\n",
            "iteration 967 :train_loss:1.1713398694992065 val_loss1.1257106065750122\n",
            "iteration 968 :train_loss:1.1713292598724365 val_loss1.1256991624832153\n",
            "iteration 969 :train_loss:1.171318769454956 val_loss1.1256877183914185\n",
            "iteration 970 :train_loss:1.171308159828186 val_loss1.125676155090332\n",
            "iteration 971 :train_loss:1.171297550201416 val_loss1.1256645917892456\n",
            "iteration 972 :train_loss:1.171286940574646 val_loss1.1256531476974487\n",
            "iteration 973 :train_loss:1.171276330947876 val_loss1.1256417036056519\n",
            "iteration 974 :train_loss:1.1712658405303955 val_loss1.1256301403045654\n",
            "iteration 975 :train_loss:1.171255111694336 val_loss1.1256186962127686\n",
            "iteration 976 :train_loss:1.1712446212768555 val_loss1.1256071329116821\n",
            "iteration 977 :train_loss:1.1712340116500854 val_loss1.1255958080291748\n",
            "iteration 978 :train_loss:1.1712234020233154 val_loss1.1255842447280884\n",
            "iteration 979 :train_loss:1.1712127923965454 val_loss1.125572919845581\n",
            "iteration 980 :train_loss:1.171202301979065 val_loss1.125561237335205\n",
            "iteration 981 :train_loss:1.1711915731430054 val_loss1.1255499124526978\n",
            "iteration 982 :train_loss:1.171181082725525 val_loss1.1255383491516113\n",
            "iteration 983 :train_loss:1.1711705923080444 val_loss1.125527024269104\n",
            "iteration 984 :train_loss:1.1711599826812744 val_loss1.1255154609680176\n",
            "iteration 985 :train_loss:1.171149492263794 val_loss1.1255042552947998\n",
            "iteration 986 :train_loss:1.171138882637024 val_loss1.1254926919937134\n",
            "iteration 987 :train_loss:1.1711283922195435 val_loss1.1254812479019165\n",
            "iteration 988 :train_loss:1.171117901802063 val_loss1.1254699230194092\n",
            "iteration 989 :train_loss:1.171107292175293 val_loss1.1254583597183228\n",
            "iteration 990 :train_loss:1.1710968017578125 val_loss1.1254470348358154\n",
            "iteration 991 :train_loss:1.1710859537124634 val_loss1.1254355907440186\n",
            "iteration 992 :train_loss:1.1710755825042725 val_loss1.1254242658615112\n",
            "iteration 993 :train_loss:1.171065092086792 val_loss1.1254127025604248\n",
            "iteration 994 :train_loss:1.1710546016693115 val_loss1.1254013776779175\n",
            "iteration 995 :train_loss:1.1710439920425415 val_loss1.1253899335861206\n",
            "iteration 996 :train_loss:1.1710333824157715 val_loss1.1253786087036133\n",
            "iteration 997 :train_loss:1.171022891998291 val_loss1.1253670454025269\n",
            "iteration 998 :train_loss:1.1710124015808105 val_loss1.125355839729309\n",
            "iteration 999 :train_loss:1.1710017919540405 val_loss1.1253443956375122\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QXP7mRfdhhKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "h_VBY7A-hhMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WLS-8DhZhhPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2Wo0d823hhSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Nywn4EAnhhVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KkUTWqlGhha-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mjaaYKF5hheT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}